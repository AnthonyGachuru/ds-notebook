# Transform & Visualize

## Transform

### General

* use spatial sign to transform data with outliers.

* Too many levels for a category?

    * limit number of categories through feature selection
    * https://stats.stackexchange.com/questions/95212/improve-classification-with-many-categorical-variables
    * bucket groups by number of samples
 
* https://cran.r-project.org/web/packages/vtreat/index.html

* Long story short, if these outliers are really such (i.e. they appear with a very low frequency and very likely are bad/random/corrupted measurements) and they do not correspond to potential events/failures that your model should be aware of, you can safely remove them. In all other cases you should evaluate case by case what those outliers represent.

* Otherwise, assuming levels of the categorical variable are ordered, the polyserial correlation (here it is in R), which is a variant of the better known polychoric correlation. Note the latter is defined based on the correlation between the numerical variable and a continuous latent trait underlying the categorical variable.

* Use runif to do sampling

* You should use a logarithmic scale when percent change, or change in orders of magnitude, is more important than changes in absolute units. You should also use a log scale to better visualize data that is heavily skewed. Taking the logarithm only works if the data is non-negative. There are other transforms, such as arcsinh or signed log, that you can use to decrease data range if you have zero or negative values. 

* Normalizing by mean and standard deviation is most meaningful when the data distribution is roughly symmetric.

* Monetary amounts are often log-https://github.com/ColinFay/erum2018normally distributed—the log of the data is normally distributed. This leads us to the idea that taking the log of the data can restore symmetry to it. 

### Imputation

### Missingness & Imputation

There are three types of missingness: 1) Completely at Random, 2) At Random, 3) Not at random

* The pros of imputation: 1) Helps retain a larger sample size of your data, 2) Does not sacrifice all the available information in an observation because of sparse missingness, 3) Can potentially avoid unwanted bias.

* The cons of imputation: 1) The standard errors of any estimates made during analyses following imputation can tend to be too small, 2) The methods are under the assumption that all measurements are actually “known,” when in fact some were imputed, 3) Can potentially induce unwanted bias.

* Other: 1) dplyr::case_when, 2) mean of each column = df.mean(axis = 0) || df.mean(axis = 'index' ), 3) mean of each row = df .mean(axis = 1) || df .mean(axis = 'columns' ) 

Some solutions:

* Mean: Simple but can distort distribution, underestimate standard deviation, and distort variable relationships by dragging correlation to zero.

* Random: Can amplify outlier observation and induce bias.

* Regression: Use observed variables and relationships between these variables. Must make assumptions and can badly extrapolate.

## Visualize

* logging converts multiplicative relationships to additive relationships, and by the same token it converts exponential (compound growth) trends to linear trends. By taking logarithms of variables which are multiplicatively related and/or growing exponentially over time, we can often explain their behavior with linear models.

* swarmplot

* Rule of thumb for bins in histogram: `ggplot() + geom_histogram(aes(x), binwidth = diff(range(x)) / (2 * IQR(x) / length(x)^(1/3)))`.
