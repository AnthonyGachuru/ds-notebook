# Stats

## General

ecdf: [1](http://www.ericmjl.com/blog/2018/7/14/ecdfs/) | [2](https://www.rdocumentation.org/packages/stats/versions/3.4.3/topics/ecdf)

http://www.stat.umn.edu/geyer/old/5101/rlook.html

https://stats.idre.ucla.edu/other/mult-pkg/whatstat/

categorical dissimilairty https://www.rdocumentation.org/packages/StatMatch/versions/1.2.5/topics/gower.dist
 
http://frog.oerc.ox.ac.uk:8080/stato-app/

Pearson’s Coefficient of Skewness | Bayes factor | Correction methods for non representative samples | Numeric-Categorical Correlation https://www.rdocumentation.org/packages/ltm/versions/1.1-1/topics/biserial.cor

Outliers in small samples: https://stats.stackexchange.com/questions/78609/outlier-detection-in-very-small-sets/78617#78617

Since all outcomes are 0 or 1, and drawn with the same (unknown) probability, we know that the number of ones and zeros follows a binomial distribution. This means that the confidence interval of a “k out of n ” scenario is a Beta distribution.

Another common measure of effect size is d, sometimes known as Cohen's d (as you might have guessed by now, Cohen was quite influential in the field of effect sizes). This can be used when comparing two means, as when you might do a t-test, and is simply the difference in the two groups' means divided by the average of their standard deviations*. This means that if we see a d of 1, we know that the two groups' means differ by one standard deviation; a d of .5 tells us that the two groups' means differ by half a standard deviation; and so on. Cohen suggested that d=0.2 be considered a 'small' effect size, 0.5 represents a 'medium' effect size and 0.8 a 'large' effect size. This means that if two groups' means don't differ by 0.2 standard deviations or more, the difference is trivial, even if it is statistically significant. 

Beta Distribution: The larger the alpha parameter is the closer the resulting distribution is to 1.0 and the larger the beta the closer it is to 0.

Fisher's Exact Test: Test for proportions

cohen's f^2 | https://stats.stackexchange.com/questions/266003/interpretation-of-cohens-f2-for-effect-size-in-multiple-regression | https://www.researchgate.net/post/How_to_calculate_the_effect_size_in_multiple_linear_regression_analysis

Bayes Rule: Posterior = Likelihood * Prior / Marginal Likelihood

https://www.rdocumentation.org/packages/effsize/versions/0.7.1/topics/cohen.d

Likelihood P(Data|theta): Probability of the date could be generated by a model with the given parameter(s)

Prior P(theta): Probability of parameter value

Posterior P(theta|D):  Credibility of the parameter value with the data taken into account.

Marginal Likelihood (Evidence): Probability of evidence

Variance:  Spread of the data | Distance from the mean

Standard Deviation: Square root of the variance | Same units as the data

Mean: Average/Expected Value

Covariance: Measure of how two random values change together. Negative - variables show opposite behavior ( As A increases B decreases). Positive - variables show similar behavior (As A increases B increases)

Pearson Correlation: Normalized version of covariance 

Median: Number at the halfway point of the dataset, found through sorting data into ascending order and selecting middle point

Mode: Most frequent value in dataset

Note about Populations and Samples: Populations have parameters. Samples have statistics.

Distributions:

Normal/Gaussian: Parameters are mean and standard deviation.  Bell shaped. z-score: indicates how many standard deviations an element is 	from the mean

Student t: used to estimate the mean of a normally distributed population where the sample size is small and the standard deviation is unknown


A t test (t.test in R) is a special case of an ANOVA and a paired t test (parameter paired = T) is a special case of a repeated measures ANOVA.

Sample size: A good formula is: if your sample is picked uniformly at random and is of size at least:

$\frac{-log(\frac{d}{2})}{2e^2}$

then with probability at least 1-d your sample measured proportion q is no further away than e from the unknown true population proportion p (what you are trying to estimate). Need to estimate with 99% certainty the unknown true population rate to +- 10% precision? That is d=0.01, e=0.1 and a sample of size 265 should be enough. A stricter precision of +-0.05 causes the estimated sample size to increase to 1060, but increasing the certainty requirement to 99.5% only increases the estimated sample size to 300.

* Type I Error: Saying there is an effect when there isn't one. Type II error: concluding there is no effect when, in fact, there is one.

* The MWW RankSum test is a useful test to determine if two distributions are significantly different or not. Unlike the t-test, the RankSum test does not assume that the data are normally distributed, potentially providing a more accurate assessment of the data sets.

* Probably the most useful types of statistics for skewed probability distributions are quantiles. 

* Hypothesis testing requires an inferential-statistical approach. Crucial are meaningful distributions of test statistics, on which p-values for hypothesis tests can be based. It is not trivial to construct such “meaningful distributions” for complete network data!

* Random sampling & random assignment: generalizable & causal

* Get 95% ci around p-value

* There is no one rule about when a number is not accurate enough to use, but as a rule of thumb, you should be cautious about using any number with a MOE (Margin-of-error) over 10%.

* Probability is calibrated if it is empirically correct, i.e. the empirical probability matches the theoretical one.

* [Effect size](https://artax.karlin.mff.cuni.cz/r-help/library/lsr/html/cohensD.html)

* jarque.bera.test: Tests the null of normaility for the series using the Jarque-Bera test statistics	

* box.test: Compute the Box-Pierce or Ljung-Box test statistics for examining the null of independence in a given series			

* shapiro.test: Test for normality		

* Sum of normally distributed random variables: sum of means & sum of variances

* Zero correlation doesn't mean independent variables. Covariance only determines linear relationships.

* test heteroscedaticity: breusch-pagan or ncv test

* Test Normality: Shapiro–Wilk test is a test of normality

* Bypass inferential stats if features at least 1/10 of data points. 

* The probability (p-value) of observing results at least as extreme as what is present in your data sample. P-value less than .05 retain h_0 else reject h_0.

* One Sample T-test: To examine the average difference between a sample and the known value 
of the population mean. Assumes the population from which the sample is drawn is normally distributed and the sample observations are randomly drawn and independent. 

* Two Sample T-test: To examine the average difference between two samples drawn from two 
different populations. Assumes the populations from which the samples are drawn are normally dist, the standard deviations of the two populations are equal, and sample observations are randomly drawn and independent.

* One-Way ANOVA: To assess the equality of means of two or more groups. Assumes the populations from which the samples are drawn are normally dist, the standard deviations of the populations are equal, and sample observations are randomly drawn and independent. 

* Chi-Squared Test of Independence: To test whether two categorical variables are independent.  Assumes the sample observations are randomly drawn and independent.

* F-Test: To assess whether the variances of two different populations are equal. Assumes the population from which the sample is drawn is normally distributed and the sample observations are randomly drawn and independent.

* Barlett Test: F-Test for more than two populations. 

* If we take a larger and larger sample from a population, its distribution will tend to become normal no matter what it is initially. It won't. The Central Limit Theorem, the misreading of which is the cause of this mistake, refers to the distribution of standardized sums of random variables as their number grow, not to the distribution of a collection of random variables.

* Frequentists: Probability is a measure of the the frequency of repeated events. The parameters are fixed (but unknown) and data are random.

* Bayesians: Probability is a measure of the degree of certainty about values, so the interpretation is that rameters are random and data are fixed.

* Independent rvs are uncorrelated. That is Cor (X,Y)=0

* Don’t sum sd’s, sum variances.

* One Sample T-Test? To examine the average difference between a sample and the known value of the population mean. Assumptions: The population from which the sample is drawn is normally distributed. Sample observations are randomly drawn and independent.

* When do we use the Two Sample T-Test? To examine the average difference between two samples drawn from two different populations. Assumptions: The populations from which the samples are drawn are normally dist. The standard deviations of the two populations are equal. Sample observations are randomly drawn and independent.

* When do we use the F-Test? To assess whether the variances of two different populations are equal. Assumptions: The populations from which the samples are drawn are normally dist. Sample observations are randomly drawn and independent.

* When do we use One-Way ANOVA?  To assess the equality of means of two or more groups. NB: When there are exactly two groups, this is equivalent to a Two Sample T-Test. Assumptions: The populations from which the samples are drawn are normally dist.  The standard deviations of the populations are equal. Sample observations are randomly drawn and independent.

* When do we use the chisq2 Test of Independence? To test whether two categorical variables are independent. Assumptions: Sample observations are randomly drawn and independent.

* To perform the one-way ANOVA, we can use the f_oneway() function of the SciPy package.

* The degrees of freedom is the number of data rows minus the number of coefficients fit.

* Hypothesis tests aim to describe the plausibility of a parameter taking on a specific value. Confidence intervals aim to describe a range of plausible values a parameter can take on. If the value of the parameter specified by H0 is contained within the 95% confidence interval, then H0 cannot be rejectedat the 0.05 p-value threshold. If the value of the parameter specified by H0 is not contained within the 95% confidence interval, then H0 can be rejectedat the 0.05 p-value threshold.

### Introduction to Data {-}

There is no random sampling since the subjects of the study were volunteers, so the results cannot be generalized to all people. However, due to random assignment, we are able to infer a causal link between the belief information is stored and the ability to recall that same information.

Explanatory variables are conditions you can impose on the experimental units, while blocking variables are characteristics that the experimental units come with that you would like to control for.

In random sampling, you use stratifying to control for a variable. In random assignment, you use blocking to achieve the same goal.

Control: compare treatment of interest to a control group 

Randomize: randomly assign subjects to treatments 

Replicate: collect a sufficiently large sample within a study, or replicate the entire study 

Block: account for the potential effect of confounding variables 

Group subjects into blocks based on these variations, and randomize within each block to treatment groups 

Random Sampling + Random Assignment: Causal + generalizable

No Random Sampling + Random Assignment: Causal + not generalizable

Random Sampling + No Random Assignment: Not causal + generalizable

No Random Sampling + No Random Assignment: Not causal + not generalizable

### Statistical Tests

The appropriate sample size depends on many things, chiefly the complexity of the analysis and the expected effect size. The "30" rule comes from one of the simplest cases: Whether to use a z-test or t-test for comparing two means. 

1) for large n (sample size), many distributions can be approximated to normal. [ courtesy : Central Limit Theorems ]

2) The approximation becomes better with larger n, given other things remaining the same.

However if you are wondering why 30 ? why not 25 or 40 or something else ? Note that actual answer will depend:

1) how much error are you going to tolerate

2) size of the effect

3) exact distribution family (like t vs chi-square vs gamma)

The power of any test of statistical significance is defined as the probability that it will reject a false null hypothesis. Statistical power is inversely related to beta or the probability of making a Type II error. In short, power = 1 – β. In plain English, statistical power is the likelihood that a study will detect an effect when there is an effect there to be detected. If statistical power is high, the probability of making a Type II error, or concluding there is no effect when, in fact, there is one, goes down. Statistical power is affected chiefly by the size of the effect and the size of the sample used to detect it. Bigger effects are easier to detect than smaller effects, while large samples offer greater test sensitivity than small samples.

The following four quantities have an intimate relationship:

* sample size

* effect size

* significance level = P(Type I error) = probability of finding an effect that is not there

* power = 1 - P(Type II error) = probability of finding an effect that is there

Given any three, we can determine the fourth.

**Other**

* The power of any test of statistical significance is defined as the probability that it will reject a false null hypothesis. Statistical power is inversely related to beta or the probability of making a Type II error. In short, power = 1 – beta.

* For quantitative explanatory variables, effect sizes always have the unit of the response variable divided by the unit of the explanatory variable.

* [Power Analysis Calculator](http://www.evanmiller.org/ab-testing/sample-size.html)

### Stats For Hackers
 
* Methods: Simluation, Boostrapping (with replacement, not good with ranks), Shuffling (Works when assumption is that the data are the same, representative samples, non-longitudinal data), & Cross Validation.

* Assumes iid.

### Think Stats

* Is it possible that the apparent effect is due to selection bias or some other error in the experimental setup? If so, then we might conclude that the effect is an artifact; that is, something we created (by accident) rather than found.

* The sampling distribution is the distribution of the samples mean.

* The CDF is the function that maps values to their percentile rank in a distribution.

* I’ll start with the exponential distribution because it is easy to work with. In the real world, exponential distributions come up when we look at a series of events and measure the times between events, which are called interarrival times. If the events are equally likely to occur at any time, the distribution of inter-arrival times tends to look like an exponential distribution.

* CLT: More specifically, if the distribution of the values has mean and standard deviation μ and σ, the distribution of the sum is approximately N (nμ, nσ^2).

* Another option is to report just the Bayes Factor or likelihood ratio, P(E | H A ) / P(E|H 0 ), rather than the posterior probability.

* Statistical power is the probability that the test will be positive if the null
hypothesis is false. In general, the power of a test depends on the sample
size, the magnitude of the effect, and the threshold α.

* If there are no outliers, the sample mean minimizes the mean squared error

* An estimator is unbiased if the expected total (or mean) error, after many iterations of the estimation game, is 0.

* A challenge in measuring correlation is that the variables we want to compare might not be expressed in the same units. For example, height might be in centimeters and weight in kilograms. And even if they are in the same
units, they come from different distributions. There are two common solutions to these problems:

1. Transform all values to standard scores. This leads to the Pearson coefficient of correlation.

2. Transform all values to their percentile ranks. This leads to the Spearman coefficient.

### AB Testing Overview

Guide: Choose metric (CTR), review stats (Hypothesis Testing), design, analyse

Need to have clear control and thing to test. Also need time. Frequency of customer interaqction is important.

shopping and searching not independent events.

Point estimate of CTR (users who clicked)/users. Need margin of error for estimate. Approximate binomial by normal if N*p & N(1-p) > 5. 

marin of error = Z*(stndad error)

se = sqrt(p(1-p)/n)

Z for 95% s 1.96 and 2.58 for 99%.

for Two samples calculate a pooled CTR point estimate and standard error. 

Need to be substantive (a worthy difference) in addition to being statistically significant. Some changes might not be useful due to the resources one might want to allocate.

Result are repeatable (stat sig) bar should be lower than business interesting (practically sig)

Stat Power: How many page view necessary to get a stat sig result. 

Size v Power: The smaller the effect or increased confidence you want to detect/have (greater power of experiment), the larger the sample you need to use.

alpha = P(reject null when true) 
beta = P(accept null when false)

small sample: low alpha, high beta
larger sample: low alpha, low beta

1- alpha = confidence level
1 - beta = sensitivity (often 80%)

practical significance: 2%, alpha = 0.05, beta = 0.2

sample size dp_to ctr (increase in se), confidence level, sensitivity
sample size invp_to practical sig (larger changes easier to detect)

above is design of experiment. below is analysis:

estimate diff:  experimental prop - control prop
margin of error = z*se

conf int: estimated diff +/- margin of error

**Policy & Ethics**

In the study, what risk is the participant undertaking? The main threshold is whether the risk exceeds that of “minimal risk”.

Next, what benefits might result from the study? Even if the risk is minimal, how might the results help? 

Third, what other choices do participants have? For example, if you are testing out changes to a search engine, participants always have the choice to use another search engine.

Finally, what data is being collected, and what is the expectation of privacy and confidentiality? This last question is quite nuanced, encompassing numerous questions: Do participants understand what data is being collected about them? What harm would befall them should that data be made public? Would they expect that data to be considered private and confidential?

Our recommendation is that there should be internal reviews of all proposed studies by experts regarding the questions:

Are participants facing more than minimal risk?
Do participants understand what data is being gathered?
Is that data identifiable?
How is the data handled?

Internal process recommendations
Finally, regarding internal process of data handling, we recommend that:

Every employee who might be involved in A/B test be educated about the ethics and the protection of the participants. Clearly there are other areas of ethics beyond what we’ve covered that discuss integrity, competence, and responsibility, but those generally are broader than protecting participants of A/B tests (cite ACM code of ethics).

All data, identified or not, be stored securely, with access limited to those who need it to complete their job. Access should be time limited. There should be clear policies of what data usages are acceptable and not acceptable. Moreover, all usage of the data should be logged and audited regularly for violations. You create a clear escalation path for how to handle cases where there is even possibly more than minimal risk or data sensitivity issues.

rate better for usability test than prob

**Choosing & Characterizing Metrics**

**Define**

Make sure to granularize funnel

**Intuition**

NA

**Characterize**

Focus groups, UER (user experience study), survey, experiments, retrospective analysis, external data

temporal effects across hours, days, weeks are important

different browsers screw with the CTR depending on how they deal with JS

Can use cookies to track users

Metric definitions

Def #1 (Cookie probability): For each <time interval>, number of cookies that click divided by number of cookies

Def #2 (Pageview probability): Number of pageviews with a click within <time interval> divided by number of pageviews

Def #3 (Rate): Number of clicks divided by number of pageviews

CTR vs CTP: The first can be >1. 

Take into consideration mean and median. Median is robust but might not be useful so look at percentiles. 

Measuring sensitiviyt and robustness: 1) Running experiments to see if metrics move as predicted. A vs A experiments: Measure people who saw the same thing to see if there is a difference between them according to the metric or its too sensitive. Look at previous experiments. 2) Retrospective analysis of logs to see how the metrics responded in the past.

Metric For Loading Vid: Look at distribution of load times for vid. Want a robust metric that doesn't really change for comparable vids. But wnat a metric that is sensitive to a change that you care about like resolution.

How to compare experiment vs control: difference or relative change. Relative change allows you to keep same significance boundary. 

Check that praticial sig level is good for metric. Need to have handle on variability for confidence interval.

sign test: Good for looking to implement a change, but doesn't give effect size.

empirical vs analytic: underlying distribution could be weird so do empirical. Analyticcal for simple metrics. If empirical and anlytical don't agree then run AvA test.

A vs A test: Std proportional to square root of number of samples. Diminishing returns for many tests. Can use bootstrap if don't have enough traffic for a big A v A test.

**Designing An Experiment**

**Choose Subject**

Unit of Diversion: How to indentify a person. Cookie (could clear so change), userid, deviceid, and address (changes all the time).

Intra-user vs inter-user experiment.

**Choose Population**

Size

Duration

Cohort: People who enter the experiment at the same time.

Cohort > Population: Learning effects, user retention

Can reduce sample if just targeting english speaking traffic

When going to run experiment: holiday or not? How many people to put through experiment and control?

**Analyzing Results**

Sanity Checks:

Pass all before move on

Good invariants: # of events, video load time (user has no control over it). Want these to  be about the same for experiment and control.

Single Metric:

sign test

Multiple Metrics:

More metrics increase false positive chance. Use Bonferroni assumption: makes no independence assumption and is conservative. 

margin of error smaller than observed diff so the confidence interval won't include 0. 

Different strategies: family-wise error rate, false discovery rate, 

Gotchas: 

Simpson's Paradox

Usually A/B testing works for testing changes in elements in the web page. A/B testing framework is following sequence:

Design a research question.

Choose test statistics method or metrics to evaluate experiment.

Designing the control group and experiment group.

Analyzing results, and draw valid conclusions.

## DataCamp: Experimental Design

Randomized Complete Block Design (RCBD) experiment

Power usually @ 80%. power effect size & sample size all inter-related (with 2 can calculate 3rd). 

power: prob that test correctly rejects null hypo when alt hypo is true

effect size: standardized measure of the difference you're trying to detect

sample size: how many experimental units needed to survey to detect the desired diff at the desired power

pwr::pwr.anova.test() [exactly one of n, d, power, and sig.level must be NULL]

aov = lm + anova
aov + tukey hsd

Another assumption of ANOVA and linear modeling is homogeneity of variance. Homogeneity means "same", and here that would mean that the variance of int_rate is the same for each level of grade. We can test for homogeneity of variances using bartlett.test(), which takes a formula and a dataset as inputs.

One non-parametric alternative to ANOVA is the Kruskal-Wallis rank sum test. For those with some statistics knowledge, it is an extension of the Mann-Whitney U test for when there are more than two groups, such as with our grade variable.

Use the correct function from pwr to find the sample size (d is the effect size)
pwr::pwr.t.test(n = NULL, d = .2,  power = .8,  sig.level  =.05, alternative = "two.sided")

multivariable experiment:

lendingclub_multi <- lm(loan_amnt ~ Group + grade + verification_status, lendingclub_ab)
tidy(lendingclub_multi)

The sampling package's cluster() creates cluster samples. It takes in a dataset name, the variable in the set to be used as the cluster variable, passed as a vector with the name as a string, e.g. c("variable"), a number of clusters to select, and a method

Randomized Complete Block Designs

the purpose of blocking an experiment is to make the experimental groups more like one another. 

A rule of thumb in experimental design is often "block what you can, randomize what you cannot", which means you should aim to block the effects you can control for (e.g. sex) randomize on those you cannot (e.g. smoking status). Variability inside a block is expected to be fairly small, but variability between blocks will be larger.
