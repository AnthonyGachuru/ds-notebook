--- 
title: "Data Science Cribsheeet"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: rstudio/bookdown-demo
description: "A collection of quick notes on the field."
---

# Workflow 

## Preamble

A typical data science workflow can be framed as following these steps:

* Business: start with a business question, define the goal and measure of success

* Data: find, access and explore the data

* Features: extract, assess and evaluate, select and sort

* Models: find the right model for the problem at hand, compare, optimize, and fine tune

* Communication: Interpret and communicate the results

* Production: transform the code into production ready code, integrate into current ecosystem and deploy

* Maintain: adapt the models and features to the evolution of the environment

More succinctly, as per Hadley Wickham:

import -> tidy -> understand [ transform <-> visualize <-> model ] -> communicate

My combination of the two:

preparation -> import -> tidy -> understand [ transform <-> visualize <-> model ] -> [communicate + deploy + maintain]

Another note is that one way to structure projects is to write user stories a la software engineering. 

What lies below is a succinct version of a workflow. Theoretical specifics can be found in other sections of this collection of cribsheets, and libraries or packages to use not included in the Stack can be found on this [page](https://gfleetwood.github.io/noted-resources/data_science.html).

## Annotated Checklist

### Preparation

* Understand the business question, goals, and measures of success

* See if the data to answer the question exists, and if it is useful.

* Structure project as user stories

* Initiliaze [base project directory](https://github.com/gfleetwood/ds-crib-sheet/blob/master/other/setup_project.R) and set up versioning.

* Initialize report to track and summarise work and to do list.

* Remember to use atomic commit messages

### Import

* Sampling from data if its too large

    * [subsample package](https://pypi.python.org/pypi/subsample) in Python
    * [sqldf library](https://stackoverflow.com/a/22262726/6627726) in R
    
* [Sqlite Converter](https://github.com/thombashi/sqlitebiter)

* [Easy Database Management](https://dataset.readthedocs.io/en/latest/)

### Tidy

* [Follow tidy data principles](http://vita.had.co.nz/papers/tidy-data.pdf). In R: widyr, tidyr, dplyr.

* Figure out what you're trying to do with the data.

* See what the data looks like: types of variables, the first and last few observations, missing values or outliers.

    * [xray](https://blog.datascienceheroes.com/x-ray-vision-on-your-datasets/)
    * [skimr](https://github.com/ropenscilabs/skimr)
    * [xda](https://github.com/ujjwalkarn/xda)
    * [janitor](https://cran.r-project.org/web/packages/janitor/vignettes/introduction.html)
    * scipy.stats.describe()
    * outlier detection: interquartile range, kernel density estimation, bonferroni test

### Transform

* Imputation. Options include the Mean, Mode, KNN, Random, and Regression. 
   * r imputation: mice, hmisc, alice


* Look at missingness. IN Python there are [naniar](https://github.com/njtierney/naniar) and [missingno](https://github.com/ResidentMario/missingno) packages. In R there is the [VIM library](https://rstudio-pubs-static.s3.amazonaws.com/4625_fa990d611f024ea69e7e2b10dd228fe7.html).

* An R library for [categorical dissimilarity](https://www.rdocumentation.org/packages/StatMatch/versions/1.2.5/topics/gower.dist)

* [Easy Regex](https://github.com/VerbalExpressions/PythonVerbalExpressions)

* Writing sql in pandas: https://github.com/yhat/pandasql

* Character Encoding Precendence: utf-8, iso-8859-1, utf-16

* Check for blank spaces and replace with NA. In R: `df[df==""] = NA`.

* [Binned Stats](https://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.stats.binned_statistic.html)

* Anomalies: xda::num/catSummary(.) | xray::anomalies(.)

### Visualize

* Quick Exploration Guide: S.O.C.S (Shape, Outlier, Center, Spread)

* Trendlines & Histograms

* Confidence intervals

* Compare the distributions of variables with overlayed density plots 

* Scatterplots: Pairwise and color/size the dots by other variables to try to identify any confounding relationships

* Dimensionality reduction (PCA, Kernel PCA, TSNE, Isomap) or hierachical clustering for multivariate data to get a feel for high dimensional structure. 

### Model

* Stick to rergression models for prescriptive analysis. Validate assumptions and tune appropriately. If using Python leverage statsmodels.

* Pre-processing: https://github.com/WinVector/vtreat

* Pipelines & Feature Unions

* Do feature selection

    * Variance Threshold
    * Univariate Feature Selection: skl_fs.chi2 | f_regression | f_classification
    * SelectKBest/SelectPercentile
    * sklearn: feature_selection.RFE(CV)
    * [Rebate](https://github.com/EpistasisLab/scikit-rebate)
    * [Boruta](https://github.com/scikit-learn-contrib/boruta_py)
    
* Do feature engineering: Standardization, dimensionality reduction, dummying. sklearn.preprocessing.binarize, pandas.factorize

* Model Selection

    * https://github.com/DistrictDataLabs/yellowbrick
    * [Sklearn Evaluation](https://edublancas.github.io/sklearn-evaluation/)
    * [skl-plot](https://github.com/reiinakano/scikit-plot)

* Model Explanability

    * [lime](https://github.com/marcotcr/lime)
    * [eli5](https://github.com/TeamHG-Memex/eli5)

* Tuning

    * https://github.com/hyperopt/hyperopt-sklearn
    * https://github.com/rsteca/sklearn-deap

* Deal with potential class imbalance

   * Gradient boosting
   * http://contrib.scikit-learn.org/imbalanced-learn/stable/

* Create ensembles: mlxtend in Python
    
* Choose Statistical Test

    * [1](http://www.ats.ucla.edu/stat/mult_pkg/whatstat/)
    * [2](http://www.qnamarkup.org/i/?source=http://colarusso.github.io/QnAMarkup/examples/source/WhatStats.txt)
    * infer library
    
* Check for correlations: https://github.com/drsimonj/corrr
    
* Ensembling: mlxtend in Python, and caretEnsemble in R.

* Look at learning curves: https://www.dataquest.io/blog/learning-curves-machine-learning/

### Communicate

* Usually an R Markdown document.

### Deployment /  Maintenance

* Write tests

    * [1](http://engineering.pivotal.io/post/test-driven-development-for-data-science/)
    * [2](http://www.tdda.info/)
    * [3](http://stochasticsolutions.com/)
    * [Data Testing](https://github.com/ericmjl/data-testing-tutorial)
    * [ML Testing](https://medium.com/@keeper6928/how-to-unit-test-machine-learning-code-57cf6fd81765)
    * [Data Validation](https://github.com/data-cleaning/validate)
    * pytest | hypothesis | testthat
    * [Argument Checks](https://rdrr.io/cran/checkmate/)[PyValidation](https://github.com/shawnbrown/datatest)

* Write helper functions.
