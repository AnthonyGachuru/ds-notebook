--- 
title: "Data Science Cribsheet"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: rstudio/bookdown-demo
description: "A collection of quick notes on the field."
---

# Workflow 

## Preamble

A typical data science workflow can be framed as following these steps:

* Business: start with a business question, define the goal and measure of success

* Data: find, access and explore the data

* Features: extract, assess and evaluate, select and sort

* Models: find the right model for the problem at hand, compare, optimize, and fine tune

* Communication: Interpret and communicate the results

* Production: transform the code into production ready code, integrate into current ecosystem and deploy

* Maintain: adapt the models and features to the evolution of the environment

More succinctly, as per Hadley Wickham: 

import -> tidy -> understand [ transform <-> visualize <-> model ] -> communicate

My combination of the two:

preparation -> import -> tidy -> understand [ transform <-> visualize <-> model ] -> [communicate + deploy + maintain]

Another note is that one way to structure projects is to write user stories a la software engineering. 

What lies below is a succinct version of a workflow. Theoretical specifics can be found in other sections of this collection of cribsheets, and libraries or packages to use not included in the Stack can be found on this [page](https://gfleetwood.github.io/noted-resources/data_science.html).

## Checklist

### Preparation

* Understand the business question, goals, and measures of success

* See if the data to answer the question exists, and if it is useful.

* Structure project as user stories

* Initialize report to track and summarise work and to do list.

* Remember to use atomic commit messages

* checklists | code books | appendices | post mortems | tests | gacp

### Import

* Character Encoding Precendence: utf-8, iso-8859-1, utf-16

* Sampling from data if its too large

### Tidy

* [Follow tidy data principles](http://vita.had.co.nz/papers/tidy-data.pdf). 

* Figure out what you're trying to do with the data.

* See what the data looks like: types of variables, the first and last few observations, missing values or outliers.

* Outlier Detection: interquartile range, kernel density estimation, bonferroni test

### Transform

* Look at missingness.

* Imputation. Options include the Mean, Mode, KNN, Random, and Regression.

* Use the Gower distance for categorical dissimilarity.

* Check for blank spaces and replace with NA. In R: `df[df==""] = NA`.

* [Binned Stats](https://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.stats.binned_statistic.html)

### Visualize

* Quick Exploration Guide: S.O.C.S (Shape, Outlier, Center, Spread)

* Trendlines & Histograms

* Confidence intervals

* Compare the distributions of variables with overlayed density plots 

* Scatterplots: Pairwise and color/size the dots by other variables to try to identify any confounding relationships.

* Dimensionality reduction (PCA, Kernel PCA, TSNE, Isomap) or hierarchical clustering for multivariate data to get a feel for high dimensional structure. 

### Model

* Stick to rergression models for prescriptive analysis. Validate assumptions and tune appropriately. If using Python leverage statsmodels.

* Pre-processing: Use binning + chisquare / mutual information to see correlation between feature and target.

* Pipelines & Feature Unions

* Feature Selection: Variance Threshold, Univariate Feature Selection, Multivariate Feature Selection.
    
* Feature Engineering: Standardization, dimensionality reduction, dummying. sklearn.preprocessing.binarize, pandas.factorize, and featuretools for automation in Python. 

* Model Selection

* Model Explanability: SHAP

* Tuning: Do bayesian tuning instead of grid or random search. Also use hyperopt-sklearn, sklearn-deap, and scikit-optimize. 

* Deal with potential class imbalance: Gradient boosting is one option.

* Create ensembles
    
* Ensembling: mlxtend in Python, and caretEnsemble in R.

* Look at learning curves: [1](https://www.dataquest.io/blog/learning-curves-machine-learning/) | [2](http://www.scikit-yb.org/en/latest/api/model_selection/learning_curve.html)

### Communicate

Reference this [blog](https://www.dataquest.io/blog/data-science-project-style-guide/).

### Deployment /  Maintenance

[Git Workflow](https://blog.osteele.com/2008/05/my-git-workflow/)

Write Tests.

Write helper functions, do testing: 

[1](http://engineering.pivotal.io/post/test-driven-development-for-data-science/) | [2](http://www.tdda.info/)
 | [3](http://stochasticsolutions.com/) | [4](https://github.com/ericmjl/data-testing-tutorial)) 
 
And validation:

[1](https://github.com/data-cleaning/validate) | [2](https://rdrr.io/cran/checkmate/) | [3](https://github.com/shawnbrown/datatest)

A really useful feature is sharing environments so others can install all the packages used in your code, with the correct versions. You can save the packages to a YAML file with: `conda env export > environment.yaml`

The first part conda env export writes out all the packages in the environment, including the Python version. Above you can see the name of the environment and all the dependencies (along with versions) are listed. The second part of the export command, > environment.yaml writes the exported text to a YAML file environment.yaml. This file can now be shared and others will be able to create the same environment you used for the project. To create an environment from an environment file use: `conda env create -f environment.yaml`

This will create a new environment with the same name listed in environment.yaml.

## Resources

### General

CoCalc

Conda

[SOTA Results](https://paperswithcode.com/sota)

[DS Project Style Guide](https://www.dataquest.io/blog/data-science-project-style-guide/

Simulated Annealing: Gradient descent extension

"No Free Lunch" theorem: no single classifier works best across all possible scenarios

Sequence mining Algorithms: spade, gsp, freespan; hmmlearn 

Network models: graphical lasso, ising model, granger causality test, network hypothesis testing, bayesian networks

Cohort analysis

Transfer learning

hierarchical clustering: ward

hamming loss: multilabel and multiclass classification

cox proportional hazards & multivariate linear regression

sequential data: rnn, lstm, hidden markov models & conditional random fields

fowles-mallows score for clustering when ground truth is known

youden's j

Multiple Correspondence Analysis

platt scaling

set the seed always

natural log goes from unit change to percentage change

generalized low rank models

conditional random field

linear probability model

[http://r-pkgs.had.co.nz/tests.html](http://r-pkgs.had.co.nz/tests.html)

[https://cartesianfaith.com/2013/03/10/better-logging-in-r-aka-futile-logger-1-3-0-released/](https://cartesianfaith.com/2013/03/10/better-logging-in-r-aka-futile-logger-1-3-0-released/)

[http://www.markvanderloo.eu/yaRb/2016/03/25/easy-data-validation-with-the-validate-package/](http://www.markvanderloo.eu/yaRb/2016/03/25/easy-data-validation-with-the-validate-package/)

[https://cran.r-project.org/web/packages/assertr/vignettes/assertr.html](https://cran.r-project.org/web/packages/assertr/vignettes/assertr.html)

confint conjoint analysis Incremental PCA fe: Permutation Measure meta learning | Rasch Model | Learning_to_rank

### Data

http://www.fast.ai/2018/10/16/aws-datasets

https://www.spe.org/en/jpt/jpt-article-detail/?art=5282

Causality: [1](https://www.kaggle.com/c/cause-effect-pairs) [2](http://webdav.tuebingen.mpg.de/cause-effect/)

https://mimic.physionet.org/

https://janelia.figshare.com/articles/Eight-probe_Neuropixels_recordings_during_spontaneous_behaviors/7739750

http://bbcsfx.acropolis.org.uk/

https://hub.packtpub.com/25-datasets-deep-learning-iot/

https://www.backblaze.com/b2/hard-drive-test-data.html

https://toolbox.google.com/datasetsearch

https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research

https://unpaywall.org/products/snapshot

https://blog.webkid.io/datasets-for-machine-learning/

https://www.githubarchive.org

https://www.datasetlist.com/

https://www.enigma.com/data

https://www.kaggle.com/about/datasets-awards/overview

http://veekaybee.github.io/2018/07/23/small-datasets/

https://grasswiki.osgeo.org/wiki/Global_datasets

https://www.figure-eight.com/datasets/

https://stanfordmlgroup.github.io/competitions/mura/

[sensor data](https://data.melbourne.vic.gov.au/Transport-Movement/Pedestrian-volume-updated-monthly-/b2ak-trbp)

[UCI ML Repo](http://archive.ics.uci.edu/ml/)

[Datanauts](https://open.nasa.gov/explore/datanauts/)

Open Census Data: [1](https://www.producthunt.com/posts/open-census-data) | [2](https://www.safegraph.com/open-census-data?ref=producthunt) | [3](https://docs.safegraph.com/docs/open-census-data)

[World Holidays](https://date.nager.at/)

Rosalind: [1](http://rosalind.info/problems/locations/) | [2](https://github.com/mtarbit/Rosalind-Problems)

[Research Data](http://academictorrents.com/)
 
SEC: [1](https://www.sec.gov/structureddata) | [2](https://www.sec.gov/dera/data/financial-statement-and-notes-data-set.html) | [3](https://www.sec.gov/dera/data/financial-statement-data-sets.html) | [4](https://www.sec.gov/dera/data/financial-statement-data-sets.html)

Neuro Data: [1](http://preprocessed-connectomes-project.org) | [2](http://www.humanconnectomeproject.org) | [3](http://fcon_1000.projects.nitrc.org/indi/abide/) | [4](https://www.ncbi.nlm.nih.gov/pubmed/25840117) | [5](https://coins.mrn.org)

time series: [1](http://www2.stat.duke.edu/~mw/ts_data_sets.html) | [2](https://datamarket.com/data/list/?q=provider%3Atsdl) | [3](https://archive.ics.uci.edu/ml/datasets.html?format=&task=&att=&area=&numAtt=&numIns=&type=ts&sort=nameUp&view=table) | [4](https://machinelearningmastery.com/challenging-machine-learning-time-series-forecasting-problems/) | [5](https://machinelearningmastery.com/challenging-machine-learning-time-series-forecasting-problems/) | [6](www.kaggle.com/c/demand-forecasting-kernels-only) | [7](https://www.kaggle.com/c/ams-2014-solar-energy-prediction-contes) | [8](https://www.kaggle.com/c/GEF2012-wind-forecasting) | [9](https://www.kaggle.com/c/web-traffic-time-series-forecasting)

[Microsoft Research](https://msropendata.com)

Disney Animation: [1](https://blog.yiningkarlli.com/2018/07/disney-animation-datasets.html) | [2](https://www.disneyanimation.com/technology/datasets)

[Million Song](https://labrosa.ee.columbia.edu/millionsong/) | [Music Net](https://homes.cs.washington.edu/~thickstn/musicnet.html)

[SMS Spam](http://www.esp.uem.es/jmgomez/smsspamcorpus/)

[ML Cameron](https://gist.github.com/olivercameron/482dcfe8f34d66b536b1048eefe8b40d#file-datasets-csv)

[Kickstarter & Indiego Archive](https://webrobots.io/kickstarter-datasets/)

[Data Is Plural Archive](https://docs.google.com/spreadsheets/d/1wZhPLMCHKJvwOkP4juclhjFgqIY8fQFMemwKL2c64vk/edit#gid=0)

[Data.gov](http://www.data.gov/)

NYC: [1](https://nycopendata.socrata.com/) | [2](https://health.data.ny.gov) | [3](http://www.nyc.gov/html/dot/html/about/datafeeds.shtml)

Global: [1](http://datacatalogs.org/) | [2](http://index.okfn.org/place/) | [3](http://data.worldbank.org/)

Sentiment: [1](http://ai.stanford.edu/~amaas/data/sentiment/) | [2](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews)

Scripts: [1](https://www.springfieldspringfield.co.uk) | [2](https://github.com/BobAdamsEE/SouthParkData) | [3](http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html) | [4](http://transcripts.foreverdreaming.org/viewtopic.php?f=809&t=31120)

[Word-Emotion Association Lexicon](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm)

[Word Bank](http://wordbank.stanford.edu/)

Q & A: [1](https://rajpurkar.github.io/SQuAD-explorer/) | [2](https://www.reddit.com/r/datasets/comments/1uyd0t/200000_jeopardy_questions_in_a_json_file/) | [3](http://jmcauley.ucsd.edu/data/amazon/qa/) | [4](http://ai2-website.s3.amazonaws.com/publications/algebra-TACL2015.pdf) | [5](http://quac.ai/)

Autonomous Driving: [1](http://robotcar-dataset.robots.ox.ac.uk/) | [2](https://medium.com/udacity/open-sourcing-223gb-of-mountain-view-driving-data-f6b5593fbfa5#.ch9a8c762) | [3](https://github.com/commaai/research) | [4](https://www.scaleapi.com/image-annotation/self-driving-cars)

[Amazon Reviews](http://jmcauley.ucsd.edu/data/amazon/)

CV: [1](https://research.googleblog.com/2016/09/introducing-open-images-dataset.html) | [2](http://aloi.science.uva.nl/?__vid=5c0983704ebd0132821522000b2a88d7) | [3](http://www.vision.caltech.edu/visipedia/CUB-200.html?__vid=5c0983704ebd0132821522000b2a88d7) | [4](http://www.robots.ox.ac.uk/~vgg/data/flowers/17/?__vid=5c0983704ebd0132821522000b2a88d7) | [5](http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/) | [6](https://github.com/vc1492a/Hey-Waldo) | [7](https://github.com/gtoderici/sports-1m-dataset)

Networks: [1](https://snap.stanford.edu/data/index.html) | [2](http://networkrepository.com/index.php) | [3](https://aminer.org/data-sna)

[Rec Systems](http://www.lab41.org/nine-datasets-for-investigating-recommender-systems/)

Sound: [1](http://labrosa.ee.columbia.edu/millionsong/) | [2](https://github.com/mdeff/fma) | [3](http://homes.cs.washington.edu/~thickstn/musicnet.html) | [4](https://datasets.freesound.org/fsd/)

[Medical Data](https://github.com/beamandrew/medical-data)

[Deep Mind](https://deepmind.com/research/open-source/open-source-datasets/)

[NASA](https://bmtgoncalves.github.io/pyNASA/)

[Visual Reasoning](http://lic.nlp.cornell.edu/nlvr/)

[Allen Institute](http://allenai.org/data.html)

[Miscellaneous Datasets](http://users.stat.ufl.edu/~winner/datasets.html)

[Sound Effects](http://bbcsfx.acropolis.org.uk/)

[NASA](https://open.nasa.gov/blog/)

[Biodiversity](https://www.gbif.org)

[Summarization Data](https://summari.es)

[Kaggle Datasets](https://www.kaggle.com/datasets)

[NLP](https://github.com/niderhoff/nlp-datasets/blob/master/README.md)

[Portals](https://www.opendatasoft.com/a-comprehensive-list-of-all-open-data-portals-around-the-world/)

[European Data Portal](http://www.europeandataportal.eu/)

[Journalism Data](http://cjlab.stanford.edu/2015/09/30/lab-launch-and-data-sets/) 

[Enigma](https://public.enigma.com/)

[Wiki Data Dump](http://dumps.wikimedia.org/other/pagecounts-raw/)

### Auto Data Science

[Data Cleaning](http://www.dataland.ai/index.php) 

[Visualize](http://holoviews.org)

Feature Engineering: [1](https://www.featuretools.com) | [2](https://www.featurelabs.com/) 

Auto ML: TPOT

### Vanderplas Jupyter

* create helper functions and units tests as R/py for Rmd/ipynb

* pytest/hypothesis for testing

* use makefile to run cmd commands

* Make package for workflow with data (__init__.py) and use this formation for function docs:

* `def fun(a): return(a) #Role Parameters Returns`

### Sample R Pipeline

[Source](https://mobile.twitter.com/TheStephLocke/status/990251709531344896):

IO: odbc readxl httr 

EDA: DataExplorer

Prep: tidyverse

Sampling: rsample modelr

Feature Engineering: recipes

Modeling: glmnet h2o FFTrees

Evaluation: broom yardstick

Deployment: sqlrutils AzureML opencpu

Monitoring: flexdashboard

Docs: rmarkdown
