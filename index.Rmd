--- 
title: "Data Science Cribsheet"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes 
github-repo: rstudio/bookdown-demo
description: "A collection of quick notes on the field."
---

# Workflow 

## Preamble

A typical data science workflow can be framed as following these steps:

* Business: start with a business question, define the goal and measure of success

* Data: find, access and explore the data

* Features: extract, assess and evaluate, select and sort

* Models: find the right model for the problem at hand, compare, optimize, and fine tune

* Communication: Interpret and communicate the results

* Production: transform the code into production ready code, integrate into current ecosystem and deploy

* Maintain: adapt the models and features to the evolution of the environment

Another instantiation via Hadley Wickham is: import -> tidy -> understand [ transform <-> visualize <-> model ] -> communicate. ( ittvmcmd as an abbreviation.) And the two combined: preparation -> import -> tidy -> understand [ transform <-> visualize <-> model ] -> [communicate + deploy + maintain]. And this is an [attempt](https://gist.github.com/gfleetwood/3c152bee44d03a00fa5e8df3210a9918) to fit this into the principles in ["The Checklist Manifesto"](https://www.amazon.com/dp/B0030V0PEW/ref=dp-kindle-redirect?_encoding=UTF8&btkr=1).

## Checklist

### Preparation

- [ ] Understand the business question, goals, and measures of success.

- [ ] See if the data to answer the question exists and if it is useful.

- [ ] Structure project as user stories 

### Pre-Modeling

- [ ] See what the data looks like by shape, outliers, center, and spread, the types of variables, first and last few observations, and check for missing and blank values.

- [ ] Plot scatterplots, overlayed density plots, trendlines, and histograms.

- [ ] Dimensionality reduction or hierarchical clustering to get a feel for high dimensional structure.

### Model

- [ ] Pre-processing - Feature Selection & Engineering

- [ ] Check relationships between features and target: scatterplots and chi-squared test

- [ ] Check model assumptions

- [ ] Model ensembles

- [ ] Model tuning

- [ ] Look at confidence intervals

- [ ] Model explanability and variable importance

- [ ] Look at learning curves

## Resources

Auto Data Science: [Visualize](http://holoviews.org) [Feature_Engineering](https://www.featuretools.com) TPOT [Google AutoML](https://cloud.google.com/automl/) [AutoML](https://www.automl.org/)

[Albon Snippets](https://chrisalbon.com/)

[Stochastic Spreadsheets](https://www.getguesstimate.com/) https://causal.app/ https://distill.pub/ https://pair-code.github.io/what-if-tool/ https://nextjournal.com/ https://paperswithcode.com/ https://gitlab.com/meltano/meltano https://jdssv.org/index.php/jdssv https://rapids.ai/ https://postersession.ai/

## Pipelines

**Sample Pipeline**

[Source](https://mobile.twitter.com/TheStephLocke/status/990251709531344896)

IO: odbc readxl httr 

EDA: DataExplorer

Prep: tidyverse

Sampling: rsample modelr

Feature Engineering: recipes

Modeling: glmnet h2o FFTrees

Evaluation: broom yardstick

Deployment: sqlrutils AzureML opencpu

Monitoring: flexdashboard

Docs: rmarkdown

**My Pipeline**

Import: tidyverse:::readr odbc dbi 

Tidy & Transform: tidyverse:::dplyr+tidyr skimr visdat data.table

Visualization: ggplot2 plotnine hypertools plotly_express 

Modeling: sklearn mlxtend yellowbrick spark shap

Communicate: rmarkdown 

Monitor: hypothesis

Deploy: shiny

**R Pipeline**

Import: odbc dbi readr data.table

Tidy: skimr dplyr tidyr visdat data.table

Transform: dplyr data.table

Visualization: ggplot2

Modeling: rsample modelr broom yardstick caret sparklyr

Communicate: rmarkdown

Monitor:

Deploy: shiny

**Python Pipeline**

Import: pyodbc pandas-modin

Tidy: pandas-modin

Transform: pandas-modin

Visualization: altair plotnine

Modeling: sklearn mlxtend shap yellowbrick pyspark chainlearn

Communicate: jupyter

Monitor:

Deploy: flask
