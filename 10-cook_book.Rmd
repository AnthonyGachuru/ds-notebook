# Codebook 

## General

[Chris Albon](http://chrisalbon.com/)

## Stats By Simulation

Notes on the talk by Jake Vanderplas. I left out bootstrap and cross validation below. For all methods look out for selection bias and make sure you have representative samples. More on the bootstrap: These are simulated confidence intervals that work well for n > 20 as a rule of thumb.

**Direct Simulation**

Works well with an apriori (generative) model of the world, like the probability distribution of a coin toss. Example: Flip a fair coin 30 times. How likely it is to see 22 heads?

```{r, eval = F}
n <- 10^6
h_a <- 22
result <- purrr::map(seq(1, n, 1), 
           ~ sum(base::sample(c(0,1), 30, replace = T)) >= h_a) %>%
  unlist() %>% sum()

result/n
```

**Shuffling**

Comparing groups

```{r, eval = F}
iris_100 <- iris[1:100, c(2,5)]
(t.test(iris_100[1:50, 1], iris_100[51:100, 1], 
       alternative = 'greater'))$p.value
```

```{r, eval = F}
ha_shuffle <- function(df){
  
  result <- df %>% 
  mutate(Species = sample(Species, n())) %>% 
  group_by(Species) %>% 
  summarise_at(1, mean) %>% 
  mutate(mu_diff = Sepal.Width - lag(Sepal.Width)) %>% 
  filter(!is.na(mu_diff)) %>% 
  pull(mu_diff)
  
  return(result >= observed_diff)
  
}

observed_diff <- iris_100 %>% 
  group_by(Species) %>% 
  summarise_at(1, mean) %>% 
  mutate(mu_diff = Sepal.Width - lag(Sepal.Width)) %>% 
  filter(!is.na(mu_diff)) %>% 
  pull(mu_diff)

n <- 10^6
result <- purrr::map(seq(1, n, 1), ~ ha_shuffle(iris_100)) %>%
  unlist() %>% sum()

result/n
```

## Simple Bayes

```{python, eval = FALSE}

#Posterior prob that CTR_A > CTR_B given data is np.mean(A_samples > B_samples). Prob that lift of A relative to B is >=3%: np.mean(100*(A_samples-B_samples)/B_samples > 3)

from numpy.random import beta as beta_dist
import numpy as np

N_samp = 10000 #number of samples to draw
clicks_A = 450
views_A = 56000
clicks_B = 345
views_B = 49000
alpha = 1.1
beta = 14.2

A_samples = beta_dist(clicks_A + alpha, views_A - clicks_A + beta, N_samp)
B_samples = beta_dist(clicks_B + alpha, views_B - clicks_B + beta, N_samp)
```

## TS Harmonic Regression

```{r, eval = F}
# Set up harmonic regressors of order 13
harmonics <- fourier(gasoline, K = 13)
# Fit regression model with ARIMA errors
fit <- auto.arima(gasoline, xreg = harmonics, seasonal = FALSE)
# Forecasts next 3 years
fc <- forecast(fit, xreg = fourier(gasoline, K = 13, h = 156))
# Plot forecasts fc
autoplot(fc)
```

## All Sklearn Metrics

```{python, eval = F}
from sklearn.metrics import SCORERS

sorted(SCORERS.keys())
```

## Save And Load Models


```{python, eval = F}
from sklearn.externals import joblib

joblib.dump(grid_search, 'model.pkl')

grid_search = joblib.load('model.pkl')
model = grid_search.best_estimator_
```

```{r, eval = F}
save(m1, file = "my_model1.rda")
m1 = load("my_model1.rda")
```

## LSTM 

Data Shape

```{python, eval = F}
data_x = np.array([
    # Datapoint 1: 3 features for timesteps 1 & 2
    [[1, 2, 3], [4, 5, 6]],
    # Datapoint 2
    [[7, 8, 9], [10, 11, 12]]])
```

Prediction Example

```{python, eval = F}
mdl.predict([[np.array([.5, .6, .6] + [0 for i in range(14)]).reshape((17, 1))]])[0]
```

## Logistic Regression Stats Models

```{python, eval = FALSE}
import statsmodels.api as sm

X = sm.add_constant(X)
clf_lr = sm.Logit(Y, X).fit(disp=False)
print(results.summary())
```

## Function Negation

```{r, eval = FALSE}
not_in <- purrr::negate(%in%)
not_between <- purrr::negate(between)
```

## Attach Date CSV

```{python, eval = F}
df.to_csv('{}_model.csv'.format(str(datetime.datetime.now()).split(' ')[0]))
```

## EBB

```{r, eval = F}
df_ebb <- df %>%
  summarise(successes = sum(success), sample_size = n()) %>%
  add_ebb_estimate(num_passed, sample_size)
```

## Plot Grid 

```{r, eval = F}
gridExtra::grid.arrange(plots[[1]],plots[[2]],plots[[3]], plots[[4]],plots[[5]],plots[[6]], nrow=3)
```

## Pairwise Scatterplot

```{r, eval = F}
GGally::ggpairs(data, mapping = aes(colour = category))
```

## Anti-join

```{python, eval = F}
# https://stackoverflow.com/questions/38516664/anti-join-pandas

# Identify what values are in TableB and not in TableA
key_diff = set(TableB.Key).difference(TableA.Key)
where_diff = TableB.Key.isin(key_diff)

# Slice TableB accordingly and append to TableA
TableA.append(TableB[where_diff], ignore_index=True)
```

## DF Diff

```{python, eval = F}
# https://stackoverflow.com/a/36893675/6627726

merged = df1.merge(df2, indicator=True, how='outer')
merged[merged['_merge'] == 'right_only']
```

## CSV Encoding Check

```{python, eval = F}
import chardet

with open("../input/kickstarter-projects/ks-projects-201801.csv", 'rb') as rawdata:
    result = chardet.detect(rawdata.read(10000))
```

## Pandas Profiling

```{python, eval = F}
import pandas_profiling
pandas_profiling.ProfileReport(data)
```

## Read SQL 

```{r, eval = F}
df <- dbGetQuery(con, statement = read_file('query.sql'))
```


## Extract Date

```{r, eval = F}
StartTime %>% as.POSIXct() %>% strftime(format="%Y-%m-%d")
YearMonth = Day %>% strftime(format="%Y-%m")
date = date %>% as.POSIXct() %>% strptime('%Y-%m-%d') %>% strftime(format="%Y-%m-%d")
```

## GGPLOT To Plotly

```{r, eval = F}
ggplotly(ggplot(filter(df_master2, group == x), aes(x,y, label = z, color = w)) +
        geom_jitter(width = .2), tooltip = c('y', 'label'))
```

## Custom Theme For Plot

```{r, eval = F}
theme_ilo <- function() {
    theme_minimal() +
        theme(
            text = element_text(family = "Bookman", color = "gray25"),
            plot.subtitle = element_text(size = 12),
            plot.caption = element_text(color = "gray30"),
            plot.background = element_rect(fill = "gray95"),
            plot.margin = unit(c(5, 10, 5, 10), units = "mm")
        )
}
```

## Heatmap

```{r, eval = F}
library(corrplot)

df_fltrd %>% 
select_if(is.numeric) %>%
    cor() %>% 
corrplot(method = "circle", is.corr = FALSE)
```

## Linear Regression Statsmodels

```{python, eval = F}
import statsmodels.api as sm

X = sm.add_constant(X)
mdl = sm.OLS(Y, X).fit(disp = False)

print(mdl.summary())
```

## Linear Regression By Groups

```{r, eval = F}
librarytidyverse)

mtcars %>% group_by(cyl) %>% split(.$cyl) %>% purrr::map(~ lm(mpg ~ ., data = .))
mtcars %>% group_by(cyl) %>% nest() %>% 
mutate(model = map(data, function(x) lm(mpg ~ ., data = x) ))
```

## RML Pipeline

```{r, eval = F}
# categorical comparison to target
chisq.test(table(cut(iris$Sepal.Length, 3), iris$Species))

# Variable Importance
varImp
```

## Rowwise

```{python, eval = F}
rectangles = [
    { 'height': 40, 'width': 10 },
    { 'height': 20, 'width': 9 },
    { 'height': 3.4, 'width': 4 }
]
rectangles_df = pd.DataFrame(rectangles)
def calculate_area(row):
    return row['height'] * row['width']
rectangles_df.apply(calculate_area, axis=1)
```

```{r, eval = F}
library(dplyr)

mtcars %>% 
  rowwise() %>% 
  mutate(mymean = mean(c(cyl,mpg))) %>% 
  select(cyl, mpg, mymean)

df %>% 
  mutate(mu = select(., R1:R16) %>% pmap_dbl(~mean(c(...))))

calc_row_mean <- function(li){
  
  df <- as.data.frame(li)
  
  result <- df %>% 
    select(contains("Round")) %>%
    mutate(mu = rowMeans(.)) %>% 
    pull(mu)
  
  return(result)
  
}
```

## Sampling File

```{bash, eval = F}
pip install subsample
subsample -s 8 -n 100000 test.csv -r > test_sample.csv
```

## SQL Dummy Columns

```{sql, eval = F}
SELECT 'Dummy Column Text' as DUMMYCOL FROM tbl
```

## Model Calibration

A quick way of visualizing model calibration without having to manually bin any observations

```{r, eval = F}
ggplot(df, aes(fitted_probability, binary_class_indicator)) + geom_smooth()
```

## Binning Rule Of Thumb

```{r, eval = F}
# https://stats.stackexchange.com/questions/798/calculating-optimal-number-of-bins-in-a-histogram/862#862
ggplot() + geom_histogram(aes(x), binwidth = diff(range(x)) / (2 * IQR(x) / length(x)^(1/3)))
```

## Featuretools

```{python, eval = F}
#https://stackoverflow.com/questions/50145953/how-to-apply-deep-feature-synthesis-to-a-single-table

import numpy as np
import pandas as pd
import featuretools as ft

df = pd.read_csv('iris.csv')
df = df.reset_index()

es = ft.EntitySet(id = "test") #.drop(columns = ['species'], axis = 1)
es = es.entity_from_dataframe(entity_id = 'd', dataframe = df, make_index=True, index='ind')

# produces 626 features
fm, features = ft.dfs(
    entityset = es, 
    target_entity = 'd',
    agg_primitives = ['mean', 'max', 'percent_true', 'last'],
    trans_primitives = ['subtract', 'divide']
)

# produces no new features
_, features2 = ft.dfs(
    entityset = es, 
    target_entity = 'd',
    max_depth = 2
)
```

## Skimage

```{python, eval = F}
import numpy as np
import matplotlib.pyplot as plt
from skimage import io
from skimage.transform import resize

image = io.imread(r'test.jpg')
image2 = resize(image,(200, 200),mode='edge')

plt.imshow(image)
plt.show()
```

## SHAP

```{python, eval = F}
import shap

mdl.fit(X_train, y_train)
explainer = shap.TreeExplainer(clf)
shap_values = explainer.shap_values(X_train)

#all records explained (both lines of code)
shap.summary_plot(shap_values, X_train)
shap.summary_plot(shap_values, X_train, plot_type = "bar")

df_shap = pd.DataFrame(list(zip(np.mean(shap_values, axis = 0), X_train.columns)),
                       columns = ['shap_mean', 'feature'])
df_shap = df_shap[['feature', 'shap_mean']]

df_shap['shap_mean_abs'] = np.absolute(df_shap['shap_mean'])
df_shap.sort_values(['shap_mean_abs'], ascending = False, inplace = True)

import shap
import matplotlib.pyplot as plt

attribution_data = np.array(train_data[:50])
explainer = shap.DeepExplainer(model, attribution_data)
```

## Hyperband CV

```{python, eval = F}
#https://github.com/civisanalytics/civisml-extensions/blob/master/civismlext/hyperband.py

from civismlext.hyperband import HyperbandSearchCV
```

## CSV To DB

```{python, eval = F}
#source: http://bit.ly/294gmmP

import pandas as pd
import sqlite3

# pass in column names for each CSV
r_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']
ratings = pd.read_csv(fdata, sep=';', names=r_cols, compression='gzip')

m_cols = ['movie_id', 'title', 'genres']
movies = pd.read_csv(fitem, sep=';', names=m_cols, dtype={'title': object, 'genres': object})

sqlite_norm = "movielens-norm.sqlite"

if os.path.exists(sqlite_norm):
    os.unlink(sqlite_norm)

conn = sqlite3.connect(sqlite_norm)
conn.text_factory = str   # Shut up problems with Unicode
ratings.to_sql("ratings", conn)
movies.to_sql("movies", conn)
conn.close()
```

```{bash, eval = F}
sqlite3 db_name.db
.mode csv db_name
.import data.csv db_name
```

## Xgboost & Light GBM

```{python, eval = F}
import xgboost as xgb
import lightgbm as lgb

param_grid_lgb = {
    'learning_rate': [0.005, 0.05, 0.1], 'n_estimators': [10, 100, 250, 500],
    'num_leaves': [6, 50, 100, 250, 500], 'boosting_type': ['gbdt', 'rf'],
    'colsample_bytree' : [0.65, 1], 'subsample': [0.7, 0.9],
    'reg_alpha': [0, 1.2], 'reg_lambda': [0, 2],
    }

param_grid_xgb = {'bootstrap': [True, False],
 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],
 'max_features': ['auto', 'sqrt'],
 'min_samples_leaf': [1, 2, 4],
 'min_samples_split': [2, 5, 10],
 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}
```

## Bayesian CV

```{python, eval = F}
import numpy as np
import pandas as pd
import lightgbm as lgb
from scipy.stats import randint as sp_randint
from skopt import BayesSearchCV
from skopt.space import Real, Integer, Categorical

#v1: Light GBM

bayes_cv_tuner = BayesSearchCV(
    estimator = lgb.LGBMClassifier(
        objective='binary',
        metric='auc',
        n_jobs=1,
        verbose=0
    ),
    search_spaces = {
        'learning_rate': (0.01, 1.0, 'log-uniform'),
        'num_leaves': (1, 100),      
        'max_depth': (0, 50),
        'min_child_samples': (0, 50),
        'max_bin': (100, 1000),
        'subsample': (0.01, 1.0, 'uniform'),
        'subsample_freq': (0, 10),
        'colsample_bytree': (0.01, 1.0, 'uniform'),
        'min_child_weight': (0, 10),
        'subsample_for_bin': (100000, 500000),
        'reg_lambda': (1e-9, 1000, 'log-uniform'),
        'reg_alpha': (1e-9, 1.0, 'log-uniform'),
        'scale_pos_weight': (1e-6, 500, 'log-uniform'),
        'n_estimators': (50, 100),
    },    
    scoring = 'roc_auc',
    cv = sk_ms.StratifiedKFold(
        n_splits=3,
        shuffle=True,
        random_state = 42
    ),
    n_jobs = 3,
    n_iter = 10,   
    verbose = 0,
    refit = True,
    random_state = 42
)

result = bayes_cv_tuner.fit(X_train.values, y_train.values)

#v2: XGBoost

bayes_cv_tuner = BayesSearchCV(
    estimator = xgb.XGBClassifier(
        n_jobs = 1,
        objective = 'binary:logistic',
        eval_metric = 'auc',
        silent=1,
        tree_method='approx'
    ),
    search_spaces = {
        'learning_rate': (0.01, 1.0, 'log-uniform'),
        'min_child_weight': (0, 10),
        'max_depth': (0, 50),
        'max_delta_step': (0, 20),
        'subsample': (0.01, 1.0, 'uniform'),
        'colsample_bytree': (0.01, 1.0, 'uniform'),
        'colsample_bylevel': (0.01, 1.0, 'uniform'),
        'reg_lambda': (1e-9, 1000, 'log-uniform'),
        'reg_alpha': (1e-9, 1.0, 'log-uniform'),
        'gamma': (1e-9, 0.5, 'log-uniform'),
        'min_child_weight': (0, 5),
        'n_estimators': (50, 100),
        'scale_pos_weight': (1e-6, 500, 'log-uniform')
    },    
    scoring = 'roc_auc',
    cv = sk_ms.StratifiedKFold(
        n_splits=3,
        shuffle=True,
        random_state=42
    ),
    n_jobs = 3,
    n_iter = 10,   
    verbose = 0,
    refit = True,
    random_state = 42
)

#v3

param_grid = {
    'learning_rate': Real(0.005, 0.1), 'n_estimators': Integer(10, 500),
    'num_leaves': Integer(6, 50), 'boosting_type' : Categorical(['gbdt', 'rf']),
    'colsample_bytree' : Real(0.65, 1), 'subsample' : Real(0.7, 0.9),
    'reg_alpha' : Real(0, 1.2), 'reg_lambda' : Real(0, 2),
    }

clf_tuned = BayesSearchCV(clf, param_grid, cv = 5, random_state = 8, scoring = 'f1', n_jobs = -1)

```

## Pyspark

```{python, eval = FALSE}
import findspark
findspark.init()

import findspark
findspark.init()
import pyspark
import random

import pyspark
from pyspark.sql import SparkSession

from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler
from pyspark.ml import Pipeline
from pyspark.ml.classification import LogisticRegression
import pyspark.ml.evaluation as evals
import pyspark.ml.tuning as tune
import numpy as np

import pyspark
from pyspark.sql import SparkSession

sc = pyspark.SparkContext()
spark = SparkSession.builder.getOrCreate() # SparkSession.builder.appName('chosenName').getOrCreate()

sc = pyspark.SparkContext()
spark = SparkSession.builder.appName('example').getOrCreate()


sc = pyspark.SparkContext(appName="Pi")
num_samples = 100000000
def inside(p):     
    x, y = random.random(), random.random()
    return x*x + y*y < 1
count = sc.parallelize(range(0, num_samples)).filter(inside).count()
pi = 4 * count / num_samples
print(pi)

df = spark.read.csv('data.csv', header = True)
df.show(5)
df.columns

# Preprocessing

df = df.withColumn("label", df["target"].cast('integer'))

scf_indexer = StringIndexer(inputCol = "some_cat_feature", outputCol = "some_cat_feature_index")
scf_encoder = OneHotEncoder(inputCol = "some_cat_feature_index", outputCol = "some_cat_feature_fact")

feature_cols = ["some list of column names"]

vec_assembler = VectorAssembler(inputCols = feature_cols, 
                                outputCol = "features")

pipe = Pipeline(stages = [scf_indexer, scf_encoder, vec_assembler])

piped_data = pipe.fit(df).transform(df)
training, test = piped_data.randomSplit([.8, .2])

# Model

clf_lr = LogisticRegression()
evaluator = evals.BinaryClassificationEvaluator(metricName = "areaUnderROC")

grid = tune.ParamGridBuilder()
grid = grid.addGrid(clf_lr.regParam, np.arange(0, .1, .01))
grid = grid.addGrid(clf_lr.elasticNetParam, [0, 1])
grid = grid.build()

clf_lr_cv = tune.CrossValidator(
    estimator = clf_lr,
    estimatorParamMaps = grid,
    evaluator = evaluator,
    numFolds = 5
               )

best_clf_lr = clf_lr_cv.fit(training).bestModel
results = best_clf_lr.transform(training)
print(evaluator.evaluate(results))

## List tables
spark.catalog.listTables()

## Access and display data
query = "FROM flights SELECT * LIMIT 10"
flights10 = spark.sql(query)
flights10.show()

## Cast
df['g'] = df['g'].astype(str)

## Read from csv
df= spark.read.csv(file_path, header = True)
df = spark.read.csv('fileNameWithPath', mode="DROPMALFORMED",inferSchema=True, header = True)
df = spark.read.format("csv").option("header","true").option("inferSchema","true").load("john_doe.csv")

## Spark df to pandas and vice versa
toPandas()
spark_temp = spark.createDataFrame(pd_temp)

## Add to the catalog
spark_temp.createOrReplaceTempView("temp")

## Mutate
df = df.withColumn("newCol", df.oldCol + 1)
model_data = model_data.withColumn("arr_delay", model_data.arr_delay.cast('integer'))
model_data = model_data.withColumn("plane_age", model_data.year - model_data.plane_year)

## Create the DataFrame flights
flights = spark.table('flights')

## Get column names
spark_df.schema.names
spark_df.printSchema()

## Filter: takes either a Spark Column of boolean (True/False) values or the WHERE clause of a SQL expression as a string
long_flights1 = flights.filter('distance > 1000')
long_flights2 = flights.filter(flights.distance > 1000)
model_data = model_data.filter("arr_delay is not NULL and dep_delay is not NULL and air_time is not NULL and plane_year is not NULL")

## Groupby examples
flights.filter(flights.origin == "PDX").groupBy().min("distance").show()
flights.filter(flights.carrier=='DL').filter(flights.origin=='SEA').groupBy().avg('air_time').show()
flights.withColumn("duration_hrs", flights.air_time/60).groupBy().sum('duration_hrs').show()

## Spark functions
import pyspark.sql.functions as F
by_month_dest.agg(F.stddev('dep_delay')).show()

## Drop column
final_test_data.drop('State')

## Dummying

#The first step to encoding your categorical feature is to create a StringIndexer. Members of this class are #Estimators that take a DataFrame with a column of strings and map each unique string to a number. Then, the #Estimator returns a Transformer that takes a DataFrame, attaches the mapping to it as metadata, and returns a #new DataFrame with a numeric column corresponding to the string column.

#The second step is to encode this numeric column as a one-hot vector using a OneHotEncoder. This works exactly #the same way as the StringIndexer by creating an Estimator and then a Transformer

## Create a StringIndexer
carr_indexer = StringIndexer(inputCol="carrier", outputCol="carrier_index")

## Create a OneHotEncoder
carr_encoder = OneHotEncoder(inputCol="carrier_index", outputCol="carrier_fact")

## Make a VectorAssembler
vec_assembler = VectorAssembler(inputCols=["month", "air_time", "carrier_fact", "dest_fact", "plane_age"], outputCol="features")

## Import & Make Pipeline
from pyspark.ml import Pipeline
flights_pipe = Pipeline(stages=[dest_indexer, dest_encoder, carr_indexer, carr_encoder, vec_assembler])

## Fit and transform the data
piped_data = flights_pipe.fit(model_data).transform(model_data)

## Train-test split
training, test = piped_data.randomSplit([.6, .4])

## Tuning & Selection

## Import LogisticRegression
from pyspark.ml.classification import LogisticRegression

## Create a LogisticRegression Estimator
lr = LogisticRegression()

## Import the evaluation submodule
import pyspark.ml.evaluation as evals

## Create a BinaryClassificationEvaluator
evaluator = evals.BinaryClassificationEvaluator(metricName="areaUnderROC")

## Import the tuning submodule
import pyspark.ml.tuning as tune

## Create the parameter grid
grid = tune.ParamGridBuilder()

## Add the hyperparameter
grid = grid.addGrid(lr.regParam, np.arange(0, .1, .01))
grid = grid.addGrid(lr.elasticNetParam, [0, 1])

## Build the grid
grid = grid.build()

## Create the CrossValidator
cv = tune.CrossValidator(
estimator=lr,
estimatorParamMaps=grid,
evaluator=evaluator
               )

## Fit cross validation models
models = cv.fit(training)

## Extract the best model
best_lr = models.bestModel

## Use the model to predict the test set
test_results = best_lr.transform(test)

## Evaluate the predictions
print(evaluator.evaluate(test_results))

sc.stop()
```

## Sparklyr

```{r, eval = FALSE}
library(sparklyr)

sc <- spark_connect(master="local")
config <- spark_config()
config$spark.executor.cores <- 8
config$spark.executor.memory <- "25G"

flights <- copy_to(sc, flights, "flights")
src_tbls(sc)

iris_tbl %>%
  spark_apply(
    function(e) summary(lm(Petal_Length ~ Petal_Width, e))$r.squared,
    names = "r.squared",
    group_by = "Species")


## Connect to your Spark cluster
spark_conn <- spark_connect(master = "local")

## Print the version of Spark
spark_version(sc = spark_conn)

## Disconnect from Spark
spark_disconnect(sc = spark_conn)

## See tables
src_tbls(sc)

## Copy track_metadata to Spark
track_metadata_tbl <- copy_to(spark_conn, track_metadata)

## Print 5 rows, all columns
print(track_metadata_tbl, n = 5, width = Inf)

## Write and run SQL query
query <- "SELECT * FROM track_metadata WHERE year < 1935 AND duration > 300"
(results <- dbGetQuery(spark_conn, query))

## General transformation structure and example
a_tibble %>%
  ft_some_transformation("x", "y", some_other_args)

hotttnesss <- track_metadata_tbl %>%
  # Select artist_hotttnesss
  select(artist_hotttnesss) %>%
  # Binarize to is_hottt_or_nottt
  ft_binarizer('artist_hotttnesss', 'is_hottt_or_nottt', threshold = .5) %>%
  # Collect the result
  collect() %>%
  # Convert is_hottt_or_nottt to logical
  mutate(is_hottt_or_nottt = as.logical(is_hottt_or_nottt))
  
## Get and transform the schema

(schema <- sdf_schema(track_metadata_tbl))
schema %>%
  lapply(function(x) do.call(data_frame, x)) %>%
  bind_rows()

## Train-test split
partitioned <- track_metadata_tbl %>%
  sdf_partition(training = 0.7, testing = 0.3)

## List ml functions
ls("package:sparklyr", pattern = "^ml")

## GBT Example

gradient_boosted_trees_model <- track_data_to_model_tbl %>%
  # Run the gradient boosted trees model
   ml_gradient_boosted_trees('year', feature_colnames)

responses <- track_data_to_predict_tbl %>%
  # Select the year column
  select(year) %>%
  # Collect the results
  collect() %>%
  # Add in the predictions
  mutate(
    predicted_year = predict(
      gradient_boosted_trees_model,
      track_data_to_predict_tbl
    )
  )


spark_disconnect(sc)
```

## Data Table

```{r, eval = FALSE}
# Operations done by reference

# subset by i calculate by j grouped using by
dt[i, j, by] 

# prints last row
DT[.N] 

# colnames
names(DT)

dim(DT) # dimensions

DT[, .(A, B)] # returns two columns

DT[, c(A, B)] # returns a concatenated vector

DT[, .(sum_c = sum(C)] # mutate

DT[, plot(A, C)] # plot?

DT[, A := NULL] # Remove column A

DT[, .(sumB = sum(B)), by = .(Grp = A%%2)] # group_by & summarize

DT[, .N, by = Sepal.Width] # .N is the count of each group

DT[, lapply(.SD, median)] # .SD is a placeholder for all the columns

dogs[, lapply(.SD, mean), .SDcols = 2:3] # Find mean of columns 2 & 3

for (i in 1:5) set(DT, i, 3L, i+1) # update first 5 rows of 3rd column

setnames(DT, 'y', 'z') # changes colname from y to z

setkey(DT, A, B)

DT[.('b')]

DT[.(c('b', 'c'))]

DT[.(c('b', 'c')), mult="first"]

DT[c("b", "c"), .SD[c(1, .N)], by = .EACHI] # First and last row of the "b" and "c" groups

DT[c("b", "c"), { print(.SD); .SD[c(1, .N)] }, by = .EACH]

dt1[dt2, roll=-Inf, rollends=FALSE] # rolling join
```

## Keras 

The general framework: 1) instantiate, 2) add layers input-hidden-output, 3) compile, 4) fit

```{r, eval = F}
network <- keras_model_sequential() 

network %>%  
  layer_dense(units = 512, activation = "relu", input_shape = c(28 * 28)) %>%  
  layer_dense(units = 10, activation = "softmax") %>% 
  compile(optimizer = "rmsprop",  loss = "categorical_crossentropy",  metrics = c("accuracy")) %>% 
  fit(train_images, train_labels, epochs = 5, batch_size = 128)

metrics <- network %>% evaluate(test_images, test_labels)
```

```{python, eval = FALSE}

#import keras
#from keras.layers import Dense
#from keras.models import Sequential

# Regression

# Specify the model
#model = Sequential()
## Input
#model.add(Dense(50, activation='relu', input_shape = 3))
# Hidden
#model.add(Dense(32, activation='relu'))
# Output
#model.add(Dense(1))
# Compile the model
#model.compile(optimizer = 'adam', loss = 'mean_squared_error') 
# Fit the model
#model.fit(predictors, target))

# Classification

# Specify the model: Two hidden layers
#model = Sequential()
## Input
#model.add(Dense(50, activation='relu', input_shape = 3))
# Hidden
#model.add(Dense(32, activation='relu'))
# Output
#model.add(Dense(2, activation = 'softmax'))
# Compile the model
#model.compile(optimizer = 'sgd', loss = 'categorical_crossentropy', metrics = 'accuracy')
# Fit the model
#model.fit(predictors, target)

# Other

# Dummying
#y = 1
#keras.utils.to_categorical(y, num_classes = 2)

#Look at summary
#model.summary()

#Calculate predictions: predictions
#predictions = model.predict(pred_data)

#Calculate predicted probability of survival
predicted_prob_true = predictions[:, 1]
```

## MNIST Example

```{python, eval = F}
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn import svm

digits = datasets.load_digits()
#print(digits.data)
#print(digits.target)
print(digits.images[0])
clf = svm.SVC(gamma=.001,C=100)
x, y = digits.data[:-10], digits.target[:-10]
clf.fit(x,y)
print("Prediction: ", clf.predict(digits.data[-2].reshape(1, -1)))
plt.imshow(digits.images[-2], cmap=plt.cm.gray_r, interpolation="nearest")
plt.show()
```

