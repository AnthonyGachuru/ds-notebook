# Cook Book

## General

[Chris Albon](http://chrisalbon.com/)

## EBB

```
df_ebb <- df %>%
  summarise(successes = sum(success), sample_size = n()) %>%
  add_ebb_estimate(num_passed, sample_size)
```

## Plot Grid 

```
gridExtra::grid.arrange(plots[[1]],plots[[2]],plots[[3]], plots[[4]],plots[[5]],plots[[6]], nrow=3)
```

## Pairwise Scatterplot

```

GGally::ggpairs(data, mapping = aes(colour = category))
```

## Anti-join

```
# https://stackoverflow.com/questions/38516664/anti-join-pandas

# Identify what values are in TableB and not in TableA
key_diff = set(TableB.Key).difference(TableA.Key)
where_diff = TableB.Key.isin(key_diff)

# Slice TableB accordingly and append to TableA
TableA.append(TableB[where_diff], ignore_index=True)
```

## DF Diff

```
# https://stackoverflow.com/a/36893675/6627726

merged = df1.merge(df2, indicator=True, how='outer')
merged[merged['_merge'] == 'right_only']
```

## CSV Encoding Check

```
import chardet

with open("../input/kickstarter-projects/ks-projects-201801.csv", 'rb') as rawdata:
    result = chardet.detect(rawdata.read(10000))
```

## Pandas Profiling

```
import pandas_profiling
pandas_profiling.ProfileReport(data)
```

## Read SQL File

```
df <- dbGetQuery(con, statement = read_file('query.sql'))
```

## Extract Date

```
StartTime %>% as.POSIXct() %>% strftime(format="%Y-%m-%d")
YearMonth = Day %>% strftime(format="%Y-%m")
date = date %>% as.POSIXct() %>% strptime('%Y-%m-%d') %>% strftime(format="%Y-%m-%d")
```

## GGPLOT To Plotly

```
ggplotly(ggplot(filter(df_master2, group == x), aes(x,y, label = z, color = w)) +
        geom_jitter(width = .2), tooltip = c('y', 'label'))
```

## Custom Theme For Plot

```
theme_ilo <- function() {
    theme_minimal() +
        theme(
            text = element_text(family = "Bookman", color = "gray25"),
            plot.subtitle = element_text(size = 12),
            plot.caption = element_text(color = "gray30"),
            plot.background = element_rect(fill = "gray95"),
            plot.margin = unit(c(5, 10, 5, 10), units = "mm")
        )
}
```

## Heatmap

```
library(corrplot)

df_fltrd %>% 
select_if(is.numeric) %>%
    cor() %>% 
corrplot(method = "circle", is.corr = FALSE)
```


## Linear Regression Statsmodels

```{python, eval = F}
import statsmodels.api as sm

X = sm.add_constant(X)
mdl = sm.OLS(Y, X).fit(disp = False)

print(mdl.summary())
```

## Linear Regression By Groups

```{r, eval = F}
librarytidyverse)

mtcars %>% group_by(cyl) %>% split(.$cyl) %>% purrr::map(~ lm(mpg ~ ., data = .))
mtcars %>% group_by(cyl) %>% nest() %>% 
mutate(model = map(data, function(x) lm(mpg ~ ., data = x) ))
```


## RML Pipeline

```{r, eval = F}
# categorical comparison to target
chisq.test(table(cut(iris$Sepal.Length, 3), iris$Species))

# Variable Importance
varImp
```

## Rowwise

```{python, eval = F}
rectangles = [
    { 'height': 40, 'width': 10 },
    { 'height': 20, 'width': 9 },
    { 'height': 3.4, 'width': 4 }
]
rectangles_df = pd.DataFrame(rectangles)
def calculate_area(row):
    return row['height'] * row['width']
rectangles_df.apply(calculate_area, axis=1)
```

```{r, eval = F}
library(dplyr)

mtcars %>% 
  rowwise() %>% 
  mutate(mymean = mean(c(cyl,mpg))) %>% 
  select(cyl, mpg, mymean)
```

## Sampling File

```{bash, eval = F}
pip install subsample
subsample -s 8 -n 100000 test.csv -r > test_sample.csv
```

## SQL Dummy Columns

```{sql, eval = F}
SELECT 'Dummy Column Text' as DUMMYCOL FROM tbl
```

## Model Calibration

A quick way of visualizing model calibration without having to manually bin any observations

```{r, eval = F}
ggplot(df, aes(fitted_probability, binary_class_indicator)) + geom_smooth()
```

## Binning Rule Of Thumb

```{r, eval = F}
# https://stats.stackexchange.com/questions/798/calculating-optimal-number-of-bins-in-a-histogram/862#862
ggplot() + geom_histogram(aes(x), binwidth = diff(range(x)) / (2 * IQR(x) / length(x)^(1/3)))
```

## Featuretools

```{python, eval = F}
#https://stackoverflow.com/questions/50145953/how-to-apply-deep-feature-synthesis-to-a-single-table

import numpy as np
import pandas as pd
import featuretools as ft

df = pd.read_csv('iris.csv')
df = df.reset_index()

es = ft.EntitySet(id = "test") #.drop(columns = ['species'], axis = 1)
es = es.entity_from_dataframe(entity_id = 'd', dataframe = df, make_index=True, index='ind')

# produces 626 features
fm, features = ft.dfs(
    entityset = es, 
    target_entity = 'd',
    agg_primitives = ['mean', 'max', 'percent_true', 'last'],
    trans_primitives = ['subtract', 'divide']
)

# produces no new features
_, features2 = ft.dfs(
    entityset = es, 
    target_entity = 'd',
    max_depth = 2
)
```

## Skimage

```{python, eval = F}
import numpy as np
import matplotlib.pyplot as plt
from skimage import io
from skimage.transform import resize

image = io.imread(r'test.jpg')
image2 = resize(image,(200, 200),mode='edge')

plt.imshow(image)
plt.show()
```

## SHAP

```{python, eval = F}
import shap

mdl.fit(X_train, y_train)
explainer = shap.TreeExplainer(clf)
shap_values = explainer.shap_values(X_train)

#all records explained (both lines of code)
shap.summary_plot(shap_values, X_train)
shap.summary_plot(shap_values, X_train, plot_type = "bar")

df_shap = pd.DataFrame(list(zip(np.mean(shap_values, axis = 0), X_train.columns)),
                       columns = ['shap_mean', 'feature'])
df_shap = df_shap[['feature', 'shap_mean']]

df_shap['shap_mean_abs'] = np.absolute(df_shap['shap_mean'])
df_shap.sort_values(['shap_mean_abs'], ascending = False, inplace = True)
```

## Hyperband CV

```{python, eval = F}
#https://github.com/civisanalytics/civisml-extensions/blob/master/civismlext/hyperband.py

from civismlext.hyperband import HyperbandSearchCV
```

## CSV To DB

```{python, eval = F}
#source: http://bit.ly/294gmmP

import pandas as pd
import sqlite3

# pass in column names for each CSV
r_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']
ratings = pd.read_csv(fdata, sep=';', names=r_cols, compression='gzip')

m_cols = ['movie_id', 'title', 'genres']
movies = pd.read_csv(fitem, sep=';', names=m_cols, dtype={'title': object, 'genres': object})

sqlite_norm = "movielens-norm.sqlite"

if os.path.exists(sqlite_norm):
    os.unlink(sqlite_norm)

conn = sqlite3.connect(sqlite_norm)
conn.text_factory = str   # Shut up problems with Unicode
ratings.to_sql("ratings", conn)
movies.to_sql("movies", conn)
conn.close()
```

```{bash, eval = F}
sqlite3 db_name.db
.mode csv db_name
.import data.csv db_name
```

## Xgboost & Light GBM

```{python, eval = F}
import numpy as np
import pandas as pd
import time
import xgboost as xgb
import lightgbm as lgb
from scipy.stats import randint as sp_randint
from mlxtend.classifier import StackingClassifier
from skopt import BayesSearchCV
from skopt.space import Real, Integer, Categorical


param_grid_lgb = {
    'learning_rate': [0.005, 0.05, 0.1], 'n_estimators': [10, 100, 250, 500],
    'num_leaves': [6, 50, 100, 250, 500], 'boosting_type': ['gbdt', 'rf'],
    'colsample_bytree' : [0.65, 1], 'subsample': [0.7, 0.9],
    'reg_alpha': [0, 1.2], 'reg_lambda': [0, 2],
    }

param_grid_xgb = {'bootstrap': [True, False],
 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],
 'max_features': ['auto', 'sqrt'],
 'min_samples_leaf': [1, 2, 4],
 'min_samples_split': [2, 5, 10],
 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}

clf_tuned = sk_ms.RandomizedSearchCV(clf, param_grid, cv = 5, scoring = 'f1', n_jobs = -1)
```

## Bayesian CV

```{python, eval = F}
import numpy as np
import pandas as pd
import lightgbm as lgb
from scipy.stats import randint as sp_randint
from skopt import BayesSearchCV
from skopt.space import Real, Integer, Categorical

#v1: Light GBM

bayes_cv_tuner = BayesSearchCV(
    estimator = lgb.LGBMClassifier(
        objective='binary',
        metric='auc',
        n_jobs=1,
        verbose=0
    ),
    search_spaces = {
        'learning_rate': (0.01, 1.0, 'log-uniform'),
        'num_leaves': (1, 100),      
        'max_depth': (0, 50),
        'min_child_samples': (0, 50),
        'max_bin': (100, 1000),
        'subsample': (0.01, 1.0, 'uniform'),
        'subsample_freq': (0, 10),
        'colsample_bytree': (0.01, 1.0, 'uniform'),
        'min_child_weight': (0, 10),
        'subsample_for_bin': (100000, 500000),
        'reg_lambda': (1e-9, 1000, 'log-uniform'),
        'reg_alpha': (1e-9, 1.0, 'log-uniform'),
        'scale_pos_weight': (1e-6, 500, 'log-uniform'),
        'n_estimators': (50, 100),
    },    
    scoring = 'roc_auc',
    cv = sk_ms.StratifiedKFold(
        n_splits=3,
        shuffle=True,
        random_state = 42
    ),
    n_jobs = 3,
    n_iter = 10,   
    verbose = 0,
    refit = True,
    random_state = 42
)

result = bayes_cv_tuner.fit(X_train.values, y_train.values)
print(result.best_score_)
print(result.best_estimator_)

#v2: XGBoost

bayes_cv_tuner = BayesSearchCV(
    estimator = xgb.XGBClassifier(
        n_jobs = 1,
        objective = 'binary:logistic',
        eval_metric = 'auc',
        silent=1,
        tree_method='approx'
    ),
    search_spaces = {
        'learning_rate': (0.01, 1.0, 'log-uniform'),
        'min_child_weight': (0, 10),
        'max_depth': (0, 50),
        'max_delta_step': (0, 20),
        'subsample': (0.01, 1.0, 'uniform'),
        'colsample_bytree': (0.01, 1.0, 'uniform'),
        'colsample_bylevel': (0.01, 1.0, 'uniform'),
        'reg_lambda': (1e-9, 1000, 'log-uniform'),
        'reg_alpha': (1e-9, 1.0, 'log-uniform'),
        'gamma': (1e-9, 0.5, 'log-uniform'),
        'min_child_weight': (0, 5),
        'n_estimators': (50, 100),
        'scale_pos_weight': (1e-6, 500, 'log-uniform')
    },    
    scoring = 'roc_auc',
    cv = sk_ms.StratifiedKFold(
        n_splits=3,
        shuffle=True,
        random_state=42
    ),
    n_jobs = 3,
    n_iter = 10,   
    verbose = 0,
    refit = True,
    random_state = 42
)

#v3

param_grid = {
    'learning_rate': Real(0.005, 0.1), 'n_estimators': Integer(10, 500),
    'num_leaves': Integer(6, 50), 'boosting_type' : Categorical(['gbdt', 'rf']),
    'colsample_bytree' : Real(0.65, 1), 'subsample' : Real(0.7, 0.9),
    'reg_alpha' : Real(0, 1.2), 'reg_lambda' : Real(0, 2),
    }

clf_tuned = BayesSearchCV(clf, param_grid, cv = 5, random_state = 8, scoring = 'f1', n_jobs = -1)

```

## Pyspark

```{python, eval = FALSE}
import findspark
findspark.init()

import findspark
findspark.init()
import pyspark
import random

import pyspark
from pyspark.sql import SparkSession

from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler
from pyspark.ml import Pipeline
from pyspark.ml.classification import LogisticRegression
import pyspark.ml.evaluation as evals
import pyspark.ml.tuning as tune
import numpy as np

import pyspark
from pyspark.sql import SparkSession

sc = pyspark.SparkContext()
spark = SparkSession.builder.getOrCreate() # SparkSession.builder.appName('chosenName').getOrCreate()

sc = pyspark.SparkContext()
spark = SparkSession.builder.appName('example').getOrCreate()


sc = pyspark.SparkContext(appName="Pi")
num_samples = 100000000
def inside(p):     
    x, y = random.random(), random.random()
    return x*x + y*y < 1
count = sc.parallelize(range(0, num_samples)).filter(inside).count()
pi = 4 * count / num_samples
print(pi)

df = spark.read.csv('data.csv', header = True)
df.show(5)
df.columns

# Preprocessing

df = df.withColumn("label", df["target"].cast('integer'))

scf_indexer = StringIndexer(inputCol = "some_cat_feature", outputCol = "some_cat_feature_index")
scf_encoder = OneHotEncoder(inputCol = "some_cat_feature_index", outputCol = "some_cat_feature_fact")

feature_cols = ["some list of column names"]

vec_assembler = VectorAssembler(inputCols = feature_cols, 
                                outputCol = "features")

pipe = Pipeline(stages = [scf_indexer, scf_encoder, vec_assembler])

piped_data = pipe.fit(df).transform(df)
training, test = piped_data.randomSplit([.8, .2])

# Model

clf_lr = LogisticRegression()
evaluator = evals.BinaryClassificationEvaluator(metricName = "areaUnderROC")

grid = tune.ParamGridBuilder()
grid = grid.addGrid(clf_lr.regParam, np.arange(0, .1, .01))
grid = grid.addGrid(clf_lr.elasticNetParam, [0, 1])
grid = grid.build()

clf_lr_cv = tune.CrossValidator(
    estimator = clf_lr,
    estimatorParamMaps = grid,
    evaluator = evaluator,
    numFolds = 5
               )

best_clf_lr = clf_lr_cv.fit(training).bestModel
results = best_clf_lr.transform(training)
print(evaluator.evaluate(results))

## List tables
spark.catalog.listTables()

## Access and display data
query = "FROM flights SELECT * LIMIT 10"
flights10 = spark.sql(query)
flights10.show()

## Cast
df['g'] = df['g'].astype(str)

## Read from csv
df= spark.read.csv(file_path, header = True)
df = spark.read.csv('fileNameWithPath', mode="DROPMALFORMED",inferSchema=True, header = True)
df = spark.read.format("csv").option("header","true").option("inferSchema","true").load("john_doe.csv")

## Spark df to pandas and vice versa
toPandas()
spark_temp = spark.createDataFrame(pd_temp)

## Add to the catalog
spark_temp.createOrReplaceTempView("temp")

## Mutate
df = df.withColumn("newCol", df.oldCol + 1)
model_data = model_data.withColumn("arr_delay", model_data.arr_delay.cast('integer'))
model_data = model_data.withColumn("plane_age", model_data.year - model_data.plane_year)

## Create the DataFrame flights
flights = spark.table('flights')

## Get column names
spark_df.schema.names
spark_df.printSchema()

## Filter: takes either a Spark Column of boolean (True/False) values or the WHERE clause of a SQL expression as a string
long_flights1 = flights.filter('distance > 1000')
long_flights2 = flights.filter(flights.distance > 1000)
model_data = model_data.filter("arr_delay is not NULL and dep_delay is not NULL and air_time is not NULL and plane_year is not NULL")

## Groupby examples
flights.filter(flights.origin == "PDX").groupBy().min("distance").show()
flights.filter(flights.carrier=='DL').filter(flights.origin=='SEA').groupBy().avg('air_time').show()
flights.withColumn("duration_hrs", flights.air_time/60).groupBy().sum('duration_hrs').show()

## Spark functions
import pyspark.sql.functions as F
by_month_dest.agg(F.stddev('dep_delay')).show()

## Drop column
final_test_data.drop('State')

## Dummying

#The first step to encoding your categorical feature is to create a StringIndexer. Members of this class are #Estimators that take a DataFrame with a column of strings and map each unique string to a number. Then, the #Estimator returns a Transformer that takes a DataFrame, attaches the mapping to it as metadata, and returns a #new DataFrame with a numeric column corresponding to the string column.

#The second step is to encode this numeric column as a one-hot vector using a OneHotEncoder. This works exactly #the same way as the StringIndexer by creating an Estimator and then a Transformer

## Create a StringIndexer
carr_indexer = StringIndexer(inputCol="carrier", outputCol="carrier_index")

## Create a OneHotEncoder
carr_encoder = OneHotEncoder(inputCol="carrier_index", outputCol="carrier_fact")

## Make a VectorAssembler
vec_assembler = VectorAssembler(inputCols=["month", "air_time", "carrier_fact", "dest_fact", "plane_age"], outputCol="features")

## Import & Make Pipeline
from pyspark.ml import Pipeline
flights_pipe = Pipeline(stages=[dest_indexer, dest_encoder, carr_indexer, carr_encoder, vec_assembler])

## Fit and transform the data
piped_data = flights_pipe.fit(model_data).transform(model_data)

## Train-test split
training, test = piped_data.randomSplit([.6, .4])

## Tuning & Selection

## Import LogisticRegression
from pyspark.ml.classification import LogisticRegression

## Create a LogisticRegression Estimator
lr = LogisticRegression()

## Import the evaluation submodule
import pyspark.ml.evaluation as evals

## Create a BinaryClassificationEvaluator
evaluator = evals.BinaryClassificationEvaluator(metricName="areaUnderROC")

## Import the tuning submodule
import pyspark.ml.tuning as tune

## Create the parameter grid
grid = tune.ParamGridBuilder()

## Add the hyperparameter
grid = grid.addGrid(lr.regParam, np.arange(0, .1, .01))
grid = grid.addGrid(lr.elasticNetParam, [0, 1])

## Build the grid
grid = grid.build()

## Create the CrossValidator
cv = tune.CrossValidator(
estimator=lr,
estimatorParamMaps=grid,
evaluator=evaluator
               )

## Fit cross validation models
models = cv.fit(training)

## Extract the best model
best_lr = models.bestModel

## Use the model to predict the test set
test_results = best_lr.transform(test)

## Evaluate the predictions
print(evaluator.evaluate(test_results))

sc.stop()
```

## Sparklyr

```{r, eval = FALSE}
library(sparklyr)

sc <- spark_connect(master="local")
config <- spark_config()
config$spark.executor.cores <- 8
config$spark.executor.memory <- "25G"

flights <- copy_to(sc, flights, "flights")
src_tbls(sc)

iris_tbl %>%
  spark_apply(
    function(e) summary(lm(Petal_Length ~ Petal_Width, e))$r.squared,
    names = "r.squared",
    group_by = "Species")


## Connect to your Spark cluster
spark_conn <- spark_connect(master = "local")

## Print the version of Spark
spark_version(sc = spark_conn)

## Disconnect from Spark
spark_disconnect(sc = spark_conn)

## See tables
src_tbls(sc)

## Copy track_metadata to Spark
track_metadata_tbl <- copy_to(spark_conn, track_metadata)

## Print 5 rows, all columns
print(track_metadata_tbl, n = 5, width = Inf)

## Write and run SQL query
query <- "SELECT * FROM track_metadata WHERE year < 1935 AND duration > 300"
(results <- dbGetQuery(spark_conn, query))

## General transformation structure and example
a_tibble %>%
  ft_some_transformation("x", "y", some_other_args)

hotttnesss <- track_metadata_tbl %>%
  # Select artist_hotttnesss
  select(artist_hotttnesss) %>%
  # Binarize to is_hottt_or_nottt
  ft_binarizer('artist_hotttnesss', 'is_hottt_or_nottt', threshold = .5) %>%
  # Collect the result
  collect() %>%
  # Convert is_hottt_or_nottt to logical
  mutate(is_hottt_or_nottt = as.logical(is_hottt_or_nottt))
  
## Get and transform the schema

(schema <- sdf_schema(track_metadata_tbl))
schema %>%
  lapply(function(x) do.call(data_frame, x)) %>%
  bind_rows()

## Train-test split
partitioned <- track_metadata_tbl %>%
  sdf_partition(training = 0.7, testing = 0.3)

## List ml functions
ls("package:sparklyr", pattern = "^ml")

## GBT Example

gradient_boosted_trees_model <- track_data_to_model_tbl %>%
  # Run the gradient boosted trees model
   ml_gradient_boosted_trees('year', feature_colnames)

responses <- track_data_to_predict_tbl %>%
  # Select the year column
  select(year) %>%
  # Collect the results
  collect() %>%
  # Add in the predictions
  mutate(
    predicted_year = predict(
      gradient_boosted_trees_model,
      track_data_to_predict_tbl
    )
  )


spark_disconnect(sc)
```

## Parquet 

```{r, eval = FALSE}
## The parquet_dir has been pre-defined
parquet_dir

## List the files in the parquet dir
filenames <- dir(parquet_dir, full.names = TRUE)

## Show the filenames and their sizes
data_frame(
  filename = basename(filenames),
  size_bytes = file.size(filenames)
)

#Import the data into Spark

timbre_tbl <- spark_read_parquet(spark_conn, 'timbre', parquet_dir)
```

## Data Table

```{r, eval = FALSE}
# Operations done by reference

dt[i, j, by] # subset by i calculate by j grouped using by

DT[.N] # prints last row

names(DT) # colnames

dim(DT) # dimensions

DT[, .(A, B)] # returns two columns

DT[, c(A, B)] # returns a concatenated vector

DT[, .(sum_c = sum(C)] # mutate

DT[, plot(A, C)] # plot?

DT[, A := NULL] # Remove column A

DT[, .(sumB = sum(B)), by = .(Grp = A%%2)] # group_by & summarize

DT[, .N, by = Sepal.Width] # .N is the count of each group

DT[, lapply(.SD, median)] # .SD is a placeholder for all the columns

dogs[, lapply(.SD, mean), .SDcols = 2:3] # Find mean of columns 2 & 3

for (i in 1:5) set(DT, i, 3L, i+1) # update first 5 rows of 3rd column

setnames(DT, 'y', 'z') # changes colname from y to z

setkey(DT, A, B)

DT[.('b')]

DT[.(c('b', 'c'))]

DT[.(c('b', 'c')), mult="first"]

DT[c("b", "c"), .SD[c(1, .N)], by = .EACHI] # First and last row of the "b" and "c" groups

DT[c("b", "c"), { print(.SD); .SD[c(1, .N)] }, by = .EACH]

dt1[dt2, roll=-Inf, rollends=FALSE] # rolling join
```

## Keras 

The general framework: 1) instantiate, 2) add layers input-hidden-output, 3) compile, 4) fit

```{r, eval = F}

network <- keras_model_sequential() 

network %>%  
  layer_dense(units = 512, activation = "relu", input_shape = c(28 * 28)) %>%  
  layer_dense(units = 10, activation = "softmax") %>% 
  compile(optimizer = "rmsprop",  loss = "categorical_crossentropy",  metrics = c("accuracy")) %>% 
  fit(train_images, train_labels, epochs = 5, batch_size = 128)

metrics <- network %>% evaluate(test_images, test_labels)

```


```{python, eval = FALSE}
import keras
from keras.layers import Dense
from keras.models import Sequential

# Regression

# Specify the model: Two hidden layers
model = Sequential()

## Input
n_cols = predictors.shape[1]
model.add(Dense(50, activation='relu', input_shape = (n_cols,)))

# Hidden
model.add(Dense(32, activation='relu'))

# Output
model.add(Dense(1))

# Compile the model
model.compile(optimizer = 'adam', loss = 'mean_squared_error') 

# Fit the model
model.fit(predictors, target)

#Look at summary
model.summary()

# Calculate predictions: predictions
predictions = model.predict(pred_data)

# Classification

# Specify the model: Two hidden layers
n_cols = predictors.shape[1]
model = Sequential()

## Input
model.add(Dense(50, activation='relu', input_shape = (n_cols,)))

# Hidden
model.add(Dense(32, activation='relu'))

# Output
model.add(Dense(2, activation = 'softmax'))

# Compile the model
model.compile(optimizer = 'sgd', loss = 'categorical_crossentropy', metrics = ['accuracy'])

# Fit the model
model.fit(predictors, target)

#Look at summary
model.summary()

#Calculate predictions: predictions
predictions = model.predict(pred_data)

#Calculate predicted probability of survival
predicted_prob_true = predictions[:, 1]

# Tuning

# Import the SGD optimizer
from keras.optimizers import SGD

# Create list of learning rates: lr_to_test
lr_to_test = [.000001, 0.01, 1]

# Loop over learning rates
for lr in lr_to_test:
    print('Testing model with learning rate: ')
    
    # Build new model to test, unaffected by previous models
    model = get_new_model()
    
    # Create SGD optimizer with specified learning rate: my_optimizer
    my_optimizer = SGD(lr = lr)
    
    # Compile the model
    model.compile(optimizer = my_optimizer, loss = 'categorical_crossentropy')
    
    # Fit the model
    model.fit(predictors, 
              target, 
              validation_split = 0.3, 
              epochs = 20, 
              callbacks = [early_stopping_monitor], 
              verbose = False)
```
