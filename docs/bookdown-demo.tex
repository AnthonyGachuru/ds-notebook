\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Data Science Cribsheet},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}
  \title{Data Science Cribsheet}
  \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
  \author{}
  \preauthor{}\postauthor{}
  \date{}
  \predate{}\postdate{}

\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\chapter{Workflow}\label{workflow}

\section{Preamble}\label{preamble}

A typical data science workflow can be framed as following these steps:

\begin{itemize}
\item
  Business: start with a business question, define the goal and measure
  of success
\item
  Data: find, access and explore the data
\item
  Features: extract, assess and evaluate, select and sort
\item
  Models: find the right model for the problem at hand, compare,
  optimize, and fine tune
\item
  Communication: Interpret and communicate the results
\item
  Production: transform the code into production ready code, integrate
  into current ecosystem and deploy
\item
  Maintain: adapt the models and features to the evolution of the
  environment
\end{itemize}

More succinctly, as per Hadley Wickham:

import -\textgreater{} tidy -\textgreater{} understand {[} transform
\textless{}-\textgreater{} visualize \textless{}-\textgreater{} model
{]} -\textgreater{} communicate

My combination of the two:

preparation -\textgreater{} import -\textgreater{} tidy -\textgreater{}
understand {[} transform \textless{}-\textgreater{} visualize
\textless{}-\textgreater{} model {]} -\textgreater{} {[}communicate +
deploy + maintain{]}

Another note is that one way to structure projects is to write user
stories a la software engineering.

What lies below is a succinct version of a workflow. Theoretical
specifics can be found in other sections of this collection of
cribsheets, and libraries or packages to use not included in the Stack
can be found on this
\href{https://gfleetwood.github.io/noted-resources/data_science.html}{page}.

\section{Annotated Checklist}\label{annotated-checklist}

\subsection{Preparation}\label{preparation}

\begin{itemize}
\item
  Understand the business question, goals, and measures of success
\item
  See if the data to answer the question exists, and if it is useful.
\item
  Structure project as user stories
\item
  Initialize report to track and summarise work and to do list.
\item
  Remember to use atomic commit messages
\end{itemize}

\subsection{Import}\label{import}

\begin{itemize}
\item
  Character Encoding Precendence: utf-8, iso-8859-1, utf-16
\item
  Sampling from data if its too large:
  \href{https://pypi.python.org/pypi/subsample}{subsample package} in
  Python, \href{https://stackoverflow.com/a/22262726/6627726}{sqldf
  library} in R
\item
  \href{https://github.com/thombashi/sqlitebiter}{Sqlite Converter}
\item
  \href{https://dataset.readthedocs.io/en/latest/}{Easy Database
  Management}
\item
  \href{https://github.com/yhat/pandasql}{Writing sql in pandas}
\end{itemize}

\subsection{Tidy}\label{tidy}

\begin{itemize}
\item
  \href{http://vita.had.co.nz/papers/tidy-data.pdf}{Follow tidy data
  principles}. widyr, tidyr, dplyr in R and pandas in Python.
\item
  Figure out what you're trying to do with the data.
\item
  See what the data looks like: types of variables, the first and last
  few observations, missing values or outliers.
  \href{https://blog.datascienceheroes.com/x-ray-vision-on-your-datasets/}{xray},
  \href{https://github.com/ropenscilabs/skimr}{skimr},
  \href{https://github.com/ujjwalkarn/xda}{xda}, and
  \href{https://cran.r-project.org/web/packages/janitor/vignettes/introduction.html}{janitor}
  in R. pandas and scipy.stats.describe() in Python.
\item
  Outlier Detection: interquartile range, kernel density estimation,
  bonferroni test
\end{itemize}

\subsection{Transform}\label{transform}

\begin{itemize}
\item
  library(DataExplorer)
\item
  Look at missingness.
  \href{https://github.com/ResidentMario/missingno}{missingno} in
  Python.
  \href{https://rstudio-pubs-static.s3.amazonaws.com/4625_fa990d611f024ea69e7e2b10dd228fe7.html}{VIM},
  hmisc::naclus/naplot(),
  \href{https://github.com/njtierney/naniar}{naniar}, visdat::vis\_dat()
  in R.
\item
  Imputation. Options include the Mean, Mode, KNN, Random, and
  Regression. mice, hmisc, alice, and
  \href{https://www.rdocumentation.org/packages/zoo/versions/1.8-2/topics/na.locf}{zoo}
  in R.
\item
  An R library for
  \href{https://www.rdocumentation.org/packages/StatMatch/versions/1.2.5/topics/gower.dist}{categorical
  dissimilarity}
\item
  \href{https://github.com/VerbalExpressions/PythonVerbalExpressions}{Easy
  Regex}
\item
  Check for blank spaces and replace with NA. In R:
  \texttt{df{[}df==""{]}\ =\ NA}.
\item
  \href{https://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.stats.binned_statistic.html}{Binned
  Stats}
\item
  Anomalies: xda::num/catSummary(.) \textbar{} xray::anomalies(.)
\end{itemize}

\subsection{Visualize}\label{visualize}

\begin{itemize}
\item
  Quick Exploration Guide: S.O.C.S (Shape, Outlier, Center, Spread)
\item
  Trendlines \& Histograms
\item
  Confidence intervals
\item
  Compare the distributions of variables with overlayed density plots
\item
  Scatterplots: Pairwise and color/size the dots by other variables to
  try to identify any confounding relationships. GGally::ggpairs() or
  seaborn.
\item
  Dimensionality reduction (PCA, Kernel PCA, TSNE, Isomap) or
  hierarchical clustering for multivariate data to get a feel for high
  dimensional structure.
\end{itemize}

\subsection{Model}\label{model}

\begin{itemize}
\item
  Stick to rergression models for prescriptive analysis. Validate
  assumptions and tune appropriately. If using Python leverage
  statsmodels.
\item
  Pre-processing: vtreat in R.
\item
  Use binning + chisquare / mutual information to see correlation
  between feature and target.
\item
  Pipelines \& Feature Unions: sklearn in Python, recipes in R.
\item
  Feature Selection: Variance Threshold, Univariate Feature Selection
  (skl\_fs.chi2 \textbar{} f\_regression \textbar{} f\_classification in
  sklearn), Multivariate Feature Selection (SelectKBest,
  SelectPercentile, feature\_selection.RFECV in sklearn). See also the
  \href{https://github.com/EpistasisLab/scikit-rebate}{Rebate} and
  \href{https://github.com/scikit-learn-contrib/boruta_py}{Boruta}
  packages in Python.
\item
  Feature Engineering: Standardization, dimensionality reduction,
  dummying. sklearn.preprocessing.binarize, pandas.factorize, and
  featuretools for automation in Python.
\item
  Model Selection: yellowbrick,
  \href{https://edublancas.github.io/sklearn-evaluation/}{Sklearn
  Evaluation}, and
  \href{https://github.com/reiinakano/scikit-plot}{skl-plot} in Python.
\item
  Model Explanability: eli5 or
  \href{https://github.com/slundberg/shap}{SHAP} in Python but
  concentrate on the latter. SHAP also exists in R's xgboost library.
\item
  Tuning: Do bayesian tuning instead of grid or random search.
  hyperopt-sklearn, sklearn-deap, and scikit-optimize.
\item
  Deal with potential class imbalance: Gradient boosting or
  \href{http://contrib.scikit-learn.org/imbalanced-learn/stable/}{inbalanced
  learn}.
\item
  Create ensembles: mlxtend in Python
\item
  Choose Statistical Test:
  \href{http://www.ats.ucla.edu/stat/mult_pkg/whatstat/}{1} \textbar{}
  \href{http://www.qnamarkup.org/i/?source=http://colarusso.github.io/QnAMarkup/examples/source/WhatStats.txt}{2}
  \textbar{} infer library
\item
  Ensembling: mlxtend in Python, and caretEnsemble in R.
\item
  Look at learning curves:
  \href{https://www.dataquest.io/blog/learning-curves-machine-learning/}{1}
  \textbar{}
  \href{http://www.scikit-yb.org/en/latest/api/model_selection/learning_curve.html}{2}
\item
  Use SHAP for model explanability
\end{itemize}

\subsection{Communicate}\label{communicate}

Reference this
\href{https://www.dataquest.io/blog/data-science-project-style-guide/}{blog}.

\subsection{Deployment / Maintenance}\label{deployment-maintenance}

Write helper functions, do testin (
\href{http://engineering.pivotal.io/post/test-driven-development-for-data-science/}{1}
\textbar{} \href{http://www.tdda.info/}{2} \textbar{}
\href{http://stochasticsolutions.com/}{3} \textbar{}
\href{https://github.com/ericmjl/data-testing-tutorial}{4}) and
validation (\href{https://github.com/data-cleaning/validate}{1}
\textbar{} \href{https://rdrr.io/cran/checkmate/}{2} \textbar{}
\href{https://github.com/shawnbrown/datatest}{3} ). Testing libraries
include: pytest, hypothesis, and testthat.

A really useful feature is sharing environments so others can install
all the packages used in your code, with the correct versions. You can
save the packages to a YAML file with:

\texttt{conda\ env\ export\ \textgreater{}\ environment.yaml}

The first part conda env export writes out all the packages in the
environment, including the Python version. Above you can see the name of
the environment and all the dependencies (along with versions) are
listed. The second part of the export command, \textgreater{}
environment.yaml writes the exported text to a YAML file
environment.yaml. This file can now be shared and others will be able to
create the same environment you used for the project. To create an
environment from an environment file use:

\texttt{conda\ env\ create\ -f\ environment.yaml}

This will create a new environment with the same name listed in
environment.yaml.

\url{https://github.com/Quartz/bad-data-guide\#margin-of-error-is-too-large}

\url{https://github.com/ianozsvald/data_science_delivered}

\url{https://github.com/jtleek/datasharing}

\chapter{Import \& Tidy}\label{import-tidy}

\section{Import}\label{import-1}

\begin{itemize}
\item
  In tidy data:

  \begin{itemize}
  \tightlist
  \item
    Each variable forms a column.
  \item
    Each observation forms a row.
  \item
    Each type of observational unit forms a table.
  \end{itemize}
\item
  A schema is a blueprint for storing data. For a table every row has
  same number of columns, and each column is of the same type.
\item
  conda create -n yourenvname python=x.x anaconda
\item
  \href{http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.json.json_normalize.html}{json
  normalize}
\item
  \href{https://github.com/jreback/PyDataNYC2015/blob/master/whats-new-in-pandas/v0.16.x.ipynb}{Temporary
  pandas Columns}
\item
  Sampling a massive CSV file:

  \begin{itemize}
  \item
    pip install subsample
  \item
    gzcat ginormous.csv.gz \textbar{} subsample -n 100000 \textgreater{}
    sampled.csv.gz
  \end{itemize}
\item
  Codebook
\item
  Data file naming convention:
  \{iso-date\}\_\{concise\_description\}.\{file\_extension\}
\item
  Variable naming conventions: Consistency and balance are important
\end{itemize}

\subsection{SQL}\label{sql}

\begin{itemize}
\item
  CSV to DB V1:

  \begin{itemize}
  \tightlist
  \item
    \$ sqlite3 db\_name.db
  \item
    sqlite\textgreater{} .mode csv db\_name
  \item
    sqlite\textgreater{} .import data.csv db\_name
  \end{itemize}
\item
  \href{https://gist.github.com/gfleetwood/c2ea91da4a8ab1a77f777e28e0b2949c}{CSV
  to DB V2}
\item
  SQL UNION stacks one dataset on top of the other. It only appends
  distinct values. More specifically, when you use UNION, the dataset is
  appended, and any rows in the appended table that are exactly
  identical to rows in the first table are dropped. If you'd like to
  append all the values from the second table, use UNION ALL.
\item
  List tables: SELECT name FROM sqlite\_master WHERE type=`table';
  {[}sqllite{]}
\item
  Show columns: .headers ON {[}sqllite{]}
\item
  ROW\_NUMBER(), RANK(), DENSE\_RANK(), NOW(), INTERVAL, x IS (NOT) NULL
\item
  COUNT(DISTINCT month): returns count of non-null values like table in
  R
\item
  The CASE statement is followed by at least one pair of WHEN and THEN
  statements: CASE WHEN year = `SR' THEN `yes' ELSE NULL END. You can
  also define a number of outcomes in a CASE statement by including as
  many WHEN/THEN statements as you'd like
\item
  If you include two (or more) columns in a SELECT DISTINCT clause, your
  results will contain all of the unique pairs of those two columns
\end{itemize}

\subsection{Scraping}\label{scraping}

\begin{itemize}
\item
  Access robots.txt: website\_url.domain/robots.txt
\item
  Tips: 1) Use bot net, 2) user agent switching
\item
  Try polling in your wait about every 500 milliseconds. Also use
  presence of element located. I generally have better luck with that
  expected condition than visibility. Also, submit the search using the
  search button.
\item
  Selenium: save\_screenshot, forward(), back(), maximize(), use
  action\_chains to do stuff like scroll find\_element\_by\_link\_name,
  element\_to\_be\_clickable, visibility\_of\_element\_located
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ pyvirtualdisplay }\ImportTok{import}\NormalTok{ Display  }
\ImportTok{from}\NormalTok{ selenium }\ImportTok{import}\NormalTok{ webdriver  }
\NormalTok{display }\OperatorTok{=}\NormalTok{ Display(visible}\OperatorTok{=}\DecValTok{0}\NormalTok{, size}\OperatorTok{=}\NormalTok{(}\DecValTok{800}\NormalTok{, }\DecValTok{600}\NormalTok{))  }
\NormalTok{display.start()  }
\NormalTok{browser }\OperatorTok{=}\NormalTok{ webdriver.Firefox()  }
\NormalTok{browser.get(}\StringTok{'http://www.google.com'}\NormalTok{)  }
\BuiltInTok{print}\NormalTok{(browser.title)}
\end{Highlighting}
\end{Shaded}

\section{Tidy}\label{tidy-1}

\begin{itemize}
\item
  Tidy Data:
  \href{https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html}{R}
  \textbar{}
  \href{https://github.com/jfpuget/Tidy-Data/blob/master/Tidy-Data.ipynb}{Py1}
  \textbar{}
  \href{http://www.jeannicholashould.com/tidy-data-in-python.html}{Py2}
\item
  import pandas\_profiling; pandas\_profiling.ProfileReport(data)
\item
  \href{https://stats.stackexchange.com/questions/798/calculating-optimal-number-of-bins-in-a-histogram/862\#862}{Optimal
  Bins For Histogram}
\item
  \textbf{The Coefficient of Unalikeability}: (Unalikeability is defined
  as how often observations differ from one another). It varies from 0
  to 1. The higher the value, the more unalike the data are.
\end{itemize}

\chapter{Transform \& Visualize}\label{transform-visualize}

\section{Transform}\label{transform-1}

\subsection{General}\label{general}

\begin{itemize}
\tightlist
\item
  There is no one rule about when a number is not accurate enough to
  use, but as a rule of thumb, you should be cautious about using any
  number with a MOE (Margin-of-error) over 10\%.
\end{itemize}

A good formula is: if your sample is picked uniformly at random and is
of size at least:

\(\frac{-log(\frac{d}{2})}{2e^2}\)

then with probability at least 1-d your sample measured proportion q is
no further away than e from the unknown true population proportion p
(what you are trying to estimate). Need to estimate with 99\% certainty
the unknown true population rate to +- 10\% precision? That is d=0.01,
e=0.1 and a sample of size 265 should be enough. A stricter precision of
+-0.05 causes the estimated sample size to increase to 1060, but
increasing the certainty requirement to 99.5\% only increases the
estimated sample size to 300.

\begin{itemize}
\item
  use spatial sign to transform data with outliers.
\item
  Too many levels for a category?

  \begin{itemize}
  \tightlist
  \item
    limit number of categories through feature selection
  \item
    \url{https://stats.stackexchange.com/questions/95212/improve-classification-with-many-categorical-variables}
  \item
    bucket groups by number of samples
  \end{itemize}
\item
  \url{https://cran.r-project.org/web/packages/vtreat/index.html}
\item
  Long story short, if these outliers are really such (i.e.~they appear
  with a very low frequency and very likely are bad/random/corrupted
  measurements) and they do not correspond to potential events/failures
  that your model should be aware of, you can safely remove them. In all
  other cases you should evaluate case by case what those outliers
  represent.
\item
  Otherwise, assuming levels of the categorical variable are ordered,
  the polyserial correlation (here it is in R), which is a variant of
  the better known polychoric correlation. Note the latter is defined
  based on the correlation between the numerical variable and a
  continuous latent trait underlying the categorical variable.
\item
  Use runif to do sampling
\item
  You should use a logarithmic scale when percent change, or change in
  orders of magnitude, is more important than changes in absolute units.
  You should also use a log scale to better visualize data that is
  heavily skewed. Taking the logarithm only works if the data is
  non-negative. There are other transforms, such as arcsinh or signed
  log, that you can use to decrease data range if you have zero or
  negative values.
\item
  Normalizing by mean and standard deviation is most meaningful when the
  data distribution is roughly symmetric.
\item
  Monetary amounts are often
  log-\url{https://github.com/ColinFay/erum2018normally}
  distributed---the log of the data is normally distributed. This leads
  us to the idea that taking the log of the data can restore symmetry to
  it.
\end{itemize}

\subsection{Imputation}\label{imputation}

\subsection{Missingness \& Imputation}\label{missingness-imputation}

There are three types of missingness: 1) Completely at Random, 2) At
Random, 3) Not at random

\begin{itemize}
\item
  The pros of imputation: 1) Helps retain a larger sample size of your
  data, 2) Does not sacrifice all the available information in an
  observation because of sparse missingness, 3) Can potentially avoid
  unwanted bias.
\item
  The cons of imputation: 1) The standard errors of any estimates made
  during analyses following imputation can tend to be too small, 2) The
  methods are under the assumption that all measurements are actually
  ``known,'' when in fact some were imputed, 3) Can potentially induce
  unwanted bias.
\item
  Other: 1) dplyr::case\_when, 2) mean of each column = df.mean(axis =
  0) \textbar{}\textbar{} df.mean(axis = `index' ), 3) mean of each row
  = df .mean(axis = 1) \textbar{}\textbar{} df .mean(axis = `columns' )
\end{itemize}

Some solutions:

\begin{itemize}
\item
  Mean: Simple but can distort distribution, underestimate standard
  deviation, and distort variable relationships by dragging correlation
  to zero.
\item
  Random: Can amplify outlier observation and induce bias.
\item
  Regression: Use observed variables and relationships between these
  variables. Must make assumptions and can badly extrapolate.
\end{itemize}

\section{Visualize}\label{visualize-1}

\begin{itemize}
\item
  logging converts multiplicative relationships to additive
  relationships, and by the same token it converts exponential (compound
  growth) trends to linear trends. By taking logarithms of variables
  which are multiplicatively related and/or growing exponentially over
  time, we can often explain their behavior with linear models.
\item
  swarmplot
\item
  Rule of thumb for bins in histogram:
  \texttt{ggplot()\ +\ geom\_histogram(aes(x),\ binwidth\ =\ diff(range(x))\ /\ (2\ *\ IQR(x)\ /\ length(x)\^{}(1/3)))}.
\end{itemize}

\chapter{Model}\label{model-1}

\section{General}\label{general-1}

\begin{itemize}
\item
  Model Template: intro/packages, fs/fe, tuning, pros/cons, notes
\item
  Don't use brier loss for ordinal predictions
\item
  brier score: .25 if say .5 and .33 if randomly assign probs
\item
  R uses class encoded as 1 in classification to make predictions. Use
  levels() function on factor to determine what 1 is.
\item
  20 observations per feature is pretty good for making predictions.
\item
  y = b\_0 + b\_1x\_1 + b\_2x\_2 + b\_3x\_1x\_2. ( b\_1 + b\_2x\_2 ) is
  the increase in y with a unit increase in x\_1.
\item
  \url{http://ml-cheatsheet.readthedocs.io/en/latest/index.html}
\item
  Algorithms for Rec Systems: Linear/Logistic/Elastic Net Regression,
  Restricted Boltzmann Machines, Singular Value Decomposition, Makov
  Chains, Latent Dirichlet Allocation, Association Rules, Gradient
  Boosted Decision Trees, Random Forests, Affinity Propagation, K-Means,
  Matrix Factorization, Alternating Least Squares.
\item
  For example, multilevel models themselves may be referred to as
  hierarchical linear models, random effects models, multilevel models,
  random intercept models, random slope models, or pooling models.
\item
  \url{https://www.listendata.com/2018/03/regression-analysis.html}
\item
  In general, raising the classification threshold reduces false
  positives, thus raising precision.
\item
  Multi-Modal Classification: In the machine learning community, the
  term Multi-Modal is used to refer to multiple kinds of data. For
  example, consider a YouTube video. It can be thought to contain 3
  different modalities: 1) The video frames (visual modality), 2) The
  audio clip of what's being spoken (audio modality), 3) Some videos
  also come with the transcription of the words spoken in the form of
  subtitles (textual modality)
\item
  Less samples and more features increase the chance of overfitting.
\item
  You can choose either of the following inference strategies: offline
  inference, meaning that you make all possible predictions in a batch,
  using a MapReduce or something similar. You then write the predictions
  to an SSTable or Bigtable, and then feed these to a cache/lookup
  table. online inference, meaning that you predict on demand, using a
  server.
\item
  A static model is trained offline. That is, we train the model exactly
  once and then use that trained model for a while. A dynamic model is
  trained online. That is, data is continually entering the system and
  we're incorporating that data into the model through continuous
  updates.
\item
  How would multiplying all of the predictions from a given model by 2.0
  (for example, if the model predicts 0.4, we multiply by 2.0 to get a
  prediction of 0.8) change the model's performance as measured by AUC?
  No change. AUC only cares about relative prediction scores. Yes, AUC
  is based on the relative predictions, so any transformation of the
  predictions that preserves the relative ranking has no effect on AUC.
  This is clearly not the case for other metrics such as squared error,
  log loss, or prediction bias (discussed later).
\item
  AUC is desirable for the following two reasons: AUC is
  scale-invariant. It measures how well predictions are ranked, rather
  than their absolute values. UC is classification-threshold-invariant.
  It measures the quality of the model's predictions irrespective of
  what classification threshold is chosen. However, both these reasons
  come with caveats, which may limit the usefulness of AUC in certain
  use cases: Scale invariance is not always desirable. For example,
  sometimes we really do need well calibrated probability outputs, and
  AUC won't tell us about that. Classification-threshold invariance is
  not always desirable. In cases where there are wide disparities in the
  cost of false negatives vs.~false positives, it may be critical to
  minimize one type of classification error. For example, when doing
  email spam detection, you likely want to prioritize minimizing false
  positives (even if that results in a significant increase of false
  negatives). AUC isn't a useful metric for this type of optimization.
\item
  Raising our classification threshold will cause the number of true
  positives to decrease or stay the same and will cause the number of
  false negatives to increase or stay the same. Thus, recall will either
  stay constant or decrease.
\item
  \url{https://www.dataquest.io/blog/learning-curves-machine-learning/}
\item
  A feature cross is a synthetic feature formed by multiplying
  (crossing) two or more features. Crossing combinations of features can
  provide predictive abilities beyond what those features can provide
  individually.
\item
  Bias: Error Introduced by approximating real-life problem by using a
  simple method
\item
  Imagine a linear model with two strongly correlated features; that is,
  these two features are nearly identical copies of one another but one
  feature contains a small amount of random noise. If we train this
  model with L2 regularization, what will happen to the weights for
  these two features? L2 regularization will force the features towards
  roughly equivalent weights that are approximately half of what they
  would have been had only one of the two features been in the model.
\item
  L2 regularization will encourage many of the non-informative weights
  to be nearly (but not exactly) 0.0.
\item
  F1 Score = TP/(TP + FP + FN). Essentially how good you are at
  identifying the positive class.
\item
  Active Learning: Finding the optimal data to manually label
\item
  Simulated Annealing: Gradient descent extension
\item
  A variable that is completely useless by itself can provide a
  significant performance improvement when taken with others.
\item
  So, if your problem involves kind of searching a needle in the
  haystack when for ex: the positive class samples are very rare
  compared to the negative classes, use a precision recall curve.
  Otherwise use a ROC curve because a ROC curve remains the same
  regardless of the baseline prior probability of your positive class
  (the important rare class).
\item
  Variance: The amount the model output will change if the model was
  trained using a different data
\item
  Flexible Methods: higher variance, low bias, Example: KNN (k = 1)
\item
  Inflexible Methods: low variance, high bias, Example: Linear
  regression
\item
  Sequence mining Algorithms: spade, gsp, freespan; hmmlearn
\item
  Network models: graphical lasso, ising model, granger causality test,
  network hypothesis testing, bayesian networks
\item
  Parametric: Model has a function shape and form, Model is trained/fit
  using training data to determine parameter values, reduces the problem
  of training a model down to training a set of parameter values,
  Examples: linear regression
\item
  Non-Parametric: No assumption about model form is made, Example: KNN
\item
  Set Random state by hand in sklearn.
\item
  Generally need 3 rows of data per variable (to prevent model from
  seeing signal where there is only noise) {[}Nina Zumel{]}
\item
  Domain knowledge for feature selection and random forest feature
  importance (5-10) best variables. {[}Nina Zumel{]}
\item
  Because we have a couple thousand data points, even though salary can
  best be represented by a Poisson distribution, I'm going to use
  Gaussian distribution. (With bigger datasets, these two distribution
  start to become very similar).
\item
  We can also tell that our input, smart\_1\_normalized, doesn't have a
  very strong relationship to our output because the standard error
  (0.009) is more than 1/10 of our estimate (0.02).
\item
  The less complex an ML model, the more likely that a good empirical
  result is not just due to the peculiarities of the sample.
\item
  The ML Fine Print: Three basic assumptions: 1) We draw examples
  independently and identically (i.i.d.) at random from the
  distribution, 2) The distribution is stationary: It doesn't change
  over time, 3) We always pull from the same distribution: Including
  training, validation, and test sets. In practice, we sometimes violate
  these assumptions. For example: 1) Consider a model that chooses ads
  to display. The i.i.d. assumption would be violated if the model bases
  its choice of ads, in part, on what ads the user has previously seen.
  2) Consider a data set that contains retail sales information for a
  year. User's purchases change seasonally, which would violate
  stationarity. When we know that any of the preceding three basic
  assumptions are violated, we must pay careful attention to metrics.
\item
  empirical risk minimization: In supervised learning, a machine
  learning algorithm builds a model by examining many examples and
  attempting to find a model that minimizes loss.
\item
  Loss is the penalty for a bad prediction. That is, loss is a number
  indicating how bad the model's prediction was on a single example. If
  the model's prediction is perfect, the loss is zero; otherwise, the
  loss is greater.
\end{itemize}

\section{Pre-processing}\label{pre-processing}

\section{Feature Engineering}\label{feature-engineering}

\begin{itemize}
\tightlist
\item
  Dealing with cyclical features: Hours of the day, days of the week,
  months in a year, and wind direction are all examples of features that
  are cyclical. Many new machine learning engineers don't think to
  convert these features into a representation that can preserve
  information such as hour 23 and hour 0 being close to each other and
  not far. Keeping with the hour example, the best way to handle this is
  to calculate the sin and cos component so that you represent your
  cyclical feature as (x,y) coordinates of a circle. In this
  representation hour, 23 and hour 0 are right next to each other
  numerically, just as they should be.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{df[}\StringTok{'hr_sin'}\NormalTok{] }\OperatorTok{=}\NormalTok{ np.sin(df.hr}\OperatorTok{*}\NormalTok{(}\DecValTok{2}\NormalTok{.}\OperatorTok{*}\NormalTok{np.pi}\OperatorTok{/}\DecValTok{24}\NormalTok{))}
\NormalTok{df[}\StringTok{'hr_cos'}\NormalTok{] }\OperatorTok{=}\NormalTok{ np.cos(df.hr}\OperatorTok{*}\NormalTok{(}\DecValTok{2}\NormalTok{.}\OperatorTok{*}\NormalTok{np.pi}\OperatorTok{/}\DecValTok{24}\NormalTok{))}
\NormalTok{df[}\StringTok{'mnth_sin'}\NormalTok{] }\OperatorTok{=}\NormalTok{ np.sin((df.mnth}\OperatorTok{-}\DecValTok{1}\NormalTok{)}\OperatorTok{*}\NormalTok{(}\DecValTok{2}\NormalTok{.}\OperatorTok{*}\NormalTok{np.pi}\OperatorTok{/}\DecValTok{12}\NormalTok{))}
\NormalTok{df[}\StringTok{'mnth_cos'}\NormalTok{] }\OperatorTok{=}\NormalTok{ np.cos((df.mnth}\OperatorTok{-}\DecValTok{1}\NormalTok{)}\OperatorTok{*}\NormalTok{(}\DecValTok{2}\NormalTok{.}\OperatorTok{*}\NormalTok{np.pi}\OperatorTok{/}\DecValTok{12}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  robust scaler python: median \& quantiles
\end{itemize}

\section{Feature Selection}\label{feature-selection}

\begin{itemize}
\item
  Variance Threshold: Use this to exclude features that don't meet a
  variance threshold.
\item
  Univariate Feature Selection: For classification can use chi-squared
  and F-Test while F-Test for regression. Functions: chi2,
  f\_regression, f\_classification from sklearn.feature\_selection.
\item
  SelectKBest/SelectPercentile: Keep the k highest scoring features and
  keep a user-specified highest scoring percentage of features. Can use
  the univariate feature selection in these functions.
\item
  sklearn.feature\_selection.RFE(CV)
\item
  To work around magic values, convert the feature into two features:
  One feature holds only quality ratings, never magic values. One
  feature holds a boolean value indicating whether or not a
  quality\_rating was supplied. Give this boolean feature a name like
  is\_quality\_rating\_defined.
\item
  Make sure that your test set meets the following two conditions: 1) Is
  large enough to yield statistically meaningful results, 2) Is
  representative of the data set as a whole. In other words, don't pick
  a test set with different characteristics than the training set.
\item
  Handling extreme outliers: Given data with a very long tail, log
  scaling does a slightly better job, but there's still a significant
  tail of outlier values. Let's pick yet another approach. What if we
  simply ``cap'' or ``clip'' the maximum value at an arbitrary value,
  say 4.0? Clipping the feature value at 4.0 doesn't mean that we ignore
  all values greater than 4.0. Rather, it means that all values that
  were greater than 4.0 now become 4.0. This explains the funny hill at
  4.0. Despite that hill, the scaled feature set is now more useful than
  the original data. Binning.
\item
  Know your data. Follow these rules: 1) Keep in mind what you think
  your data should look like. 2) Verify that the data meets these
  expectations (or that you can explain why it doesn't). 3) Double-check
  that the training data agrees with other sources (for example,
  dashboards).
\end{itemize}

\section{Other}\label{other}

\subsection{Unbalanced Classes}\label{unbalanced-classes}

Research on imbalanced classes often considers imbalanced to mean a
minority class of 10\% to 20\%.

That said, here is a rough outline of useful approaches. These are
listed approximately in order of effort:

Do nothing. Sometimes you get lucky and nothing needs to be done. You
can train on the so-called natural (or stratified) distribution and
sometimes it works without need for modification.

Balance the training set in some way: Oversample the minority class.
Undersample the majority class. Synthesize new minority classes.

Throw away minority examples and switch to an anomaly detection
framework.

At the algorithm level, or after it: Adjust the class weight
(misclassification costs). Adjust the decision threshold. Modify an
existing algorithm to be more sensitive to rare classes.

Construct an entirely new algorithm to perform well on imbalanced data.

Use AUC, F1 Score, Cohen's Kappa, ROC curve, a precision-recall curve, a
lift curve, or a profit (gain) curve for evaluation.

Use probability estimates and not the default .5 threshold.

Undersampling, oversampling, increase weight of minority class

Decision trees often perform well on imbalanced datasets because their
hierarchical structure allows them to learn signals from both classes.

Reframe as Anomaly Detection

\begin{itemize}
\tightlist
\item
  Sklearn Eg: wclf = svm.SVC(kernel=`linear', class\_weight=\{1: 10\})
  \& svm.SVC(kernel=`linear', class\_weight=`balanced')
\end{itemize}

\chapter{Model}\label{model-2}

\section{Linear Regression}\label{linear-regression}

\subsection{Intro}\label{intro}

\begin{itemize}
\item
  The standard least squares coefficient estimates are scale
  equivariant: if we multiply a predictor variable by a constant, the
  corresponding least squares coefficient estimate will be scaled down
  by the same constant.
\item
  Used when you're predicting a continuous value.
\item
  In regression, it is often recommended to center the variables so that
  the predictors have mean 0. This makes it so the intercept term is
  interpreted as the expected value of Y\_i when the predictor values
  are set to their means. Otherwise, the intercept is interpreted as the
  expected value of Y\_i when the predictors are set to 0, which may not
  be a realistic or interpretable situation.
\item
  When p \textgreater{} n, we can no longer calculate a unique least
  square coefficient estimate, the variances become infinite, so OLS
  cannot be used at all.
\item
  In presence of few variables with medium / large sized effect, use
  lasso regression. In presence of many variables with small / medium
  sized effect, use ridge regression.
\item
  The fitted regression line using a sample of data gives imperfect
  predictions for future observations due to sampling variability and
  randomness in Y that is not related to X.
\item
  Linear Correlation =\textgreater{} Causation if covariates respect:
  linearity normality indep homoscedasticity + all confounders in model
\item
  In summary, ``pooling'' your data and fitting one combined regression
  function allows you to easily and efficiently answer research
  questions concerning the binary predictor variable.
\item
  If two independent variables are measured in exactly the same units,
  we can asses their relative importance in their effect on y quite
  simply: the larger the coefficient, the stronger the effect.
\item
  Okay, so many of the features are strongly correlated. We can make use
  of this in our model by including polynomial features
  (e.g.~PolynomialFeatures(degree=2) from scikit-learn). Adding these
  brings our validation loss down to 0.69256 (-0.05\% from baseline).
\item
  We prefer natural logs (that is, logarithms base e) because, as
  described above, coefficients on the natural-log scale are directly
  interpretable as approximate proportional differences: with a
  coefficient of 0.06, a difference of 1 in x corresponds to an
  approximate 6\% difference in y, and so forth.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ statsmodels.api }\ImportTok{as}\NormalTok{ sm}
\NormalTok{X }\OperatorTok{=}\NormalTok{ sm.add_constant(X)}
\NormalTok{rgr_lr }\OperatorTok{=}\NormalTok{ sm.OLS(Y, X).fit(disp}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(results.summary())}
\end{Highlighting}
\end{Shaded}

\subsection{Assumptions}\label{assumptions}

The regression assumption that is generally least important is that the
errors are normally distributed. In fact, for the purpose of estimating
the regression line (as compared to predicting individual data points),
the assumption of normality is barely important at all. Thus, in
contrast to many regression textbooks, we do not recommend diagnostics
of the normality of regression residuals. {[}Gelman{]}

Linear regression errors are not autocorrelated. The Durbin-Watson
statistic detects this; if it is close to 2, there is no
autocorrelation.

A linear regression model is only as good as the validity of its
assumptions, which can be summarized as: \(y = b_0+b_1x + \epsilon\),
\(\epsilon\) is i.i.d from N(0, \(\sigma\)). More verbosely:

\begin{itemize}
\item
  Linearity: This is a linear relationship between the predictor and the
  response variables. If this relationship is not clearly present,
  transformations (log, polynomial, exponent and so on) of the X or Y
  may solve the problem. Check with a scatterplot.
\item
  Normality: The error terms are drawn from an identical Gaussian
  distribution, i.e a normal distribution of the dependent variable for
  each value of the independent variable. Inspect the quantile-quantile
  plot of the residuals. Also Shapiro-Wilk Test for Normality.
\item
  Non-correlation or independence of errors: A common problem in the
  time series and panel data where en = betan-1; if the errors are
  correlated, you run the risk of creating a poorly specified model. The
  residual value for an arbitrary observation is not predictable from
  knowledge of another observation's residual value; they are
  uncorrelated. Inspect the residual plot after fitting the regression.
  Ideally, there would be no clear pattern in the fluctuation of the
  error terms.
\item
  Homoscedasticity (constant variance): Normally the distributed and
  constant variance of errors, which means that the variance of the
  errors is constant across the different values of inputs. Violations
  of this assumption can create biased coefficient estimates, leading to
  statistical tests for significance that can be either too high or too
  low. This, in turn, leads to the wrong conclusion. This violation is
  referred to as heteroscedasticity. The error terms have the same
  variance. Check the residual plot which is a scatterplot of the
  residuals vs the fitted values.
\item
  No multi-colinearity: No linear relationship between two predictor
  variables, which is to say that there should be no correlation between
  the features. This, again, can lead to biased estimates. If not
  coefficient estimates could be unstable, introduce redundancies within
  predictors, and make it more difficult to make inferences. Could
  inflate standard errors, decrease power and reliability of regression
  coefficients, and create need for a larger sample size. Check the
  variance inflation factors. Tells the factor by which the estimated
  variance of a variable is larger in comparison ti it it were
  completely uncorrelated with other variables in the model. If the VIF
  is more than five then the variable is a candidate to remove from the
  model. Its information is contained in the other variables. Variance
  Inflation Factor: MEasures how the model facotrs influence hte
  uncertainity of coefficient estimates. \textgreater{}=5 is bad.
\item
  Minimal outliers: Outliers can severely skew the estimation and,
  ideally, must be removed prior to fitting a model using linear
  regression; this again can lead to a biased estimate.
\end{itemize}

A practical
\href{http://www.statmethods.net/stats/rdiagnostics.html}{summary} using
R.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# general check}
\KeywordTok{plot}\NormalTok{(rgr_lr_st1)}

\CommentTok{# linearity check}
\KeywordTok{avPlots}\NormalTok{(rgr_lr_st1)}

\CommentTok{# repeat of outlier analysis}
\CommentTok{#influencePlot(rgr_lr_st1)}

\CommentTok{# multi-colinearity}
\KeywordTok{vif}\NormalTok{(model) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{data.frame}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{VIF =} \StringTok{`}\DataTypeTok{GVIF..1..2.Df..}\StringTok{`}\NormalTok{, }\DataTypeTok{VIF =}\NormalTok{ VIF}\OperatorTok{^}\DecValTok{2}\NormalTok{)}

\CommentTok{# Test for Autocorrelated Errors}
\KeywordTok{durbinWatsonTest}\NormalTok{(rgr_lr_st1)}

\KeywordTok{set.seed}\NormalTok{(}\DecValTok{0}\NormalTok{)}

\NormalTok{data =}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{) }\CommentTok{# Generating data from a standard normal distribution.}
\KeywordTok{shapiro.test}\NormalTok{(data) }\CommentTok{# P-value insignificant; retain H0, conclude data is normally distributed.}

\CommentTok{# Inspect this same data using the QQ-plots (For normal data, the QQ-plot produces a straight-line relationship. For uniform data, the QQ-plot does NOT produce a straight-line relationship): }
\KeywordTok{qqnorm}\NormalTok{(normal.data) }\OperatorTok{+}\StringTok{ }\KeywordTok{qqline}\NormalTok{(normal.data)}
\CommentTok{#Breusch-Pagan (BP) test lmtest package: is used to test for heteroskedasticity in a linear regression model}
\end{Highlighting}
\end{Shaded}

Outliers are observations that have high residual values. Leverage
points are observations that have unusually small or large independent
variable values. Cook's distance helps to measure the effect of deleting
an observation from the \#dataset and rerunning the regression.
Observations that have large residual values and also high leverage tend
to pose threats to the accuracy of the regression line and thus need to
be further investigated. Look at influence plot. Confidence and
prediction intervals. For avplot distinct patterns are indications of
good contributions. Influence plot to look at hat values (lower is
better). Use F-test to compare models. F-test \& partial f-testr for
simple and multiple linear regression.

Shapiro-Wilk Test for Normality: H\_0 says the data is normally
distributed while H\_a says it is not. To run this test in R, the syntax
is quite simple:

And a guide for python.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{statsmodels.stats.outliers_influence.variance_inflation_factor}
\end{Highlighting}
\end{Shaded}

Fixing assumptions:

Transformations: Can remedy assumption violations and strengthen linear
relationships between variables, but can lead to overfitting and less
interpretability. square root correction: Take square root of x
variable(s). log transformation: Take log of y. General rule of thumb:
powers \textgreater{} 1 for skewed left data, powers \textless{} 1 for
data skewed right.

Box-Cox: Iterates along values of lambda by maximum likelihood so as to
maximize the fit of the transformed variable to a normal distribution.
Does not guarantee normality since it minimizes standard deviation. Can
only be used on positive values. Use on negative values by shifting by a
constant value. Use Box-Cox transformations when predictions are skewed.

\subsection{Selection}\label{selection}

\begin{itemize}
\item
  Forward and Backward Stepwise Selection.
\item
  For simple linear regression, the principal hypothesis test is as
  follows: H\_0 : ? 1 = 0, H\_A: ? 1 ≠ 0. If H\_0 is true then the
  population mean of Y would be ? 0 no matter what the value of X. (X
  has no effect on Y.)
\item
  F-Test: H\_0 says that all/some coefficients are zero versus H\_a
  which says that there is at least one non-zero coefficient. If h0 fail
  to reject than F value close to 1, else F value greater than 1.
\item
  A.I.C and B.I.C: Smaller values are better. Can be used to compare
  multiple models at the same time. They reward goodness of fit and
  penalizes complexity. Favor A.I.C for prediction and B.I.C for
  descriptive power (penalty term more stringest and favors a
  parsimonious model)
\item
  R-squared increases whenever a predictor is added, and it doesn't take
  model complexity into consideration which makes it prone to
  overfitting. Use adjusted R-squared instead. Additional predictors
  only make this increase is they are statistically significant. It's
  easy to show that given no other data for a set of numbers, the
  predictor that minimises the MAE is the median, while the predictor
  that minimises the MSE is the mean.
\item
  Compare RSME to sd of data for sense of how good it is. (If RSME
  \textless{} sd then good.)
\end{itemize}

\subsection{Pros/Cons}\label{proscons}

\textbf{Pros}

\begin{itemize}
\item
  Very fast (runs in constant time)
\item
  Easy to understand the model
\item
  Less prone to overfitting
\end{itemize}

\textbf{Cons}

\begin{itemize}
\item
  Unable to model complex relationships
\item
  Unable to capture nonlinear relationships without first transforming
  the inputs
\end{itemize}

\subsection{Other}\label{other-1}

\begin{itemize}
\item
  \href{https://en.wikipedia.org/wiki/Quantile_regression}{Quantile
  regression} is an alternative for quantiles.
\item
  Type 1 Error: Accept ha when h0 is true.
\item
  RSS is The sum of the squared error terms for each observation in our
  dataset. RSE is The standard deviation of the residuals about the
  regression surface and is an estimate of sigma. TSS is a measure of
  the total squared deviation of our response variable from its mean
  value. R\^{}2 = 1 -- RSS/TSS.
\item
  Increasing x1 by 1 means you are increasing the underlying X1 by 1
  standard deviation x1+1=(X1+sig-mu)/sig. Therefore you can say that b1
  is the effect on y of increasing X1 by 1 sigma.
\item
  If there is an ascending or descending order to your dependent
  variable, ordinal regression is more appropriate as it has more power
  than multinomial logistic regression.
\item
  leaps lm package r is a greedy way to find the best predictors by
  cycling through all the combinations.
\item
  Linear Regression by Year
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(purrr)}
\NormalTok{pga }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(name,year,driveavg,drivepct, score.avg) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{split}\NormalTok{(.}\OperatorTok{$}\NormalTok{year) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{map}\NormalTok{(}\OperatorTok{~}\KeywordTok{lm}\NormalTok{(score.avg }\OperatorTok{~}\StringTok{ }\NormalTok{driveavg }\OperatorTok{+}\StringTok{ }\NormalTok{drivepct, }\DataTypeTok{data=}\NormalTok{.)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{map}\NormalTok{(summary)}
\end{Highlighting}
\end{Shaded}

\section{Ridge/Lasso}\label{ridgelasso}

\subsection{Intro}\label{intro-1}

Minimizes RSS but also has shrinkage penalty (L2 penalty). A small
lambda penalizes the RSS more than the shrinkage penalty. A large lambda
penalizes the shrinkage penalty more than the RSS. By shrinking the
coefficient estimates towards 0 by increasing lambda, the bias increases
slightly but remains relatively small, the variance reduces
substantially, and the mean squared error of the predictions drops. Not
scale invariant due to the shrinkage penalty. To avoid the issue of
overvaluing or undervaluing certain predictor variables simply based on
their magnitudes, we must standardize the variables prior to performing
ridge regression. The main disadvantage of ridge regression is that,
while parameter estimates are shrunken, they only asymptotically
approach 0 as we increase the value of lambda.

Standardize for ridge/lasso alpha makes them not scale invariant

Minimizes RSS but also has L1 penalty (norm). This necessarily
forcessome coefficient estimates to be exactly 0 (when lambda is
sufficiently large). It has the added advantage of essentially
performing variable selection, yielding models that are both accurate
and parsimonious. Restricting ourselves to simpler models by including a
Lasso penalty will generally decrease the variance of the fits at the
cost of higher bias.

Note: In both ridge and lasso regression, is important to select an
appropriate value of lambda by means of cross-validation.

\subsection{Assumptions}\label{assumptions-1}

\subsection{Evaluation}\label{evaluation}

\subsection{Pros/Cons}\label{proscons-1}

\section{Logistic Regression}\label{logistic-regression}

\subsection{Intro}\label{intro-2}

logistic is the same as probit

wilkinson rogers logistic regression

weighted logistic regression

The right-hand predictor side of the equation must be linear with the
left-hand outcome side of the equation. You must test for linearity in
the logit (in logistic regression the logit is the outcome side). This
is commonly done with the Box-Tidwell Transformation (Test): Add to the
logistic model interaction terms which are the crossproduct of each
independent times its natural logarithm {[}(X)ln(X){]}. If these terms
are significant, then there is nonlinearity in the logit. This method is
not sensitive to small nonlinearities.

Logistic regression assumes that logit(y) is linear in the values of x.
Like linear regression, logistic regression will find the best
coefficients to predict y, including finding advantageous combinations
and cancellations when the inputs are correlated. In other words, you
can think of logistic regression as a linear regression that finds the
log-odds of the probability that you're interested in. Logistic
Regression can also be used with kernel methods.

Logistic: When you're predicting which category your observation is in.
(Is this is a cat or a dog?)

The link function is the logit function. Logit is the inverse of the
sigmoid and gives the log odds, i.e p/(1-p).

Easy to incorpotate prior knowledge, number fo features are small,
training is fast, precision not critical.

No closed form expression to maximize coefficients with maximum
likelihood estimation, so one uses gradient descent or stochastic
gradient descent. The second one is faster.

For logistic regression value of c penalizes features. Low c penalizes a
lot and vice versa. A large c decreases the effect of the regularization
term (the l2 penalization).

Makes a linear boundary between classes.

For multiclass classification it will build multiple models. Given 3
classes 0,1, and 2, there will be models 0 and not 0, 1 and not 1, and 2
and not 2.

Standardization isn't required for logistic regression. The main goal of
standardizing features is to help convergence of the technique used for
optimization. For example, if you use Newton-Raphson to maximize the
likelihood, standardizing the features makes the convergence faster.
Otherwise, you can run your logistic regression without any
standardization treatment on the features.

\subsection{Assumptions}\label{assumptions-2}

\begin{itemize}
\item
  The linear regression analysis requires all variables to be
  multivariate normal. This assumption can best be checked with a
  histogram or a Q-Q-Plot.
\item
  Requires the dependent variable to be binary and ordinal logistic
  regression requires the dependent variable to be ordinal.
\item
  The observations to be independent of each other. In other words, the
  observations should not come from repeated measurements or matched
  data.
\item
  There to be little or no multicollinearity among the independent
  variables.
\item
  Assumes linearity of independent variables and logit of the
  probabilities (logit(p) = log(p/(1-p))). Check
  \href{https://stats.stackexchange.com/questions/169348/how-should-i-check-the-assumption-of-linearity-to-the-logit-for-the-continuous-i}{in
  R}.
\item
  There is no influential values (extreme values or outliers) in the
  continuous predictors
\end{itemize}

Notice the points fall along a line in the middle of the graph, but
curve off in the extremities. Normal Q-Q plots that exhibit this
behavior usually mean your data have more extreme values than would be
expected if they truly came from a Normal distribution.

The Durbin-Watson test is used to test the null hypothesis that linear
regression residuals are uncorrelated, against the alternative that
autocorrelation exists

\url{https://data.library.virginia.edu/diagnostic-plots/}

\textbf{No Multicolinearity}

\texttt{multicol} shows which columns could be removed for
multi-collinearity. (The vif is not working because of
multicolinearity.)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# vif(clf_lr$finalModel)}
\CommentTok{# multicol <- alias(clf_lr$finalModel)$Complete}
\CommentTok{# multicol}
\end{Highlighting}
\end{Shaded}

\textbf{Linear With The Logit Of Predictions}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# box tidwell doesn't work since features have negative values.}

\CommentTok{# min_max_scaler <- function(x) (x - min(x))/(max(x) - min(x))}
\CommentTok{# }
\CommentTok{# df_num_selected <- df2 %>% }
\CommentTok{#   select(}
\CommentTok{#     OverallMap,}
\CommentTok{#     StrandMap,}
\CommentTok{#     lesson_length,}
\CommentTok{#     g_avg_overall_map,}
\CommentTok{#     g_avg_strand_map,}
\CommentTok{#     g_avg_score,}
\CommentTok{#     g_num_students,}
\CommentTok{#     cohort_avg_overall_map,}
\CommentTok{#     cohort_avg_strand_map}
\CommentTok{#   ) %>%}
\CommentTok{#   map(min_max_scaler) %>% }
\CommentTok{#   data.frame() %>% }
\CommentTok{# }
\CommentTok{# formula_all_num_features <- as.formula(glue('pass ~ \{paste(df_num_selected %>% names, collapse = " + ")\}'))}
\CommentTok{# car::boxTidwell(formula_all_num_features, data = df_num_selected )}

\CommentTok{# Visual test too many features to work well}

\CommentTok{# lin_test <- df2 %>%}
\CommentTok{#   dplyr::select_if(is.numeric) }
\CommentTok{# }
\CommentTok{# predictors <- colnames(lin_test)}
\CommentTok{# }
\CommentTok{# lin_test <- lin_test %>%}
\CommentTok{#   mutate(logit = log(clf_lr_final$fitted.values/(1-clf_lr_final$fitted.values))) %>%}
\CommentTok{#   gather(key = "predictors", value = "predictor.value", -logit)}
\CommentTok{# }
\CommentTok{# ggplot(lin_test, aes(logit, predictor.value))+}
\CommentTok{#   geom_point(size = 0.5, alpha = 0.5) +}
\CommentTok{#   geom_smooth(method = "loess") + }
\CommentTok{#   theme_bw() + }
\CommentTok{#   facet_wrap(~predictors, scales = "free_y")}
\end{Highlighting}
\end{Shaded}

\subsection{Evaluation}\label{evaluation-1}

\begin{itemize}
\item
  Wald Test: h\_0: b = 0 and h\_a: b =/= 0. h\_0 means the log odds are
  unaffected by x and so x has no bearing on the prediction of success.
\item
  Deviance G\^{}2 and Goodness of Fit: The deviance associated with a
  given logistic regression model M is based on comparing the maximum
  log-likelihood of model M against the saturated model S. The smaller
  the deviance the better. For goodness of fit h0 says the model is
  appropriate while H\_a says the opposite.
\item
  McFadden's Pseudo R\^{}2.
\item
  A.I.C / B.I.C
\item
  Absolute scale odds instead of relative
\end{itemize}

\subsection{Pros/Cons}\label{proscons-2}

\textbf{Pros}

\begin{itemize}
\item
  Highly interpretably
\item
  Model training and prediction are fast
\item
  No tuning outside of regularization required
\item
  Features don't need scaling
\item
  Can perform with a small number of observations
\item
  Outputs well-calibrated predicted probabilities
\item
  low variance
\end{itemize}

\textbf{Cons}

\begin{itemize}
\item
  Presumes a linear relationship between the features and the log-odds
  of the response.
\item
  Performance is generally not competitive with the best supervised
  learning methods
\item
  Can't automatically learn feature interactions
\item
  Breaks down when classes are perfectly separable.
\item
  High bias
\end{itemize}

\subsection{Code Example}\label{code-example}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ statsmodels.api }\ImportTok{as}\NormalTok{ sm}
\NormalTok{X }\OperatorTok{=}\NormalTok{ sm.add_constant(X)}
\NormalTok{clf_lr }\OperatorTok{=}\NormalTok{ sm.Logit(Y, X).fit(disp}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(results.summary())}
\end{Highlighting}
\end{Shaded}

\section{Poisson Regression}\label{poisson-regression}

\subsection{Intro}\label{intro-3}

Poisson: When you're predicting a count value. (How many dogs will I see
in the park?)

Poisson Regression: The Poisson distribution is used to model variation
in count data (that is, data that can equal 0,1,2,\ldots{}).

Remember that you must specify family = poisson or family = quasipoisson
when using glm() to fit a count model.

\subsection{Assumptions}\label{assumptions-3}

One of the assumptions of Poisson regression to predict counts is that
the event you are counting is Poisson distributed: the average count per
unit time is the same as the variance of the count. In practice, ``the
same'' means that the mean and the variance should be of a similar order
of magnitude.

When the variance is much larger than the mean, the Poisson assumption
doesn't apply, and one solution is to use quasipoisson regression, which
does not assume that variance=meanvariance=mean.

\subsection{Evaluation}\label{evaluation-2}

\subsection{Pros/Cons}\label{proscons-3}

\textbf{Pros}

\textbf{Cons}

\section{Generalized Additive Models}\label{generalized-additive-models}

\subsection{Intro}\label{intro-4}

mgcv::gam: More complex than lm, more likely to overfit, best used on
larger data sets. use s(var) to denote non-linear relationship. Don't
use s() with categorical variable. Use type = terms for y values of
plots to get predictions type = response.

Also remember that gam() from the package mgcv has the calling interface

gam(formula, family, data) For standard regression, use family =
gaussian (the default).

For GAM models, the predict() method returns a matrix, so use
as.numeric() to convert the matrix to a vector.

\subsection{Assumptions}\label{assumptions-4}

\subsection{Evaluation}\label{evaluation-3}

\subsection{Pros/Cons}\label{proscons-4}

\textbf{Pros}

\textbf{Cons}

\section{K-Means Clustering {[}US{]}}\label{k-means-clustering-us}

\subsection{Intro}\label{intro-5}

K-Means clustering method considers two assumptions regarding the
clusters -- first that the clusters are spherical and second that the
clusters are of similar size.

k-means assumes the variance of the distribution of each attribute
(variable) is spherical; all variables have the same variance;

the prior probability for all k clusters is the same, i.e., each cluster
has roughly equal number of observations;

Must choose k for clusters. Initialize cluster centes randomly, assign
each point to its closest cluster by a distance metric. Recalculate
centroids. Halt when cluster assignments no longer change. Clusters will
be distinct and non-overlapping. Goal to minimize within-cluster
variation. The within-cluster variation for the kth cluster is the sum
of all of the pairwise squared Euclidean distances between the
observations in the kth cluster divided by the total number of
observations in the kth cluster. Limitation is that it depends on
initialization of centroids. Guaranteed convergence but only to a local
minima. Run the algorithm several times and pick the solution that
yields the smallest within-cluster variance. Increasing k will decrease
variance and increase bias and vice versa. K-means: Jaccard coefficient.

Note: Not enough to use within-cluster variance as this decreases as one
increases k. Use elbow method or scree plot to choose optimal k where
the variance no longer decreases dramatically. To choose how many groups
to use, I ran a k-means analysis for each possible number of groups from
5 to 35 and found that 20 was right about the spot that the amount of
variance stopped decreasing consistently. This is called the elbow test
and while the results weren't totally definitive, they fit in well with
the general rule of thumb for determining \# of groups which is the
square root of the \# of observations/2.

One way to assess whether a cluster represents true structure is to see
if the cluster holds up under plausible variations in the dataset. The
fpc package has a function called clusterboot() that uses bootstrap
resampling to evaluate how stable a given cluster is.{[}3{]}
clusterboot() is an integrated function that both performs the
clustering and evaluates the final produced clusters. It has interfaces
to a number of R clustering algorithms, including both hclust and
kmeans.

In K-means, we assume that each cluster fits a Gaussian distribution
(normal distribution)

We can set K to optimally cluster the data by starting with a small
number of clusters, and then iteratively splitting them until all
clusters fit a normal distribution.

\subsection{Assumptions}\label{assumptions-5}

\subsection{Characteristics}\label{characteristics}

\subsection{Evaluation}\label{evaluation-4}

Davies--Bouldin index: This is an internal evaluation scheme, where the
validation of how well the clustering has been done is made using
quantities and features inherent to the dataset. This has a drawback
that a good value reported by this method does not imply the best
information retrieval.

\subsection{Pros/Cons}\label{proscons-5}

\textbf{Pros}

Uses simple principle which can be explained in non-statistical terms.
It is efficient.

\textbf{Cons}

Less sophisticated, random choices of centers, and requires guess for
centers.

\section{Hierarchical Clustering}\label{hierarchical-clustering}

\subsection{Intro}\label{intro-6}

Generates dendrograms. The lower down a fusion occurs the more similar
the group of observations and vice versa. Begin with n observations and
a distance measure of all pairwise dissimialirities. Evaluate pairwise
intercluster dissimiliarities among the clusters and fuse pair of
clusters that are least dissimilar. Repeat process for remaining
clusters. Continue for i=n to i=2. Need to pick a dissimilarity measure
and a linkage method. Complete linkage: maximal inter-cluster
dissimilarity, single linkage: minimal inter-cluster dissimilarity,
avergae linkage: mean inter-cluster dissimilarity. Complete is sensitive
to outliers but tends to produce compact clusters. Distance between
groups: complete, average, and ward. Single not as sensitive to outliers
but susceptible to chaining effect where clusters can often not
represent intuitive groups. Average strikes a balance between the two.
Usually standardize data before clustering. Sklearn.metrics has pairwise
distances.

K-Means vs Hierarchical Clustering: K-means clustering needs the number
of clusters to be speciﬁed. Hierarchical clustering doesn't need the
number of clusters to be speciﬁed. K-means clustering is usually more
effcient run-time wise.

\subsection{Assumptions}\label{assumptions-6}

\subsection{Characteristics}\label{characteristics-1}

\subsection{Evaluation}\label{evaluation-5}

\subsection{Pros/Cons}\label{proscons-6}

\section{PCA}\label{pca}

\subsection{Intro}\label{intro-7}

\subsection{Assumptions}\label{assumptions-7}

\subsection{Characteristics}\label{characteristics-2}

\subsection{Evaluation}\label{evaluation-6}

\subsection{Pros/Cons}\label{proscons-7}

\begin{itemize}
\item
  Center and scale data. The eigenvectors yield orthogonal directions of
  greatest variability(principal components). The eigenvalues correspond
  to the magnitude of variancealong the principal components.
  Interpretability is a negative. PCA is completely nonparametric: any
  data set can be plugged in and an answer comes out, requiring no
  parameters to tweak and no regard for how the data was recorded. From
  one perspective, the fact that PCA is non-parametric (or
  plug-and-play) can be considered a positive feature because the answer
  is unique and independent of the user. From another perspective the
  fact that PCA is agnostic to the source of the data is also a
  weakness. When using dimensional reduction we restrict ourselves to
  simpler models. Thus, we expect bias to increase and variance to
  decrease. Getting latent components. Visualize high dimensional data.
  Reduce noise. Preprocessing. scale and center data before doing. Needs
  regularization. Too many principal components F1 score starts to drop
  after increase. Best F1 is 1. set number of components. Use
  fit\_transforms, use explained\_variance\_ratio\_ to see how much of
  the variance is explained by each component. Choose features that
  explain most of the variances, then use a inverse\_transform to
  reconstruct your x. Minimum number of principal components is
  min(n,p).
\item
  pca: It accounts for as much of the variability in the data as
  possible by considering highly correlated features. Each succeeding
  component in turn has the highest variance using the features that are
  less correlated with the first principal component and that are
  orthogonal to the preceding component.
\end{itemize}

\subsection{Other}\label{other-2}

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\arabic{enumi})}
  \tightlist
  \item
    look at variance: apply(USArrests , 2, var) \textbar{} 2) pca \&
    scale: pca = prcomp(USArrests , scale = TRUE) \textbar{} 3) plot:
    biplot(pca, scale = 0)
  \end{enumerate}
\item
  pca\$rotation contains the principal component loadings matrix which
  explains proportion of each variable along each principal component.
\item
  Kaiser-Harris criterion suggests retaining PCs with eigenvalues
  \textgreater{} 1; PCs with eigenvalues \textless{} 1 explain less
  variance than contained in a single variable. Cattell Scree test
  visually inspects the elbow graph for diminishing return; retain PCs
  before a drastic drop-off.
\end{itemize}

PC columns contain loadings; correlations of the observed variables with
the Pcs. h2 column displays the component comunalities; amount of
variance explained by the components.

Note: Look at factorplot. Shows pot of relationship between variables
and principal components.

It is also possible to decompress the reduced dataset back to 784
dimensions by applying the inverse transformation of the PCA projection.
Of course this won't give you back the original data, since the
projection lost a bit of information (within the 5\% variance that was
dropped), but it will likely be quite close to the original data. The
mean squared distance between the original data and the reconstructed
data (compressed and then decompressed) is called the reconstruction
error.

It turns out that many things behave very differently in
high-dimensional space. For example, if you pick a random point in a
unit square (a 1 × 1 square), it will have only about a 0.4\% chance of
being located less than 0.001 from a border (in other words, it is very
unlikely that a random point will be ``extreme'' along any dimension).
But in a 10,000-dimensional unit hypercube (a 1 × 1 × ⋯ × 1 cube, with
ten thousand 1s), this probability is greater than 99.999999\%. Most
points in a high-dimensional hypercube are very close to the border.

Before looking at the PCA algorithm for dimensionality reduction in more
detail, let's summarize the approach in a few simple steps: Standardize
the -dimensional dataset.

Construct the covariance matrix.Decompose the covariance matrix into its
eigenvectors and eigenvalues. Select eigenvectors that correspond to the
largest eigenvalues, where is the dimensionality of the new feature
subspace ().

Construct a projection matrix from the ``''top``'' eigenvectors.

Transform the -dimensional input dataset using the projection matrix to
obtain the new -dimensional feature subspace.

Probabilistic PCA introduces the Gaussian distribution to the PCA
modeling framework.

Kernel PCA must project data into a higher dimensional space than that
of the original data.

\section{KNN}\label{knn}

\subsection{Intro}\label{intro-8}

Calculate distance between each observation and all the others.
Determine the k nearest observations to the observation. Classify the
observation as the most frequent class of the k nearest observations.
Small k's are not robust to outliers, highlight local variations and
induce unstable decision boundaries. Large k's are robust to outliers,
highlight global variations and induce stable decision boundaries. Good
value to choose is k = sqrt(n). Can use maximum prior probability to
decide ties. Use Euclidean distance for numerical data and Hamming
distance for categorical data. Latter treats dimensions equally and is
symmetric. Low k is low bias, high variance and high k is high bias low
variance. 1NN can't adapt to outliers and has no notion of class
frequencies. Can use weighted KNN. Easy in sklearn.

curse of dimensionality, overfitting, correlated features, cost to
update, sensitivity of distance metrics

smaller k means smaller training error, ut larger k is more stable.

\subsection{Assumptions}\label{assumptions-8}

\subsection{Characteristics}\label{characteristics-3}

\subsection{Evaluation}\label{evaluation-7}

\subsection{Pros/Cons}\label{proscons-8}

\textbf{Pros}

Only assumption is proximity. Non-parametric.

The only assumption we are making about our data is related to proximity
(i. e., observations that are close by in the feature space are similar
to each other in respect to the target value). We do not have to fit a
model to the data since this is a non-parametric approach.

\begin{itemize}
\tightlist
\item
  Powerful
\item
  No training involved (``lazy'')
\item
  Naturally handles multiclass classification and regression
\end{itemize}

What is a common drawback of 3NN classifiers? Prediction on large data
sets is slow.

\textbf{Cons}

Have to decide k and distance metric. Can be sensitive to outliers or
irrelevant attributes. Computationally expensive.

knn is terrible with high dimensions. Bad with categorical data.

Expensive and slow to predict new instances - Must define a meaningful
distance function - Performs poorly on high-dimensionality datasets

We have to decide on K and a distance metric.

Can be sensitive to outliers or irrelevant attributes because they add
noise.

Computationally expensive; as the number of observations, dimensions,
and K increases, the time it takes for the algorithm to run and the
space it takes to store the computations increases dramatically.

\section{SVM}\label{svm}

\subsection{Intro}\label{intro-9}

Selection of margin in SVM is a convex optimization problem. What is the
objective of the maximum margin approach? To ensure small prediction
error when the underlying distribution is not known

The LinearSVC class regularizes the bias term, so you should center the
training set first by subtracting its mean. This is automatic if you
scale the data using the StandardScaler. Moreover, make sure you set the
loss hyperparameter to ``hinge'', as it is not the default value.
Finally, for better performance you should set the dual hyperparameter
to False, unless there are more features than training instances (we
will discuss duality later in the chapter).

With so many kernels to choose from, how can you decide which one to
use? As a rule of thumb, you should always try the linear kernel first
(remember that LinearSVC is much faster than SVC(kernel=``linear'')),
especially if the training set is very large or if it has plenty of
features. If the training set is not too large, you should try the
Gaussian RBF kernel as well; it works well in most cases. Then if you
have spare time and computing power, you can also experiment with a few
other kernels using cross-validation and grid search, especially if
there are kernels specialized for your training set's data structure.

Here mostly talking about a maximal margin classifier. A separating
hyperline of data of two classes. Maximizes distance to the nearest
point in either class, i.e, the margin. Points which delineate the
boundary are called support vectors. Correct classification first and
then maximizing the margin. SVM only really works well with linear
hyperplanes. The c hyperparameter deals with the tradeoff between a
smooth decision boundary and correct classification. The larger the c
the more points classified correctly as it increases the penalty for
wrong classification. A smaller c gives more room for intial error which
helps improve accuracy for not easily separable data. Work well in
complicated domains with clear line of separation. Not so well in for
large datasets because of training and not when there is a lot of noise.
Can solve nonlinear decision boundaries by using polynomial terms.
Kernel trick that draws boundaries by looking into higher dimensions.

Extracting the coefficients from the hyperplane equation (not including
the intercept) yields what is called a normal vector. This vector points
in a direction orthogonal to the surface of the hyperplane and
essentially defines its orientation.

Big data set and lot of features SVM might be slow and prone to
overfitting. SVM slower than naïve bayes. A kernelis a function that
quantifies the similarity of two observations. The beauty of the
``kernel trick'' is that, even if there is an infinite-dimensional
basis, we need only look at the n\^{}2 inner products between training
data points. SVM not scale invariant. Check if library normalizes by
default. RBF kernel is a good default. Try exponential sequences for
parameters.

Extension of above info about relaxing margin. Doesn't perfectly
separate classes but is more robust to outliers. Helps better classify
obervations. Take a penalty by potentially misclassifying soe
observations. Have better predictive power. Called a soft margin. The C
parameter is the budget for the slack variables which tell where
observations are relative to the margin and hyperplane. Only
observations that either fall on the margin or violate the margin affect
the solution to the optimization problem. More robust than SVM.

When the support vector classifier is combined with a non-linear kernel,
the resulting classifier is called a support vector machine.

Note: Important to make sure the normal vector is of unit length. For
the radial kernel, suppose we have a test observation. If it is far from
a training observation, the Euclidean distance will be large, but the
value of the radial kernel will be small. using kernels is much more
computationally efficientbecause we only need to compute the kernel for
distinct pairs of observations in our dataset. Furthermore, we need not
actually work in the enlarged feature space. Intuitively, the gamma
parameter defines how far the influence of a single training example
reaches, with low values meaning `far' and high values meaning `close'.
The gamma parameters can be seen as the inverse of the radius of
influence of samples selected by the model as support vectors.

Multiclass

1v1: Construct a support vector machine for each pair of categories. For
each classifier, record the prediction for each observation. Have the
classifiers vote on the prediction for each observation.

1vAll: Construct a support vector machine for each individual category
against all other categories combined. Assign the observation to the
classifier with the largest function value.

\subsection{Assumptions}\label{assumptions-9}

\subsection{Characteristics}\label{characteristics-4}

\subsection{Evaluation}\label{evaluation-8}

\subsection{Pros/Cons}\label{proscons-9}

\textbf{Pros}

\begin{itemize}
\item
  Performs similarly to logistic regression when linear separation
\item
  Performs well with non-linear boundary depending on the kernel used
\item
  Handle high dimensional data well
\item
  Can model complex, nonlinear relationships
\item
  Robust to noise (because they maximize margins)
\end{itemize}

\textbf{Cons}

\begin{itemize}
\item
  Susceptible to overfitting/training issues depending on kernel
\item
  Need to select a good kernel function
\item
  Model parameters are difficult to interpret
\item
  Sometimes numerical stability problems
\item
  Requires significant memory and processing power
\end{itemize}

\section{Decision Tree}\label{decision-tree}

\subsection{Intro}\label{intro-10}

So should you use Gini impurity or entropy? The truth is, most of the
time it does not make a big difference: they lead to similar trees. Gini
impurity is slightly faster to compute, so it is a good default.
However, when they differ, Gini impurity tends to isolate the most
frequent class in its own branch of the tree, while entropy tends to
produce slightly more balanced trees.

Decision Trees make very few assumptions about the training data (as
opposed to linear models, which obviously assume that the data is
linear, for example). Such a model is often called a nonparametric
model, not because it does not have any parameters (it often has a lot)
but because the number of parameters is not determined prior to
training, so the model structure is free to stick closely to the data.

More generally, the main issue with Decision Trees is that they are very
sensitive to small variations in the training data. For example, if you
just remove the widest Iris-Versicolor from the iris training set (the
one with petals 4.8 cm long and 1.8 cm wide) and train a new Decision
Tree, you may get the model represented in Figure~6-8. As you can see,
it looks very different from the previous Decision Tree (Figure~6-2).
Actually, since the training algorithm used by Scikit-Learn is
stochastic6 you may get very different models even on the same training
data (unless you set the random\_state hyperparameter).

Construct solutions that stratify the feature space into relatively easy
to describe regions. Partition predictor space into rectangular or
box-like regions. For regression predict the meant of the reponse
variables in each region for the training observations in that space.
Top down and greedy approach called recursive binary spltting to find
ebst solution. Greedy because splits based on best result. The recursive
binary splitting process depends on the greatest reduction in the RSS.
Entropy very important. Decides where tree splits. Measure of impurity
in examples. Entropy between 0 and 1. 0 means no impurity. Maximial is 1
when there is an even split between options. Key term: information gain.
Decisions tree maximize this. Easy to interpret, prone to overfitting.
Where each data point has its own node, the variance is high and the
training error is 0. Deciding on the number of splits prior to building
a tree isn't the best strategy. The best subtree will be the one that
yields the lowest test error rate. Given a subtree, we can estimate the
test error by implementing the crossvalidationprocess, but this is too
cumbersome because the large number of possible subtrees. The tuning
parameter alpha helps balance the tradeoff between the overall
complexity of the tree and its fit to the training data. Small values of
alpha yield trees that are quite extensive (have many terminal nodes).
Large values of alpha yield trees that are quite limited (have few
terminal nodes). Similar to ridge/lasso regression. Ultimately, the
subtree that is used for prediction is built using all of the available
data with the determined optimal value of alpha.

Regression: Use cost complexity pruning to build set of subtrees as a
function of lambda. Classification: Use Gini index, measures total
variance among classes.

Note: Duplicate splits happen when they increase node purity. While
pruning reduces the variance of the overall tree model upon repeated
builds with different datasets, we induce biasbecause the trees are much
simpler.

\subsection{Assumptions}\label{assumptions-10}

\subsection{Characteristics}\label{characteristics-5}

\subsection{Evaluation}\label{evaluation-9}

\subsection{Pros/Cons}\label{proscons-10}

\textbf{Pros}

\begin{itemize}
\item
  Fast
\item
  Robust to noise and missing values
\item
  Accurate
\end{itemize}

\textbf{Cons}

\begin{itemize}
\item
  Weak predictive accuracy and prone to overfitting.
\item
  Complex trees are hard to interpret
\item
  Duplication within the same sub-tree is possible
\end{itemize}

\section{Bagging}\label{bagging}

\subsection{Intro}\label{intro-11}

used to decrease variance in decision trees

How it works: train multiple trees using bootstrapped datasets,
regression trees: average the predictions from all the trees,
classification: take a majority vote of the predictions, majority vote:
the class which occurs most frequently, Out of bag (OOB) error, Only a
portion of the data is used to train each tree, The remaining data which
wasn't selected can be used to assess the tree's performance, (the
response for each observation is predicted using only the trees that
were not fit using that observation)

Bagging involves randomly generating Bootstrap samples from the dataset
and trains the models individually. Predictions are then made by
aggregating or averaging all the response variables:For example,
consider a dataset (Xi, Yi), where i=1 \ldots{}n, contains n data
points.Now, randomly select B samples with replacements from the
original dataset using Bootstrap technique.Next, train the B samples
with regression/classification models independently. Then, predictions
are made on the test set by averaging the responses from all the B
models generated in the case of regression. Alternatively, the most
often occurring class among B samples is generated in the case of
classification.

Bagging stabilizes decision trees and improves accuracy by reducing
variance.

Bagging reduces generalization error.

Take repeated samples of the same size from the single overall training
dataset. Treat these different sets of data as pseudo-training sets.
Fitting separate, independent decision treesto each of the bootstrapped
training data sets. We can then average all predictions (or take the
majority vote) to obtain the bagged estimate. Instead of pruning back
our trees, create very large trees in the first place. These large trees
will tend to have low bias, but high variance. Retain the low bias, but
get rid of the high variance by averaging. Con is that the trees are
correlation. train multiple trees using bootstrapped data to reduce
variance and prevent overfitting

\subsection{Assumptions}\label{assumptions-11}

\subsection{Characteristics}\label{characteristics-6}

\subsection{Evaluation}\label{evaluation-10}

\subsection{Pros/Cons}\label{proscons-11}

\textbf{Pros}

\begin{itemize}
\item
  reduces variance in comparison to regular decision trees
\item
  Can provide variable importance measures: classification: Gini index +
  regression: RSS
\item
  Can easily handle qualitative (categorical) features
\item
  Out of bag (OOB) estimates can be used for model validation
\end{itemize}

\textbf{Cons}

\begin{itemize}
\item
  Not as easy to visually interpret
\item
  Does not reduce variance if the features are correlated
\end{itemize}

\section{Random Forest}\label{random-forest}

\subsection{Intro}\label{intro-12}

Build a number of decision trees using bootstrapped samples

At each split in the tree only a portion of the features are considered

a feature is selected from a subset of features not the whole feature
space

the subset is usually sqrt(n.features)

This allows trees to be built without some of the strong features
decorrelates the trees, unlike bagging

Random forests are improvised supervised algorithms than bootstrap
aggregation or bagging methods, though they are built on a similar
approach. Unlike selecting all the variables in all the B samples
generated using the Bootstrap technique in bagging, we select only a few
predictor variables randomly from the total variables for each of the B
samples. Then, these samples are trained with the models. Predictions
are made by averaging the result of each model. The number of predictors
in each sample is decided using the formula m = √p, where p is the total
variable count in the original dataset.Here are some key
\url{notes:This} approach removes the condition of dependency of strong
predictors in the dataset as we intentionally select fewer variables.

Draw a random bootstrap sample of size n (randomly choose n samples from
the training set with replacement).Grow a decision tree from the
bootstrap sample. At each node:Randomly select d features without
replacement.Split the node using the feature that provides the best
split according to the objective function, for instance, by maximizing
the information gain.Repeat the steps 1 to 2 k times.Aggregate the
prediction by each tree to assign the class label by majority vote.
Majority voting will be discussed in more detail in Chapter 7, Combining
Different Models for Ensemble Learning.

Random forests further improve decision tree performance by
de-correlating the individual trees in the bagging ensemble.

Random forests decorrelate tree it generates and thus results in a
reduction in variance. Similar to bagging, we first build various
decision trees on bootstrapped training samples, but we split internal
nodes in a special way. Each time a split is considered within the
construction of a decision tree, only a random subset of ᬊthe overall
predictors are allowed to be candidates. At every split, a new subset of
predictors is randomly selected. The random forest procedure forces the
decision tree building process to use different predictors to split at
different times. Should a good predictor be left out of consideration
for some splits, it still has many chances to be consideredin the
construction of other splits. We can't overfit by adding more trees. The
variance just ends up decreasing. Out of bag score is a good estimate of
the test score and is the non-bootstrapped data. Need to turn on this
scoing method. Error Rate: Depends on correlation between trees (higher
is worse), strength of single trees (higher is better). Increasing
number of features for each split increases correlation and strength of
single trees.

\subsection{Assumptions}\label{assumptions-12}

\subsection{Characteristics}\label{characteristics-7}

\subsection{Evaluation}\label{evaluation-11}

\subsection{Pros/Cons}\label{proscons-12}

\textbf{Pros}

Decorrelates trees (relative to boosted trees)

important when dealing with mulitple features which may be correlated

reduced variance (relative to regular trees)

All purpose model that performs well on most problems. Automatically
chooses the best features. High accuracy.

\textbf{Cons}

Hard to interpret and needs a lot of work to tune parameters.

Not as easy to visually interpret

\section{Boosting}\label{boosting}

\subsection{Intro}\label{intro-13}

Unlike boosting and random forests, can overfit if number of trees is
too large

Similar to bagging, but trees are grown sequentially. Each tree is
created using information from previously grown trees

Each tree is generated using information from previously grown trees;
the addition of a new tree improves upon the performance of the previous
trees. The trees are now dependentupon one another. Boosting approach
tends to slowly learn our data. Given a current decision tree model, we
fit a new decision tree to the residualsof the current decision tree.
The new decision tree (based on the residuals) is then added to the
current decision tree, and the residuals are updated. We limit the
number of terminal nodes in order to sequentially fit small trees. By
fitting small trees to the residuals, we slowly improve the overall
model in areas where it does not perform well.

Note: Unlike in bagging and random forests, boosting can overfit if ᫦is
large (although very slowly). Use cross-validation to select number of
trees᫦. Alpha controls the rate at which boosting learns and is usually
between 0.01 to 0.001. A small alpha usually goes with a large number of
trees. Can also choose the number of splits. Typically using stumps
(single splits d = 1) is sufficient and results in an additive model.

Note on Variable Importance: For regression trees, we can use the
reduction in the RSS. For classification trees, we can use the reduction
in the Gini index. A relatively large value indicates a notable drop in
the RSS or Gini index, and thus a better fit to the data; corresponding
variables are relatively important predictors.

Similar to bagging, but learns sequentially and builds off previous
trees

\subsection{Assumptions}\label{assumptions-13}

\subsection{Characteristics}\label{characteristics-8}

\subsection{Evaluation}\label{evaluation-12}

\subsection{Pros/Cons}\label{proscons-13}

\textbf{Pros}

Somewhat more interpretable than boosted trees/random forest as the user
can define the size of each tree resulting in a collection of stumps (1
level) which can be viewed as an additive model

Can easily handle qualitative (categorical) features

\textbf{Cons}

\section{Association Rule Mining}\label{association-rule-mining}

\subsection{Intro}\label{intro-14}

\begin{itemize}
\item
  Used in Genetics, Fraud, and Market Basket Analysis, etc. A typical
  rule might be: if someone buys peanut butter and jelly, then that
  person is likely to buy bread as well.
\item
  Incredibly big feature space (2\^{}k-1).
\item
  The Apriori algorithm employs a simple a priori belief as a guideline
  for reducing the association rule space.
\item
  Support: the fraction of which each item appears within the dataset as
  a whole. Support(item) = count(item)/N. Higher support is better.
\item
  nfidence: the likelihood that a constructed rule is correct given the
  items on the left hand side of the transaction. A higher level of
  confidence implies a higher likelihood that Y appears alongside
  transactions in which X appears.
\item
  ft: the ratio by which the confidence of a rule exceeds the expected
  outcome. When lift \textgreater{} 1, the presence of X seems to have
  increasedthe probability of Y occurring in the transaction. When lift
  \textless{} 1, the presence of X seems to have decreasedthe
  probability of Y occurring in the transaction. When lift = 1, X and Y
  are independent.
\item
  t thresholds for support and confidence and then the algorithm goes
  from all 1-combinations to 2-combinations and up. Those subset below
  the threshold don't make it to the higher iterations.
\end{itemize}

\subsection{Assumptions}\label{assumptions-14}

\subsection{Characteristics}\label{characteristics-9}

\subsection{Evaluation}\label{evaluation-13}

\subsection{Pros/Cons}\label{proscons-14}

\textbf{Pros}

\begin{itemize}
\item
  Ideally suited for working with very large amounts of transactional
  data.
\item
  The results are rules that are generally easy to understand and have a
  high amount of interpretability.
\item
  The process is useful for data mining and uncovering unexpected
  knowledge within a dataset.
\end{itemize}

\textbf{Cons}

\begin{itemize}
\item
  The outcome is usually not interesting when applied to smaller
  datasets.
\item
  It is difficult to separate actual insights from common sense notions.
\item
  The analyst might be compelled to draw spurious conclusions--remember
  that correlation doesn't imply causation!
\end{itemize}

\section{Naïve Bayes}\label{naive-bayes}

\subsection{Intro}\label{intro-15}

Practitioners do use Naive Bayes regularly for ranking, where the actual
values of the probabilities are not relevant---only the relative values
for examples in the different classes. Another advantage of Naive Bayes
is that it is naturally an ``incremental learner.'' An incremental
learner is an induction technique that can update its model one training
example at a time. It does not need to reprocess all past training
examples when new training data become available.

Document/spam classification is one use. Assumptions are that all the
features are equally likely and are all important. Would be
computational expensive without these assumptions with having to track
all the joint probabilities. Gaussian for continuous variables,
Bernoulli for binary input, and Multinomial for binary and more input.
Use MLE to estimate parameters for Gaussian. Bernoulli/Multinomial used
for spam classification.The Laplace Estimator (usually chosen to be 1)
is a corrective measure that adds a small amount of error to each of the
counts in the frequency table of words. The addition of error ensures
that each resulting probability of each event will necessarily be
nonzero, even if the event did not appear in the training data.

Note: In addition, we can use function ``partial\_fit'' to fit on a
batch of samples incrementally while we are using. MultinomialNB and
Bernoulli NB also support sample weighting.

\subsection{Assumptions}\label{assumptions-15}

\subsection{Characteristics}\label{characteristics-10}

\subsection{Evaluation}\label{evaluation-14}

\subsection{Pros/Cons}\label{proscons-15}

\textbf{Pros}

\begin{itemize}
\item
  It is relatively simple to understand. Training the classifier does
  not require many observations, and the method also works well with
  large amounts of data. It is easy to obtain the estimated
  probabilityfor a classification prediction.
\item
  Computationally fast
\item
  Simple to implement
\item
  Works well with high dimensions
\end{itemize}

\textbf{Cons}

\begin{itemize}
\item
  Relies on independence assumption and will perform badly if this
  assumption is not met
\item
  While easily attainable, the estimated probabilities are often less
  reliable than the predicted class labels themselves.
\end{itemize}

\section{LDA}\label{lda}

\subsection{Intro}\label{intro-16}

Before we take a look into the inner workings of LDA in the following
subsections, let's summarize the key steps of the LDA approach:

Standardize the -dimensional dataset ( is the number of features).

For each class, compute the -dimensional mean vector.

Construct the between-class scatter matrix and the within-class scatter
matrix .

Compute the eigenvectors and corresponding eigenvalues of the matrix .

Choose the eigenvectors that correspond to the largest eigenvalues to
construct a -dimensional transformation matrix ; the eigenvectors are
the columns of this matrix.

Project the samples onto the new feature subspace using the
transformation matrix.

LDA similar to Naive Bayes but doesn't assume variables are independent,
assumes that p(x\textbar{}y) is a multivariate normal distribution, and
assumes gaussian distributions for each class share the same covariance
matrix. If they don't share the same covariance matrix, then we use
Quadratic Discriminant Analysis which assumes each class has its own.

\subsection{Assumptions}\label{assumptions-16}

\subsection{Characteristics}\label{characteristics-11}

\subsection{Evaluation}\label{evaluation-15}

\subsection{Pros/Cons}\label{proscons-16}

\section{QDA}\label{qda}

\subsection{Intro}\label{intro-17}

TBD

\subsection{Assumptions}\label{assumptions-17}

\subsection{Characteristics}\label{characteristics-12}

\subsection{Evaluation}\label{evaluation-16}

\subsection{Pros/Cons}\label{proscons-17}

\section{Model Selection}\label{model-selection}

\subsection{General}\label{general-2}

\begin{itemize}
\item
  Linear SVM often outperforms logistic regression and is explainable as
  well.
\item
  However, the problem with cross-validation is that it is rarely
  applicable to real world problems, for all the reasons described in
  the above sections. Cross-validation only works in the same cases
  where you can randomly shuffle your data to choose a validation set.
\item
  If the performance of a classification model on the test set
  (out-of-sample) error is poor, you can't just re-calibrate your model
  parameters to achieve a better model. This is cheating.
\item
  When the classes are well-separated, the parameter estimates for the
  logistic regression model are surprisingly unstable. Linear
  discriminant analysis does not suffer from this problem. If n is small
  and the distribution of the predictors X is approximately normal in
  each of the classes, the linear discriminant model is again more
  stable than the logistic regression model.
\item
  Roughly speaking, LDA tends to be a better bet than QDA if there are
  relatively few training observations and so reducing variance is
  crucial. In contrast, QDA is recommended if the training set is very
  large, so that the variance of the classifier is not a major concern,
  or if the assumption of a common covariance matrix for the K classes
  is clearly untenable.
\item
  Two features interact if the effect on the prediction of one feature
  depends on the value of the other feature
\item
  ``No Free Lunch'' theorem: no single classifier works best across all
  possible scenarios
\item
  Warning PolynomialFeatures(degree=d) transforms an array containing n
  features into an array containing features, where n! is the factorial
  of n, equal to 1 × 2 × 3 × ⋯ × n. Beware of the combinatorial
  explosion of the number of features!
\item
  If a model suffers from overfitting, we also say that the model has a
  high variance, which can be caused by having too many parameters that
  lead to a model that is too complex given the underlying data.
  Similarly, our model can also suffer from underfitting (high bias),
  which means that our model is not complex enough to capture the
  pattern in the training data well and therefore also suffers from low
  performance on unseen data.
\item
  It appears that each feature has modestly improved our model. There
  are certainly more features that we could add to our model. For
  example, we could add the day of the week and the hour of the posting,
  we could determine if the article is a listicle by running a regex on
  the headline, or we could examine the sentiment of each article. This
  only begins to touch on the features that could be important to model
  virality. We would certainly need to go much further to continue
  reducing the error in our model.
\item
  Suppose Fixitol reduces symptoms by 20\% over a placebo, but the trial
  you're using to test it is too small to have adequate statistical
  power to detect this difference reliably. We know that small trials
  tend to have varying results; it's easy to get 10 lucky patients who
  have shorter colds than usual but much harder to get 10,000 who all
  do.
\item
  As I mentioned earlier, one drawback of the Bonferroni correction is
  that it greatly decreases the statistical power of your experiments,
  making it more likely that you'll miss true effects. More
  sophisticated procedures than Bonferroni correction exist, ones with
  less of an impact on statistical power, but even these are not magic
  bullets. Worse, they don't spare you from the base rate fallacy. You
  can still be misled by your p threshold and falsely claim there's
  ``only a 5\% chance I'm wrong.'' Procedures like the Bonferroni
  correction only help you eliminate some false positives.
\item
  In step 1, we find where missing values are located. The md.pattern()
  function from the mice package is a really useful function. It gives
  you a clear view of where missing values are located, helping you in
  decisions regarding exclusions or substitution. You can refer to the
  next recipe for missing value substitution.
\item
  ``Product classification is an example of multicategory or multinomial
  classification. Most classification problems and most classification
  algorithms are specialized for two-category, or binomial,
  classification. There are tricks to using binary classifiers to solve
  multicategory problems (for example, building one classifier for each
  category, called a ``one versus rest'' classifier). But in most cases
  it's worth the effort to find a suitable multiple-category
  implementation, as they tend to work better than multiple binary
  classifiers (for example, using the package mlogit instead of the base
  method glm() for logistic regression).''
\item
  Another possible indication of collinearity in the inputs is seeing
  coefficients with an unexpected sign: for example, seeing that income
  is negatively correlated with years in the workforce.
\item
  The overall model can still predict income quite well, even when the
  inputs are correlated; it just can't determine which variable deserves
  the credit for the prediction. Using regularization (especially ridge
  regression as found in lm.ridge() in the package MASS) is helpful in
  collinear situations (we prefer it to ``x-alone'' variable
  preprocessing, such as principal components analysis). If you want to
  use the coefficient values as advice as well as to make good
  predictions.
\item
  The degrees of freedom is thought of as the number of training data
  rows you have after correcting for the number of coefficients you
  tried to solve for. You want the degrees of freedom to be large
  compared to the number of coefficients fit to avoid overfitting.
\item
  Overfitting is when you find chance relations in your training data
  that aren't present in the general population. Overfitting is bad: you
  think you have a good model when you don't.
\item
  The probable reason for nonconvergence is separation or
  quasi-separation: one of the model variables or some combination of
  the model variables predicts the outcome perfectly for at least a
  subset of the training data. You'd think this would be a good thing,
  but ironically logistic regression fails when the variables are too
  powerful.
\item
  For categorical variables (male/female, or small/medium/large), you
  can define the distance as 0 if two points are in the same category,
  and 1 otherwise. If all the variables are categorical, then you can
  use Hamming distance, which counts the number of mismatches.
\item
  Your next step is to select a performance measure. A typical
  performance measure for regression problems is the Root Mean Square
  Error (RMSE). It measures the standard deviation of the errors the
  system makes in its predictions.
\item
  You should save every model you experiment with, so you can come back
  easily to any model you want. Make sure you save both the
  hyperparameters and the trained parameters, as well as the
  cross-validation scores and perhaps the actual predictions as well.
  This will allow you to easily compare scores across model types, and
  compare the types of errors they make. You can easily save
  Scikit-Learn models by using Python's pickle module, or using
  sklearn.externals.joblib, which is more efficient at serializing large
  NumPy arrays.
\item
  If all classifiers are able to estimate class probabilities (i.e.,
  they have a predict\_proba() method), then you can tell Scikit-Learn
  to predict the class with the highest class probability, averaged over
  all the individual classifiers. This is called soft voting. It often
  achieves higher performance than hard voting because it gives more
  weight to highly confident votes. All you need to do is replace
  voting=``hard'' with voting=``soft'' and ensure that all classifiers
  can estimate class probabilities.
\item
  The standard value for k in k-fold cross-validation is 10, which is
  typically a reasonable choice for most applications. However, if we
  are working with relatively small training sets, it can be useful to
  increase the number of folds. If we increase the value of k, more
  training data will be used in each iteration, which results in a lower
  bias towards estimating the generalization performance by averaging
  the individual model estimates. However, large values of k will also
  increase the runtime of the cross-validation algorithm and yield
  estimates with higher variance since the training folds will be more
  similar to each other. On the other hand, if we are working with large
  datasets, we can choose a smaller value for k.
\item
  Distance Metrics: Hamming distance for categorical variables, Cosine
  distance for text data, Gower distance for categorical data
\item
  AIC x BIC: Explanatory power x Parsimonious
\item
  Bias is how far offon the average the model is from the truth.
  Variance is how much the estimate varies about its average. With low
  model complexity bias is high because predictions are more likely to
  stray from the truth, and variance is low because there are only few
  parameters being fit. With high model complexity bias is low because
  the model can adapt to more subtleties in the data, and variance is
  high because we have more parameters to estimate from the same amount
  of data.
\item
  The mean of highly correlated quantities has higher variance than the
  mean of uncorrelated quantities. The smaller the number of folds, the
  more biased the error estimates (they will be biased to be
  conservative indicating higher error than there is in reality) but the
  less variable they will be. On the extreme end you can have one fold
  for each data point which is known as Leave-One-Out-Cross-Validation.
  In this case, your error estimate is essentially unbiased but it could
  potentially have high variance.
\item
  LDA vs Logistic Regression: When the classes are well-separated
  parameters of logistic regression are unstable and LDA doesn't have
  this problem. If the distribution of X is approximately normal then
  LDA is more stable again. LDA doesn't need to fit multiple models for
  multiclass classification.
\item
  SVM vs Logistic Regression: SVM is likely to overfit the data and can
  dominated LR on the training set. Kernel method makes SVM more
  flexible.
\item
  Decision Tree vs Linear Model: Tree has non-linear boundary, selects
  important features, easier to intepret but unstable especially for
  small data.
\item
  Naive Bayes vs LDA: Both use Bayes theorem, both assume gaussian
  distribution, but LDA takes care of the correlations. LDA might
  out-perform Naive Bayes when the features are highly correlated.
\item
  KNN vs Other Classification Methods: KNN is completely non-parametic,
  doesn't tell which features are important. Dominates LDA and Logistic
  Regression when the decision boundary is highly non-linear.
\item
  When building, testing, and validating a suite of models, the optimal
  model (i.e., the optimal point in the ROC curve) is the spot where the
  overall accuracy of the model is essentially unchanged as we make
  small adjustments in the choice of model. That is to say, the change
  (from one model to the next) in the model's precision (specificity) is
  exactly offset by the change in the model's recall (sensitivity).
  Consequently, allowing for some imperfection, where the false positive
  rate and the false negative rate are in proper balance (so that
  neither one has too great of an impact on the overall accuracy and
  effectiveness of the model), is good fruit from your big data labors.
  The metric I used to guide my cross-validation is the F-score. This is
  a good metric when we have a lot more samples from one category than
  from the other categories.
\item
  The cost of the holdout method comes in the amount of data that is
  removed from the model training process. For instance, in the
  illustrative example here, we removed 30\% of our data. This means
  that our model is trained on a smaller data set and its error is
  likely to be higher than if we trained it on the full data set. The
  standard procedure in this case is to report your error using the
  holdout set, and then train a final model using all your data.
\item
  A metric that minimizes false positives, by rarely flagging players
  and teams that fail to achieve the desired outcome, is specific. A
  metric that minimizes false negatives, by rarely failing to flag
  players and teams that achieve the desired outcome, is sensitive.
\item
  Although the previous code example was useful to illustrate how k-fold
  cross-validation works, scikit-learn also implements a k-fold
  cross-validation scorer, which allows us to evaluate our model using
  stratified k-fold cross-validation more efficiently
\item
  Sometimes it may be more useful to report the coefficient of
  determination (), which can be understood as a standardized version of
  the MSE, for better interpretability of the model performance. In
  other words, is the fraction of response variance that is captured by
  the model. The value is defined as follows:Here, SSE is the sum of
  squared errors and SST is the total sum of squares , or in other
  words, it is simply the variance of the response.
\item
  Most people start working with data from exactly the wrong end. They
  begin with a data set, then apply their favorite tools and techniques
  to it. The result is narrow questions and shallow arguments. Starting
  with data, without first doing a lot of thinking, without having any
  structure, is a short road to simple questions and unsurprising
  results. We don't want unsurprising---we want knowledge.
\item
  Remember that the positive class in scikit-learn is the class that is
  labeled as class 1. If we want to specify a different positive label,
  we can construct our own scorer via the make\_scorer function, which
  we can then directly provide as an argument to the scoring parameter
  in GridSearchCV
\item
  While the weighted macro-average is the default for multiclass
  problems in scikit-learn, we can specify the averaging method via the
  average parameter inside the different scoring functions that we
  import from the sklean.metrics module, for example, the
  precision\_score or make\_scorer functions
\item
  Transforming words into feature vectorsTo construct a bag-of-words
  model based on the word counts in the respective documents, we can use
  the CountVectorizer class implemented in scikit-learn. As we will see
  in the following code section, the CountVectorizer class takes an
  array of text data, which can be documents or just sentences, and
  constructs the bag-of-words model for us:
\item
  Scikit-learn implements yet another transformer, the TfidfTransformer,
  that takes the raw term frequencies from CountVectorizer as input and
  transforms them into tf-idfs.
\item
  Naive Bayes is a linear classifier, while k-NN is not. The curse of
  dimensionality and large feature sets are a problem for k-NN, while
  Naive Bayes performs well. k-NN requires no training (just load in the
  dataset), whereas Naive Bayes does.
\end{itemize}

\subsection{Clustering}\label{clustering}

\begin{itemize}
\item
  \href{http://scikit-learn.org/stable/modules/clustering.html\#clustering}{Comparison}
\item
  \href{https://twitter.com/thomaswdinsmore/status/965223193043718145}{Rules}
\item
  \url{https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68}
\item
  \url{http://hunch.net/?p=224}
\end{itemize}

\chapter{Communicate, Deploy,
Maintain}\label{communicate-deploy-maintain}

\section{Communicate}\label{communicate-1}

Summarize the motivation and goals of the project. It should either be
an executive summary or a technical abstract.

State results with details as needed. For an end user presentation show
how the model fits into and improves the user's workflow, and how to use
it.

Discuss recommendations, outstanding issues, and possible future work.

Introduction \textbar{} Motivation \textbar{} Summary \textbar{} Results
\textbar{} Conclusion

\section{Deploy}\label{deploy}

\section{Maintain}\label{maintain}

\chapter{Frameworks}\label{frameworks}

\section{PySpark}\label{pyspark}

A
{[}cheatsheet{[}(\url{https://www.qubole.com/resources/pyspark-cheatsheet/}).

Pipeline

Setup

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ findspark}
\NormalTok{findspark.init()}
\ImportTok{import}\NormalTok{ pyspark}
\ImportTok{from}\NormalTok{ pyspark.sql }\ImportTok{import}\NormalTok{ SparkSession}
\NormalTok{sc }\OperatorTok{=}\NormalTok{ pyspark.SparkContext()}
\NormalTok{spark }\OperatorTok{=}\NormalTok{ SparkSession.builder.appName(}\StringTok{'example'}\NormalTok{).getOrCreate()}
\ImportTok{import}\NormalTok{ findspark}
\NormalTok{findspark.init()}
\ImportTok{import}\NormalTok{ pyspark}
\ImportTok{import}\NormalTok{ random}
\NormalTok{sc }\OperatorTok{=}\NormalTok{ pyspark.SparkContext(appName}\OperatorTok{=}\StringTok{"Pi"}\NormalTok{)}
\NormalTok{num_samples }\OperatorTok{=} \DecValTok{100000000}
\KeywordTok{def}\NormalTok{ inside(p):     }
\NormalTok{    x, y }\OperatorTok{=}\NormalTok{ random.random(), random.random()}
    \ControlFlowTok{return}\NormalTok{ x}\OperatorTok{*}\NormalTok{x }\OperatorTok{+}\NormalTok{ y}\OperatorTok{*}\NormalTok{y }\OperatorTok{<} \DecValTok{1}
\NormalTok{count }\OperatorTok{=}\NormalTok{ sc.parallelize(}\BuiltInTok{range}\NormalTok{(}\DecValTok{0}\NormalTok{, num_samples)).}\BuiltInTok{filter}\NormalTok{(inside).count()}
\NormalTok{pi }\OperatorTok{=} \DecValTok{4} \OperatorTok{*}\NormalTok{ count }\OperatorTok{/}\NormalTok{ num_samples}
\BuiltInTok{print}\NormalTok{(pi)}
\NormalTok{sc.stop()}
\end{Highlighting}
\end{Shaded}

Libs

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ pyspark.ml.feature }\ImportTok{import}\NormalTok{ StringIndexer, OneHotEncoder, VectorAssembler}
\ImportTok{from}\NormalTok{ pyspark.ml }\ImportTok{import}\NormalTok{ Pipeline}
\ImportTok{from}\NormalTok{ pyspark.ml.classification }\ImportTok{import}\NormalTok{ LogisticRegression}
\ImportTok{import}\NormalTok{ pyspark.ml.evaluation }\ImportTok{as}\NormalTok{ evals}
\ImportTok{import}\NormalTok{ pyspark.ml.tuning }\ImportTok{as}\NormalTok{ tune}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{df }\OperatorTok{=}\NormalTok{ spark.read.csv(}\StringTok{'data.csv'}\NormalTok{, header }\OperatorTok{=} \VariableTok{True}\NormalTok{)}
\NormalTok{df.show(}\DecValTok{5}\NormalTok{)}
\NormalTok{df.columns}
\end{Highlighting}
\end{Shaded}

Preprocessing

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\OperatorTok{=}\NormalTok{ df.withColumn(}\StringTok{"label"}\NormalTok{, df[}\StringTok{"target"}\NormalTok{].cast(}\StringTok{'integer'}\NormalTok{))}
\NormalTok{scf_indexer }\OperatorTok{=}\NormalTok{ StringIndexer(inputCol }\OperatorTok{=} \StringTok{"some_cat_feature"}\NormalTok{, outputCol }\OperatorTok{=} \StringTok{"some_cat_feature_index"}\NormalTok{)}
\NormalTok{scf_encoder }\OperatorTok{=}\NormalTok{ OneHotEncoder(inputCol }\OperatorTok{=} \StringTok{"some_cat_feature_index"}\NormalTok{, outputCol }\OperatorTok{=} \StringTok{"some_cat_feature_fact"}\NormalTok{)}
\NormalTok{feature_cols }\OperatorTok{=}\NormalTok{ [}\StringTok{"some list of column names"}\NormalTok{]}
\NormalTok{vec_assembler }\OperatorTok{=}\NormalTok{ VectorAssembler(inputCols }\OperatorTok{=}\NormalTok{ feature_cols, }
\NormalTok{                                outputCol }\OperatorTok{=} \StringTok{"features"}\NormalTok{)}
\NormalTok{pipe }\OperatorTok{=}\NormalTok{ Pipeline(stages }\OperatorTok{=}\NormalTok{ [scf_indexer, scf_encoder, vec_assembler])}
\NormalTok{piped_data }\OperatorTok{=}\NormalTok{ pipe.fit(df).transform(df)}
\NormalTok{training, test }\OperatorTok{=}\NormalTok{ piped_data.randomSplit([.}\DecValTok{8}\NormalTok{, .}\DecValTok{2}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

Model

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{clf_lr }\OperatorTok{=}\NormalTok{ LogisticRegression()}
\NormalTok{evaluator }\OperatorTok{=}\NormalTok{ evals.BinaryClassificationEvaluator(metricName }\OperatorTok{=} \StringTok{"areaUnderROC"}\NormalTok{)}
\NormalTok{grid }\OperatorTok{=}\NormalTok{ tune.ParamGridBuilder()}
\NormalTok{grid }\OperatorTok{=}\NormalTok{ grid.addGrid(clf_lr.regParam, np.arange(}\DecValTok{0}\NormalTok{, .}\DecValTok{1}\NormalTok{, .}\DecValTok{01}\NormalTok{))}
\NormalTok{grid }\OperatorTok{=}\NormalTok{ grid.addGrid(clf_lr.elasticNetParam, [}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{])}
\NormalTok{grid }\OperatorTok{=}\NormalTok{ grid.build()}
\NormalTok{clf_lr_cv }\OperatorTok{=}\NormalTok{ tune.CrossValidator(}
\NormalTok{    estimator }\OperatorTok{=}\NormalTok{ clf_lr,}
\NormalTok{    estimatorParamMaps }\OperatorTok{=}\NormalTok{ grid,}
\NormalTok{    evaluator }\OperatorTok{=}\NormalTok{ evaluator,}
\NormalTok{    numFolds }\OperatorTok{=} \DecValTok{5}
\NormalTok{               )}
\NormalTok{best_clf_lr }\OperatorTok{=}\NormalTok{ clf_lr_cv.fit(training).bestModel}
\NormalTok{results }\OperatorTok{=}\NormalTok{ best_clf_lr.transform(training)}
\BuiltInTok{print}\NormalTok{(evaluator.evaluate(results))}
\CommentTok{#stop}
\NormalTok{sc.stop()}
\end{Highlighting}
\end{Shaded}

Quick Test:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ findspark}
\NormalTok{findspark.init()}
\ImportTok{import}\NormalTok{ pyspark}
\ImportTok{import}\NormalTok{ random}
\NormalTok{sc }\OperatorTok{=}\NormalTok{ pyspark.SparkContext(appName}\OperatorTok{=}\StringTok{"Pi"}\NormalTok{)}
\NormalTok{num_samples }\OperatorTok{=} \DecValTok{100000000}
\KeywordTok{def}\NormalTok{ inside(p):     }
\NormalTok{    x, y }\OperatorTok{=}\NormalTok{ random.random(), random.random()}
    \ControlFlowTok{return}\NormalTok{ x}\OperatorTok{*}\NormalTok{x }\OperatorTok{+}\NormalTok{ y}\OperatorTok{*}\NormalTok{y }\OperatorTok{<} \DecValTok{1}
\NormalTok{count }\OperatorTok{=}\NormalTok{ sc.parallelize(}\BuiltInTok{range}\NormalTok{(}\DecValTok{0}\NormalTok{, num_samples)).}\BuiltInTok{filter}\NormalTok{(inside).count()}
\NormalTok{pi }\OperatorTok{=} \DecValTok{4} \OperatorTok{*}\NormalTok{ count }\OperatorTok{/}\NormalTok{ num_samples}
\BuiltInTok{print}\NormalTok{(pi)}
\NormalTok{sc.stop()}
\end{Highlighting}
\end{Shaded}

First have to create a SparkSession object from your SparkContext. The
SparkContext is ther connection to the cluster and the SparkSession as
your interface with that connection.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pyspark}
\ImportTok{from}\NormalTok{ pyspark.sql }\ImportTok{import}\NormalTok{ SparkSession}
\NormalTok{sc }\OperatorTok{=}\NormalTok{ pyspark.SparkContext()}
\NormalTok{spark }\OperatorTok{=}\NormalTok{ SparkSession.builder.getOrCreate() }\CommentTok{# SparkSession.builder.appName('chosenName').getOrCreate()}
\end{Highlighting}
\end{Shaded}

The rest of my notes:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## List tables}
\NormalTok{spark.catalog.listTables()}
\CommentTok{## Access and display data}
\NormalTok{query }\OperatorTok{=} \StringTok{"FROM flights SELECT * LIMIT 10"}
\NormalTok{flights10 }\OperatorTok{=}\NormalTok{ spark.sql(query)}
\NormalTok{flights10.show()}
\CommentTok{## Cast}
\NormalTok{df[}\StringTok{'g'}\NormalTok{] }\OperatorTok{=}\NormalTok{ df[}\StringTok{'g'}\NormalTok{].astype(}\BuiltInTok{str}\NormalTok{)}
\CommentTok{## Read from csv}
\NormalTok{df}\OperatorTok{=}\NormalTok{ spark.read.csv(file_path, header }\OperatorTok{=} \VariableTok{True}\NormalTok{)}
\NormalTok{df }\OperatorTok{=}\NormalTok{ spark.read.csv(}\StringTok{'fileNameWithPath'}\NormalTok{, mode}\OperatorTok{=}\StringTok{"DROPMALFORMED"}\NormalTok{,inferSchema}\OperatorTok{=}\VariableTok{True}\NormalTok{, header }\OperatorTok{=} \VariableTok{True}\NormalTok{)}
\NormalTok{df }\OperatorTok{=}\NormalTok{ spark.read.}\BuiltInTok{format}\NormalTok{(}\StringTok{"csv"}\NormalTok{).option(}\StringTok{"header"}\NormalTok{,}\StringTok{"true"}\NormalTok{).option(}\StringTok{"inferSchema"}\NormalTok{,}\StringTok{"true"}\NormalTok{).load(}\StringTok{"john_doe.csv"}\NormalTok{)}
\CommentTok{## Spark df to pandas and vice versa}
\NormalTok{toPandas()}
\NormalTok{spark_temp }\OperatorTok{=}\NormalTok{ spark.createDataFrame(pd_temp)}
\CommentTok{## Add to the catalog}
\NormalTok{spark_temp.createOrReplaceTempView(}\StringTok{"temp"}\NormalTok{)}
\CommentTok{## Mutate}
\NormalTok{df }\OperatorTok{=}\NormalTok{ df.withColumn(}\StringTok{"newCol"}\NormalTok{, df.oldCol }\OperatorTok{+} \DecValTok{1}\NormalTok{)}
\NormalTok{model_data }\OperatorTok{=}\NormalTok{ model_data.withColumn(}\StringTok{"arr_delay"}\NormalTok{, model_data.arr_delay.cast(}\StringTok{'integer'}\NormalTok{))}
\NormalTok{model_data }\OperatorTok{=}\NormalTok{ model_data.withColumn(}\StringTok{"plane_age"}\NormalTok{, model_data.year }\OperatorTok{-}\NormalTok{ model_data.plane_year)}
\CommentTok{## Create the DataFrame flights}
\NormalTok{flights }\OperatorTok{=}\NormalTok{ spark.table(}\StringTok{'flights'}\NormalTok{)}
\CommentTok{## Get column names}
\NormalTok{spark_df.schema.names}
\NormalTok{spark_df.printSchema()}
\CommentTok{## Filter: takes either a Spark Column of boolean (True/False) values or the WHERE clause of a SQL expression as a string}
\NormalTok{long_flights1 }\OperatorTok{=}\NormalTok{ flights.}\BuiltInTok{filter}\NormalTok{(}\StringTok{'distance > 1000'}\NormalTok{)}
\NormalTok{long_flights2 }\OperatorTok{=}\NormalTok{ flights.}\BuiltInTok{filter}\NormalTok{(flights.distance }\OperatorTok{>} \DecValTok{1000}\NormalTok{)}
\NormalTok{model_data }\OperatorTok{=}\NormalTok{ model_data.}\BuiltInTok{filter}\NormalTok{(}\StringTok{"arr_delay is not NULL and dep_delay is not NULL and air_time is not NULL and plane_year is not NULL"}\NormalTok{)}
\CommentTok{## Groupby examples}
\NormalTok{flights.}\BuiltInTok{filter}\NormalTok{(flights.origin }\OperatorTok{==} \StringTok{"PDX"}\NormalTok{).groupBy().}\BuiltInTok{min}\NormalTok{(}\StringTok{"distance"}\NormalTok{).show()}
\NormalTok{flights.}\BuiltInTok{filter}\NormalTok{(flights.carrier}\OperatorTok{==}\StringTok{'DL'}\NormalTok{).}\BuiltInTok{filter}\NormalTok{(flights.origin}\OperatorTok{==}\StringTok{'SEA'}\NormalTok{).groupBy().avg(}\StringTok{'air_time'}\NormalTok{).show()}
\NormalTok{flights.withColumn(}\StringTok{"duration_hrs"}\NormalTok{, flights.air_time}\OperatorTok{/}\DecValTok{60}\NormalTok{).groupBy().}\BuiltInTok{sum}\NormalTok{(}\StringTok{'duration_hrs'}\NormalTok{).show()}
\CommentTok{## Spark functions}
\ImportTok{import}\NormalTok{ pyspark.sql.functions }\ImportTok{as}\NormalTok{ F}
\NormalTok{by_month_dest.agg(F.stddev(}\StringTok{'dep_delay'}\NormalTok{)).show()}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## Drop column}
\NormalTok{final_test_data.drop(}\StringTok{'State'}\NormalTok{)}
\CommentTok{## Dummying}
\CommentTok{#The first step to encoding your categorical feature is to create a StringIndexer. Members of this class are #Estimators that take a DataFrame with a column of strings and map each unique string to a number. Then, the #Estimator returns a Transformer that takes a DataFrame, attaches the mapping to it as metadata, and returns a #new DataFrame with a numeric column corresponding to the string column.}
\CommentTok{#The second step is to encode this numeric column as a one-hot vector using a OneHotEncoder. This works exactly #the same way as the StringIndexer by creating an Estimator and then a Transformer}
\CommentTok{## Create a StringIndexer}
\NormalTok{carr_indexer }\OperatorTok{=}\NormalTok{ StringIndexer(inputCol}\OperatorTok{=}\StringTok{"carrier"}\NormalTok{, outputCol}\OperatorTok{=}\StringTok{"carrier_index"}\NormalTok{)}
\CommentTok{## Create a OneHotEncoder}
\NormalTok{carr_encoder }\OperatorTok{=}\NormalTok{ OneHotEncoder(inputCol}\OperatorTok{=}\StringTok{"carrier_index"}\NormalTok{, outputCol}\OperatorTok{=}\StringTok{"carrier_fact"}\NormalTok{)}
\CommentTok{## Make a VectorAssembler}
\NormalTok{vec_assembler }\OperatorTok{=}\NormalTok{ VectorAssembler(inputCols}\OperatorTok{=}\NormalTok{[}\StringTok{"month"}\NormalTok{, }\StringTok{"air_time"}\NormalTok{, }\StringTok{"carrier_fact"}\NormalTok{, }\StringTok{"dest_fact"}\NormalTok{, }\StringTok{"plane_age"}\NormalTok{], outputCol}\OperatorTok{=}\StringTok{"features"}\NormalTok{)}
\CommentTok{## Import & Make Pipeline}
\ImportTok{from}\NormalTok{ pyspark.ml }\ImportTok{import}\NormalTok{ Pipeline}
\NormalTok{flights_pipe }\OperatorTok{=}\NormalTok{ Pipeline(stages}\OperatorTok{=}\NormalTok{[dest_indexer, dest_encoder, carr_indexer, carr_encoder, vec_assembler])}
\CommentTok{## Fit and transform the data}
\NormalTok{piped_data }\OperatorTok{=}\NormalTok{ flights_pipe.fit(model_data).transform(model_data)}
\CommentTok{## Train-test split}
\NormalTok{training, test }\OperatorTok{=}\NormalTok{ piped_data.randomSplit([.}\DecValTok{6}\NormalTok{, .}\DecValTok{4}\NormalTok{])}
\CommentTok{## Tuning & Selection}
\CommentTok{## Import LogisticRegression}
\ImportTok{from}\NormalTok{ pyspark.ml.classification }\ImportTok{import}\NormalTok{ LogisticRegression}
\CommentTok{## Create a LogisticRegression Estimator}
\NormalTok{lr }\OperatorTok{=}\NormalTok{ LogisticRegression()}
\CommentTok{## Import the evaluation submodule}
\ImportTok{import}\NormalTok{ pyspark.ml.evaluation }\ImportTok{as}\NormalTok{ evals}
\CommentTok{## Create a BinaryClassificationEvaluator}
\NormalTok{evaluator }\OperatorTok{=}\NormalTok{ evals.BinaryClassificationEvaluator(metricName}\OperatorTok{=}\StringTok{"areaUnderROC"}\NormalTok{)}
\CommentTok{## Import the tuning submodule}
\ImportTok{import}\NormalTok{ pyspark.ml.tuning }\ImportTok{as}\NormalTok{ tune}
\CommentTok{## Create the parameter grid}
\NormalTok{grid }\OperatorTok{=}\NormalTok{ tune.ParamGridBuilder()}
\CommentTok{## Add the hyperparameter}
\NormalTok{grid }\OperatorTok{=}\NormalTok{ grid.addGrid(lr.regParam, np.arange(}\DecValTok{0}\NormalTok{, .}\DecValTok{1}\NormalTok{, .}\DecValTok{01}\NormalTok{))}
\NormalTok{grid }\OperatorTok{=}\NormalTok{ grid.addGrid(lr.elasticNetParam, [}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{])}
\CommentTok{## Build the grid}
\NormalTok{grid }\OperatorTok{=}\NormalTok{ grid.build()}
\CommentTok{## Create the CrossValidator}
\NormalTok{cv }\OperatorTok{=}\NormalTok{ tune.CrossValidator(}
\NormalTok{estimator}\OperatorTok{=}\NormalTok{lr,}
\NormalTok{estimatorParamMaps}\OperatorTok{=}\NormalTok{grid,}
\NormalTok{evaluator}\OperatorTok{=}\NormalTok{evaluator}
\NormalTok{               )}
\CommentTok{## Fit cross validation models}
\NormalTok{models }\OperatorTok{=}\NormalTok{ cv.fit(training)}
\CommentTok{## Extract the best model}
\NormalTok{best_lr }\OperatorTok{=}\NormalTok{ models.bestModel}
\CommentTok{## Use the model to predict the test set}
\NormalTok{test_results }\OperatorTok{=}\NormalTok{ best_lr.transform(test)}
\CommentTok{## Evaluate the predictions}
\BuiltInTok{print}\NormalTok{(evaluator.evaluate(test_results))}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  Spark's assumes the target is called `label' in ML. Everything else is
  a feature.
\item
  alias() method to rename a column you're selecting.
\item
  cast() method: nominative determinism on columns
\item
  withColumn(): create new column
\item
  pyspark.ml.feature
\item
  At the core of the pyspark.ml module are the Transformer and Estimator
  classes. Transformer classes are used for pre-processing and have a
  .transform() method. eg. PCA
\item
  You can create what are called `one-hot vectors' to represent the
  carrier and the destination of each flight. A one-hot vector is a way
  of representing a categorical feature where every observation has a
  vector in which all elements are zero except for at most one element,
  which has a value of one (1).
\item
  Estimator classes are for modeling and all implement a .fit() method.
  eg. StringIndexerModel for including categorical data saved as strings
  in your models
\item
  selectExpr() takes SQL expressions as a string
\item
  show() vs collect()
\item
  Spark only handles numeric data
\item
  In Spark it's important to make sure you split the data after all the
  transformations. This is because operations like StringIndexer don't
  always produce the same index even when given the same list of
  strings.
\end{itemize}

\section{Sparklyr}\label{sparklyr}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(sparklyr)}

\NormalTok{## Connect to your Spark cluster}
\NormalTok{spark_conn <-}\StringTok{ }\KeywordTok{spark_connect}\NormalTok{(}\DataTypeTok{master =} \StringTok{"local"}\NormalTok{)}

\NormalTok{## Print the version of Spark}
\KeywordTok{spark_version}\NormalTok{(}\DataTypeTok{sc =}\NormalTok{ spark_conn)}

\NormalTok{## Disconnect from Spark}
\KeywordTok{spark_disconnect}\NormalTok{(}\DataTypeTok{sc =}\NormalTok{ spark_conn)}

\NormalTok{## See tables}
\KeywordTok{src_tbls}\NormalTok{(sc)}

\NormalTok{## Copy track_metadata to Spark}
\NormalTok{track_metadata_tbl <-}\StringTok{ }\KeywordTok{copy_to}\NormalTok{(spark_conn, track_metadata)}

\NormalTok{## Print 5 rows, all columns}
\KeywordTok{print}\NormalTok{(track_metadata_tbl, }\DataTypeTok{n =} \DecValTok{5}\NormalTok{, }\DataTypeTok{width =} \OtherTok{Inf}\NormalTok{)}

\NormalTok{## Write and run SQL query}
\NormalTok{query <-}\StringTok{ "SELECT * FROM track_metadata WHERE year < 1935 AND duration > 300"}
\NormalTok{(results <-}\StringTok{ }\KeywordTok{dbGetQuery}\NormalTok{(spark_conn, query))}

\NormalTok{## General transformation structure and example}
\NormalTok{a_tibble }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ft_some_transformation}\NormalTok{(}\StringTok{"x"}\NormalTok{, }\StringTok{"y"}\NormalTok{, some_other_args)}

\NormalTok{hotttnesss <-}\StringTok{ }\NormalTok{track_metadata_tbl }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# Select artist_hotttnesss}
\StringTok{  }\KeywordTok{select}\NormalTok{(artist_hotttnesss) }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# Binarize to is_hottt_or_nottt}
\StringTok{  }\KeywordTok{ft_binarizer}\NormalTok{(}\StringTok{'artist_hotttnesss'}\NormalTok{, }\StringTok{'is_hottt_or_nottt'}\NormalTok{, }\DataTypeTok{threshold =}\NormalTok{ .}\DecValTok{5}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# Collect the result}
\StringTok{  }\KeywordTok{collect}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# Convert is_hottt_or_nottt to logical}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{is_hottt_or_nottt =} \KeywordTok{as.logical}\NormalTok{(is_hottt_or_nottt))}
  
\NormalTok{## Get and transform the schema}

\NormalTok{(schema <-}\StringTok{ }\KeywordTok{sdf_schema}\NormalTok{(track_metadata_tbl))}
\NormalTok{schema }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{lapply}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{do.call}\NormalTok{(data_frame, x)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{bind_rows}\NormalTok{()}

\NormalTok{## Train-test split}
\NormalTok{partitioned <-}\StringTok{ }\NormalTok{track_metadata_tbl }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{sdf_partition}\NormalTok{(}\DataTypeTok{training =} \FloatTok{0.7}\NormalTok{, }\DataTypeTok{testing =} \FloatTok{0.3}\NormalTok{)}

\NormalTok{## List ml functions}
\KeywordTok{ls}\NormalTok{(}\StringTok{"package:sparklyr"}\NormalTok{, }\DataTypeTok{pattern =} \StringTok{"^ml"}\NormalTok{)}

\NormalTok{## GBT Example}

\NormalTok{gradient_boosted_trees_model <-}\StringTok{ }\NormalTok{track_data_to_model_tbl }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# Run the gradient boosted trees model}
\StringTok{   }\KeywordTok{ml_gradient_boosted_trees}\NormalTok{(}\StringTok{'year'}\NormalTok{, feature_colnames)}

\NormalTok{responses <-}\StringTok{ }\NormalTok{track_data_to_predict_tbl }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# Select the year column}
\StringTok{  }\KeywordTok{select}\NormalTok{(year) }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# Collect the results}
\StringTok{  }\KeywordTok{collect}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# Add in the predictions}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \DataTypeTok{predicted_year =} \KeywordTok{predict}\NormalTok{(}
\NormalTok{      gradient_boosted_trees_model,}
\NormalTok{      track_data_to_predict_tbl}
\NormalTok{    )}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\textbf{Parquet Aside}

Parquet files are quicker to read and write. parquet files can be used
with other tools in the Hadoop ecosystem, like Shark, Impala, Hive, and
Pig.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{## The parquet_dir has been pre-defined}
\NormalTok{parquet_dir}

\NormalTok{## List the files in the parquet dir}
\NormalTok{filenames <-}\StringTok{ }\KeywordTok{dir}\NormalTok{(parquet_dir, }\DataTypeTok{full.names =} \OtherTok{TRUE}\NormalTok{)}

\NormalTok{## Show the filenames and their sizes}
\KeywordTok{data_frame}\NormalTok{(}
  \DataTypeTok{filename =} \KeywordTok{basename}\NormalTok{(filenames),}
  \DataTypeTok{size_bytes =} \KeywordTok{file.size}\NormalTok{(filenames)}
\NormalTok{)}

\CommentTok{#Import the data into Spark}

\NormalTok{timbre_tbl <-}\StringTok{ }\KeywordTok{spark_read_parquet}\NormalTok{(spark_conn, }\StringTok{'timbre'}\NormalTok{, parquet_dir)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  To collect your data: that is, to move it from Spark to R, you call
  collect(). copy\_to() moves your data from R to Spark.
\item
  Use compute() to compute the calculation, but store the results in a
  temporary data frame on Spark.
\item
  If you want to delay returning the data, you can use dbSendQuery() to
  execute the query, then dbFetch() to return the results.
\item
  feature transforms: ft\_, ml functions: ml\_, spark df functions:
  sdf\_
\item
  ft\_tokenizer(): to lower and split into individual words,
  ft\_regex\_tokenizer, sdf\_sort
\end{itemize}

\section{Caret}\label{caret}

attributes(clf\_lr): results, finalModel

\subsection{Preprocessing}\label{preprocessing}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Apply median imputation: model}
\NormalTok{model <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ breast_cancer_x, }
  \DataTypeTok{y =}\NormalTok{ breast_cancer_y,}
  \DataTypeTok{method =} \StringTok{'glm'}\NormalTok{,}
  \DataTypeTok{trControl =}\NormalTok{ myControl,}
  \DataTypeTok{preProcess =} \KeywordTok{c}\NormalTok{(}\StringTok{'medianImpute'}\NormalTok{, }\StringTok{"knnImpute"}\NormalTok{, }\StringTok{'center'}\NormalTok{, }\StringTok{'scale'}\NormalTok{, }\StringTok{'pca'}\NormalTok{)}
\NormalTok{)}

\KeywordTok{dotplot}\NormalTok{(resamples, }\DataTypeTok{metric =} \StringTok{"ROC"}\NormalTok{)}

\KeywordTok{min}\NormalTok{(model}\OperatorTok{$}\NormalTok{results}\OperatorTok{$}\NormalTok{RSME)}

\CommentTok{# Identify near zero variance predictors: remove_cols}
\NormalTok{remove_cols <-}\StringTok{ }\KeywordTok{nearZeroVar}\NormalTok{(bloodbrain_x, }\DataTypeTok{names =} \OtherTok{TRUE}\NormalTok{, }
                           \DataTypeTok{freqCut =} \DecValTok{2}\NormalTok{, }\DataTypeTok{uniqueCut =} \DecValTok{20}\NormalTok{)}

\CommentTok{# Get all column names from bloodbrain_x: all_cols}
\NormalTok{all_cols =}\StringTok{ }\KeywordTok{colnames}\NormalTok{(bloodbrain_x)}

\CommentTok{# Remove from data: bloodbrain_x_small}
\NormalTok{bloodbrain_x_small <-}\StringTok{ }\NormalTok{bloodbrain_x[ , }\KeywordTok{setdiff}\NormalTok{(all_cols, remove_cols)]}
\end{Highlighting}
\end{Shaded}

\subsection{LM}\label{lm}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Set seed}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}

\CommentTok{# Fit lm model: model}
\NormalTok{model <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(price }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ diamonds)}

\CommentTok{# Predict on full data: p}
\NormalTok{p <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(model, diamonds)}

\CommentTok{# Compute errors: error}
\NormalTok{error <-}\StringTok{ }\NormalTok{p }\OperatorTok{-}\StringTok{ }\NormalTok{diamonds}\OperatorTok{$}\NormalTok{price}

\CommentTok{# Calculate RMSE}
\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{mean}\NormalTok{(error}\OperatorTok{^}\DecValTok{2}\NormalTok{))}

\CommentTok{# Fit lm model using 5 x 5-fold CV: model}
\NormalTok{model <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
\NormalTok{  medv }\OperatorTok{~}\StringTok{ }\NormalTok{., }
\NormalTok{  Boston,}
  \DataTypeTok{method =} \StringTok{"lm"}\NormalTok{,}
  \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}
    \DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{5}\NormalTok{,}
    \DataTypeTok{repeats =} \DecValTok{5}\NormalTok{, }\DataTypeTok{verboseIter =} \OtherTok{TRUE}
\NormalTok{  )}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Lasso/Ridge}\label{lassoridge}

Alpha controls balance between lasso and ridge. Lambda controls penalty.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Fit glmnet model: model}
\NormalTok{model <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
\NormalTok{  y}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ overfit,}
  \DataTypeTok{method =} \StringTok{"glmnet"}\NormalTok{,}
  \DataTypeTok{trControl =}\NormalTok{ myControl}
\NormalTok{)}

\CommentTok{# Train glmnet with custom trainControl and tuning: model}
\NormalTok{model <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
\NormalTok{  y}\OperatorTok{~}\NormalTok{., overfit,}
  \DataTypeTok{tuneGrid =} \KeywordTok{expand.grid}\NormalTok{(}\DataTypeTok{alpha =} \DecValTok{0}\OperatorTok{:}\DecValTok{1}\NormalTok{, }\DataTypeTok{lambda =} \KeywordTok{seq}\NormalTok{(}\FloatTok{0.0001}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DataTypeTok{length =} \DecValTok{20}\NormalTok{)),}
  \DataTypeTok{method =} \StringTok{'glmnet'}\NormalTok{,}
  \DataTypeTok{trControl =}\NormalTok{ myControl}
\NormalTok{)}

\CommentTok{# Print model to console}
\NormalTok{model}

\CommentTok{# Print maximum ROC statistic}
\KeywordTok{max}\NormalTok{(model[[}\StringTok{"results"}\NormalTok{]][[}\StringTok{"ROC"}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\subsection{Logistic Regression}\label{logistic-regression-1}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Fit glm model}
\NormalTok{model =}\StringTok{ }\KeywordTok{glm}\NormalTok{(Class }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{, train)}

\CommentTok{# Predict on test: p}
\NormalTok{p =}\StringTok{ }\KeywordTok{predict}\NormalTok{(model, test, }\DataTypeTok{type =} \StringTok{"response"}\NormalTok{)}

\CommentTok{# Calculate class probabilities: p_class}
\NormalTok{p_class <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(p }\OperatorTok{>}\StringTok{ }\FloatTok{0.50}\NormalTok{, }\StringTok{"M"}\NormalTok{, }\StringTok{"R"}\NormalTok{)}

\CommentTok{# Create confusion matrix}
\KeywordTok{confusionMatrix}\NormalTok{(p_class, test}\OperatorTok{$}\NormalTok{Class)}
\end{Highlighting}
\end{Shaded}

\subsection{Train-Test Split, Folds, \&
CV}\label{train-test-split-folds-cv}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Shuffle row indices: rows}
\NormalTok{rows =}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(Sonar))}

\CommentTok{# Randomly order data: Sonar}
\NormalTok{Sonar =}\StringTok{ }\NormalTok{Sonar[rows,]}

\CommentTok{# Identify row to split on: split}
\NormalTok{split <-}\StringTok{ }\KeywordTok{round}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(Sonar) }\OperatorTok{*}\StringTok{ }\NormalTok{.}\DecValTok{6}\NormalTok{)}

\CommentTok{# Create train: 60%}
\NormalTok{train =}\StringTok{ }\NormalTok{Sonar[}\DecValTok{1}\OperatorTok{:}\NormalTok{split,]}

\CommentTok{# Create test: 40%}
\NormalTok{test =}\StringTok{ }\NormalTok{Sonar[(split}\OperatorTok{+}\DecValTok{1}\NormalTok{)}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(Sonar),]}

\CommentTok{# Create custom indices: myFolds}
\NormalTok{myFolds <-}\StringTok{ }\KeywordTok{createFolds}\NormalTok{(churn_y, }\DataTypeTok{k =} \DecValTok{5}\NormalTok{)}

\CommentTok{# Create reusable trainControl object: myControl}
\NormalTok{myControl <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}
  \DataTypeTok{summaryFunction =}\NormalTok{ twoClassSummary, }\CommentTok{#default option is default summary}
  \DataTypeTok{classProbs =} \OtherTok{TRUE}\NormalTok{, }\CommentTok{# IMPORTANT!}
  \DataTypeTok{verboseIter =} \OtherTok{TRUE}\NormalTok{,}
  \DataTypeTok{savePredictions =} \OtherTok{TRUE}\NormalTok{,}
  \DataTypeTok{index =}\NormalTok{ myFolds}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Random Forest}\label{random-forest-1}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Fit random forest: model}
\NormalTok{model <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
\NormalTok{  quality}\OperatorTok{~}\NormalTok{.,}
  \DataTypeTok{tuneLength =} \DecValTok{3}\NormalTok{,}
  \CommentTok{#tuneGrid = data.frame(mtry = c(2, 3, 7)),}
  \DataTypeTok{data =}\NormalTok{ wine, }
  \DataTypeTok{method =} \StringTok{'ranger'}\NormalTok{,}
  \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{5}\NormalTok{, }\DataTypeTok{verboseIter =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{)}

\CommentTok{# Print model to console}
\NormalTok{model}

\CommentTok{# Plot model}
\KeywordTok{plot}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\subsection{SVM}\label{svm-1}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{clf_svm <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
\NormalTok{  pass }\OperatorTok{~}\NormalTok{., }
  \DataTypeTok{data =}\NormalTok{ df3, }
  \DataTypeTok{method =} \StringTok{"svmLinear"}\NormalTok{,}
  \DataTypeTok{preProcess =} \KeywordTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{)}
\NormalTok{                 )}
\end{Highlighting}
\end{Shaded}

\subsection{Model Selection}\label{model-selection-1}

Now that you have fit two models to the churn dataset, it's time to
compare their out-of-sample predictions and choose which one is the best
model for your dataset.

You can compare models in caret using the resamples() function, provided
they have the same training data and use the same trainControl object
with preset cross-validation folds. resamples() takes as input a list of
models and can be used to compare dozens of models at once (though in
this case you are only comparing two models).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Create model_list}
\NormalTok{model_list <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{item1 =}\NormalTok{ model_glmnet, }\DataTypeTok{item2 =}\NormalTok{ model_rf)}

\CommentTok{# Pass model_list to resamples(): resamples}
\NormalTok{resamples =}\StringTok{ }\KeywordTok{resamples}\NormalTok{(model_list)}

\CommentTok{# Summarize the results}
\KeywordTok{summary}\NormalTok{(resamples)}

\CommentTok{# Create bwplot}
\KeywordTok{bwplot}\NormalTok{(resamples, }\DataTypeTok{metric =} \StringTok{'ROC'}\NormalTok{)}

\CommentTok{# Create xyplot}
\KeywordTok{xyplot}\NormalTok{(resamples)}

\CommentTok{# Predict on test: p}
\NormalTok{p =}\StringTok{ }\KeywordTok{predict}\NormalTok{(model, test, }\DataTypeTok{type =} \StringTok{'response'}\NormalTok{)}

\CommentTok{# Make ROC curve}
\KeywordTok{colAUC}\NormalTok{(p, test}\OperatorTok{$}\NormalTok{Class, }\DataTypeTok{plotROC =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  caret: finalModel of model object
\end{itemize}

\subsection{Stacking}\label{stacking}

caretEnsemble provides the caretList() function for creating multiple
caret models at once on the same dataset, using the same resampling
folds. You can also create your own lists of caret models. Use the
caretStack() function to make a stack of caret models, with the two
sub-models (glmnet and ranger) feeding into another (hopefully more
accurate!) caret model.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Create ensemble model: stack}
\NormalTok{stack <-}\StringTok{ }\KeywordTok{caretStack}\NormalTok{(model_list, }\DataTypeTok{method =} \StringTok{'glm'}\NormalTok{)}

\CommentTok{# Look at summary}
\KeywordTok{summary}\NormalTok{(stack)}
\end{Highlighting}
\end{Shaded}

\& caretEnnsemble

\section{Data Table}\label{data-table}

\begin{itemize}
\item
  Operations done by reference
\item
  dt{[}i, j, by{]} : subset by i calculate by j grouped using by
\item
  1: numeric \textbar{} 1L: integer
\item
  NA\_integer\_: integer
\item
  DT{[}.N{]} : prints last row
\item
  names(DT): colnames
\item
  dim(DT): dimensions
\item
  DT{[}, .(A, B){]}: returns two columns
\item
  DT{[}, c(A, B){]}: returns a concatenated vector
\item
  DT{[}, .(sum\_c = sum(C){]}
\item
  DT{[}, plot(A, C){]}
\item
  DT{[}, A:=NULL{]}: Remove column A
\item
  DT{[}, .(sumB = sum(B)), by = .(Grp = A\%\%2){]}
\item
  DT{[}, .N, by = Sepal.Width{]}: .N is the count of each group
\item
  DT{[}, lapply(.SD, median){]}: .SD is a placeholder for all the
  columns
\item
  dogs{[}, lapply(.SD, mean), .SDcols = 2:3{]}: Find mean of columns 2
  \& 3
\item
  for (i in 1:5) set(DT, i, 3L, i+1): update first 5 rows of 3rd column
\item
  setnames(DT, `y', `z'): changes colname from y to z
\item
  setkey(DT, A, B)
\item
  DT{[}.(`b'){]}
\item
  DT{[}.(c(`b', `c')){]}
\item
  DT{[}.(c(`b', `c')), mult=``first''{]}
\item
  DT{[}c(``b'', ``c''), .SD{[}c(1, .N){]}, by = .EACHI{]}: First and
  last row of the ``b'' and ``c'' groups
\item
  DT{[}c(``b'', ``c''), \{ print(.SD); .SD{[}c(1, .N){]} \}, by =
  .EACHI{]}
\item
  roll=TRUE, `nearest', -Inf, Inf \textbar{} rollends=FALSE
\end{itemize}

\section{Keras}\label{keras}

Here's a
\href{https://github.com/fastforwardlabs/keras-hello-world/blob/master/kerashelloworld.ipynb}{Hello
World} and the general framework:

\begin{itemize}
\item
  declare: sequential
\item
  add layers input-hidden-output
\item
  compile
\item
  fit
\end{itemize}

Keras only uses numpy arrays.

\begin{itemize}
\tightlist
\item
  keras.callbacks.EarlyStopping: Stop training when validation score
  stop improving after a certain number of epochs (baches?)
\end{itemize}

\subsection{Regression}\label{regression}

\begin{itemize}
\item
  loss = mean\_squared\_error
\item
  metric: rmse
\item
  activation\_function = relu
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ keras}
\ImportTok{from}\NormalTok{ keras.layers }\ImportTok{import}\NormalTok{ Dense}
\ImportTok{from}\NormalTok{ keras.models }\ImportTok{import}\NormalTok{ Sequential}
\CommentTok{# Specify the model: Two hidden layers}
\NormalTok{model }\OperatorTok{=}\NormalTok{ Sequential()}
\CommentTok{## Input}
\NormalTok{n_cols }\OperatorTok{=}\NormalTok{ predictors.shape[}\DecValTok{1}\NormalTok{]}
\NormalTok{model.add(Dense(}\DecValTok{50}\NormalTok{, activation}\OperatorTok{=}\StringTok{'relu'}\NormalTok{, input_shape }\OperatorTok{=}\NormalTok{ (n_cols,)))}
\CommentTok{# Hidden}
\NormalTok{model.add(Dense(}\DecValTok{32}\NormalTok{, activation}\OperatorTok{=}\StringTok{'relu'}\NormalTok{))}
\CommentTok{# Output}
\NormalTok{model.add(Dense(}\DecValTok{1}\NormalTok{))}
\CommentTok{# Compile the model}
\NormalTok{model.}\BuiltInTok{compile}\NormalTok{(optimizer }\OperatorTok{=} \StringTok{'adam'}\NormalTok{, loss }\OperatorTok{=} \StringTok{'mean_squared_error'}\NormalTok{) }
\CommentTok{# Fit the model}
\NormalTok{model.fit(predictors, target)}
\CommentTok{#Look at summary}
\NormalTok{model.summary()}
\CommentTok{# Calculate predictions: predictions}
\NormalTok{predictions }\OperatorTok{=}\NormalTok{ model.predict(pred_data)}
\end{Highlighting}
\end{Shaded}

\subsection{Classification}\label{classification}

\begin{itemize}
\item
  loss = categorical\_crossentropy
\item
  metric = accuracy
\item
  activation\_function = softmax
\item
  output layer with stuff equal to number of categorical groups
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ keras}
\ImportTok{from}\NormalTok{ keras.layers }\ImportTok{import}\NormalTok{ Dense}
\ImportTok{from}\NormalTok{ keras.models }\ImportTok{import}\NormalTok{ Sequential}
\CommentTok{# Specify the model: Two hidden layers}
\NormalTok{n_cols }\OperatorTok{=}\NormalTok{ predictors.shape[}\DecValTok{1}\NormalTok{]}
\NormalTok{model }\OperatorTok{=}\NormalTok{ Sequential()}
\CommentTok{## Input}
\NormalTok{model.add(Dense(}\DecValTok{50}\NormalTok{, activation}\OperatorTok{=}\StringTok{'relu'}\NormalTok{, input_shape }\OperatorTok{=}\NormalTok{ (n_cols,)))}
\CommentTok{# Hidden}
\NormalTok{model.add(Dense(}\DecValTok{32}\NormalTok{, activation}\OperatorTok{=}\StringTok{'relu'}\NormalTok{))}
\CommentTok{# Output}
\NormalTok{model.add(Dense(}\DecValTok{2}\NormalTok{, activation }\OperatorTok{=} \StringTok{'softmax'}\NormalTok{))}
\CommentTok{# Compile the model}
\NormalTok{model.}\BuiltInTok{compile}\NormalTok{(optimizer }\OperatorTok{=} \StringTok{'sgd'}\NormalTok{, loss }\OperatorTok{=} \StringTok{'categorical_crossentropy'}\NormalTok{, metrics }\OperatorTok{=}\NormalTok{ [}\StringTok{'accuracy'}\NormalTok{])}
\CommentTok{# Fit the model}
\NormalTok{model.fit(predictors, target)}
\CommentTok{#Look at summary}
\NormalTok{model.summary()}
\CommentTok{#Calculate predictions: predictions}
\NormalTok{predictions }\OperatorTok{=}\NormalTok{ model.predict(pred_data)}
\CommentTok{#Calculate predicted probability of survival}
\NormalTok{predicted_prob_true }\OperatorTok{=}\NormalTok{ predictions[:, }\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\subsection{Another Example}\label{another-example}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Import the SGD optimizer}
\ImportTok{from}\NormalTok{ keras.optimizers }\ImportTok{import}\NormalTok{ SGD}
\CommentTok{# Create list of learning rates: lr_to_test}
\NormalTok{lr_to_test }\OperatorTok{=}\NormalTok{ [.}\DecValTok{000001}\NormalTok{, }\FloatTok{0.01}\NormalTok{, }\DecValTok{1}\NormalTok{]}
\CommentTok{# Loop over learning rates}
\ControlFlowTok{for}\NormalTok{ lr }\KeywordTok{in}\NormalTok{ lr_to_test:}
    \BuiltInTok{print}\NormalTok{(}\StringTok{'Testing model with learning rate: '}\NormalTok{)}
    
    \CommentTok{# Build new model to test, unaffected by previous models}
\NormalTok{    model }\OperatorTok{=}\NormalTok{ get_new_model()}
    
    \CommentTok{# Create SGD optimizer with specified learning rate: my_optimizer}
\NormalTok{    my_optimizer }\OperatorTok{=}\NormalTok{ SGD(lr }\OperatorTok{=}\NormalTok{ lr)}
    
    \CommentTok{# Compile the model}
\NormalTok{    model.}\BuiltInTok{compile}\NormalTok{(optimizer }\OperatorTok{=}\NormalTok{ my_optimizer, loss }\OperatorTok{=} \StringTok{'categorical_crossentropy'}\NormalTok{)}
    
    \CommentTok{# Fit the model}
\NormalTok{    model.fit(predictors, }
\NormalTok{              target, }
\NormalTok{              validation_split }\OperatorTok{=} \FloatTok{0.3}\NormalTok{, }
\NormalTok{              epochs }\OperatorTok{=} \DecValTok{20}\NormalTok{, }
\NormalTok{              callbacks }\OperatorTok{=}\NormalTok{ [early_stopping_monitor], }
\NormalTok{              verbose }\OperatorTok{=} \VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{LME4}\label{lme4}

Run a glmer with poisson family error term, predicting count as a
function of fixed effects age and year, and include year as a random
effect of county as well.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{modelOut <-}\StringTok{ }\KeywordTok{glmer}\NormalTok{(count }\OperatorTok{~}\StringTok{ }\NormalTok{age }\OperatorTok{+}\StringTok{ }\NormalTok{year }\OperatorTok{+}\StringTok{ }\NormalTok{(year}\OperatorTok{|}\NormalTok{county), }\DataTypeTok{family =} \StringTok{'poisson'}\NormalTok{,}
                  \DataTypeTok{data =}\NormalTok{ ILdata)}
\end{Highlighting}
\end{Shaded}

glmer can be an alternative to chi-squared tests

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Include the AverageAgeofMother as a correlated random-effect slope parameter}
\NormalTok{ageMotherModelRandomCorrelated <-}\StringTok{ }\KeywordTok{lmer}\NormalTok{( BirthRate }\OperatorTok{~}\StringTok{ }\NormalTok{AverageAgeofMother }\OperatorTok{+}\StringTok{ }\NormalTok{(AverageAgeofMother }\OperatorTok{|}\StringTok{ }\NormalTok{State),}
\NormalTok{                       countyBirthsData)}

\NormalTok{ assumed that observations within each group were correlated.}

\CommentTok{# Include the AverageAgeofMother as a correlated random-effect slope parameter}
\NormalTok{ageMotherModelRandomUncorrelated <-}\StringTok{ }\KeywordTok{lmer}\NormalTok{( BirthRate }\OperatorTok{~}\StringTok{ }\NormalTok{AverageAgeofMother }\OperatorTok{+}\StringTok{ }\NormalTok{(AverageAgeofMother }\OperatorTok{||}\StringTok{ }\NormalTok{State),}
\NormalTok{                       countyBirthsData)}


\CommentTok{# Extract the fixed-effect coefficients}
\KeywordTok{fixef}\NormalTok{(out)}

\CommentTok{# Extract the random-effect coefficients}
\KeywordTok{ranef}\NormalTok{(out)}

\CommentTok{# Estimate the confidence intervals }
\KeywordTok{confint}\NormalTok{(out)}
\end{Highlighting}
\end{Shaded}

\section{Infer Package}\label{infer-package}

The
\href{https://cran.r-project.org/web/packages/infer/infer.pdf}{infer}
library has the main verbs: specify() variables, hypothesize(),
generate() data, calculate() metrics, visualize() results

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{4747}\NormalTok{)}
\NormalTok{perm_slope <-}\StringTok{ }\NormalTok{twins }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{specify}\NormalTok{(Foster }\OperatorTok{~}\StringTok{ }\NormalTok{Biological) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{hypothesize}\NormalTok{(}\DataTypeTok{null =} \StringTok{"independence"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{generate}\NormalTok{(}\DataTypeTok{reps =} \DecValTok{10}\NormalTok{, }\DataTypeTok{type =} \StringTok{"permute"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{calculate}\NormalTok{(}\DataTypeTok{stat =} \StringTok{"slope"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Networks}\label{networks}

`Classical SNA' is mainly about descriptive network statistics:
proximity, similarity, centrality, brokerage, positional measures,
equivalence, etc.

Import necessary modules and draw graph.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ networkx }\ImportTok{as}\NormalTok{ nx}
\NormalTok{nx.draw(T_sub)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

Get the nodes of interest.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{noi }\OperatorTok{=}\NormalTok{ [n }\ControlFlowTok{for}\NormalTok{ n, d }\KeywordTok{in}\NormalTok{ T.nodes(data}\OperatorTok{=}\VariableTok{True}\NormalTok{) }\ControlFlowTok{if}\NormalTok{ d[}\StringTok{'occupation'}\NormalTok{] }\OperatorTok{==} \StringTok{'scientist'}\NormalTok{]}
\CommentTok{### Use a list comprehension to get the edges of interest: eoi}
\NormalTok{eoi }\OperatorTok{=}\NormalTok{ [(u, v) }\ControlFlowTok{for}\NormalTok{ u, v, d }\KeywordTok{in}\NormalTok{ T.edges(data}\OperatorTok{=}\VariableTok{True}\NormalTok{) }\ControlFlowTok{if}\NormalTok{ d[}\StringTok{'date'}\NormalTok{] }\OperatorTok{<}\NormalTok{ date(}\DecValTok{2010}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)]}
\CommentTok{### Set the weight of the edge}
\NormalTok{weight }\OperatorTok{=} \DecValTok{2}
\ControlFlowTok{for}\NormalTok{ u, v, d }\KeywordTok{in}\NormalTok{ T.edges(data}\OperatorTok{=}\VariableTok{True}\NormalTok{):}
  \CommentTok{# Check if node 293 is involved}
  \ControlFlowTok{if} \DecValTok{293} \KeywordTok{in}\NormalTok{ [u,v]:}
    
    \CommentTok{# Set the weight to 1.1}
\NormalTok{    T[u][v][}\StringTok{'weight'}\NormalTok{] }\OperatorTok{=} \FloatTok{1.1}
\end{Highlighting}
\end{Shaded}

\chapter{Code Snippets}\label{code-snippets}

\section{Python}\label{python}

\subsection{General}\label{general-3}

\begin{itemize}
\item
  \url{https://chrisalbon.com/}
\item
  pandas: isin \textbar{} crosstab
\item
  pd.group\_by aggregate functions: size, count, sum, mean, median, sd,
  var, min, max, prod, first, last
\item
  The only float that isn't equal to itself is nan.
\item
  \%matplotlib notebook: interactive plots
\item
  Memory storage: use pd.copy to create a deep copy instead of a shallow
  copy
\item
  IPy: \%who or \%whos shows defined variables \& \%reset resets them
\item
  Append dict to df: use ignore\_index = True. eg. films\_df =
  films\_df.append(film\_dict, ignore\_index=1)
\item
  Debugging: import pdb \textbar{} pdb.set\_trace(),
\item
  dir() gives a list of in scope variables
\item
  globals() gives a dictionary of global variables
\item
  locals() gives a dictionary of local variables
\item
  \url{https://bugra.github.io/work/notes/2015-01-03/i-wish-i-knew-these-things-when-i-first-learned-python/}
\item
  np.dot = np.multiply \%\textgreater{}\% np.sum
\item
  flatten dict

  \begin{itemize}
  \tightlist
  \item
    pd.io.json.json\_normalize(stuff)
    *\url{https://stackoverflow.com/questions/6027558/flatten-nested-python-dictionaries-compressing-keys/41689055\#41689055}
  \item
    \url{https://towardsdatascience.com/flattening-json-objects-in-python-f5343c794b10}
  \end{itemize}
\end{itemize}

\subsection{Vanderplas Jupyter}\label{vanderplas-jupyter}

\begin{itemize}
\item
  create helper functions and units tests as R/py for Rmd/ipynb
\item
  pytest/hypothesis for testing
\item
  use makefile to run cmd commands
\item
  Write file with date:
  df.to\_csv('\{\}\_model.csv'.format(str(datetime.datetime.now()).split(`'){[}0{]}))
\item
  Make package for workflow with data (\textbf{init}.py) and use this
  formation for function docs:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ fun(a):}
  \CommentTok{#Role}
  \CommentTok{#-----}
  \CommentTok{#Does something}
  
  \CommentTok{#Parameters}
  \CommentTok{#---------}
  \CommentTok{#Accepts something}
  
  \CommentTok{#Returns}
  \CommentTok{#-------}
  \CommentTok{#Returns something}
  
  \ControlFlowTok{return}\NormalTok{(a)}
\end{Highlighting}
\end{Shaded}

\subsection{Auto Feature Engineering}\label{auto-feature-engineering}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#https://stackoverflow.com/questions/50145953/how-to-apply-deep-feature-synthesis-to-a-single-table}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ featuretools }\ImportTok{as}\NormalTok{ ft}
\NormalTok{df }\OperatorTok{=}\NormalTok{ pd.read_csv(}\StringTok{'iris.csv'}\NormalTok{)}
\NormalTok{df }\OperatorTok{=}\NormalTok{ df.reset_index()}
\NormalTok{es }\OperatorTok{=}\NormalTok{ ft.EntitySet(}\BuiltInTok{id} \OperatorTok{=} \StringTok{"test"}\NormalTok{) }\CommentTok{#.drop(columns = ['species'], axis = 1)}
\NormalTok{es }\OperatorTok{=}\NormalTok{ es.entity_from_dataframe(entity_id }\OperatorTok{=} \StringTok{'d'}\NormalTok{, dataframe }\OperatorTok{=}\NormalTok{ df, make_index}\OperatorTok{=}\VariableTok{True}\NormalTok{, index}\OperatorTok{=}\StringTok{'ind'}\NormalTok{)}
\NormalTok{fm, features }\OperatorTok{=}\NormalTok{ ft.dfs(}
\NormalTok{    entityset }\OperatorTok{=}\NormalTok{ es, }
\NormalTok{    target_entity }\OperatorTok{=} \StringTok{'d'}\NormalTok{,}
\NormalTok{    agg_primitives }\OperatorTok{=}\NormalTok{ [}\StringTok{'mean'}\NormalTok{, }\StringTok{'max'}\NormalTok{, }\StringTok{'percent_true'}\NormalTok{, }\StringTok{'last'}\NormalTok{],}
\NormalTok{    trans_primitives }\OperatorTok{=}\NormalTok{ [}\StringTok{'subtract'}\NormalTok{, }\StringTok{'divide'}\NormalTok{]}
\NormalTok{)}
\CommentTok{# produces 626 features}
\NormalTok{_, features2 }\OperatorTok{=}\NormalTok{ ft.dfs(}
\NormalTok{    entityset }\OperatorTok{=}\NormalTok{ es, }
\NormalTok{    target_entity }\OperatorTok{=} \StringTok{'d'}\NormalTok{,}
\NormalTok{    max_depth }\OperatorTok{=} \DecValTok{2}
\NormalTok{)}
\CommentTok{# produces no new features}
\end{Highlighting}
\end{Shaded}

\subsection{SHAP Explainer}\label{shap-explainer}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{clf.fit(X_train, y_train)}
\NormalTok{explainer }\OperatorTok{=}\NormalTok{ shap.TreeExplainer(clf)}
\NormalTok{shap_values }\OperatorTok{=}\NormalTok{ explainer.shap_values(X_train)}
\NormalTok{shap.force_plot(explainer.expected_value, shap_values[}\DecValTok{0}\NormalTok{,:], X.iloc[}\DecValTok{0}\NormalTok{,:]) }\CommentTok{# one record explained}
\NormalTok{shap.summary_plot(shap_values, X_train, plot_type }\OperatorTok{=} \StringTok{"bar"}\NormalTok{) }\CommentTok{# all records explained}
\NormalTok{shap.summary_plot(shap_values, X_train, plot_type }\OperatorTok{=} \StringTok{"bar"}\NormalTok{) }\CommentTok{# shap mae for features across all records}
\end{Highlighting}
\end{Shaded}

\subsection{DF To HTML Table}\label{df-to-html-table}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ flask }\ImportTok{import}\NormalTok{ Flask}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\NormalTok{app }\OperatorTok{=}\NormalTok{ Flask(}\VariableTok{__name__}\NormalTok{)}
\NormalTok{pd.set_option(}\StringTok{'display.max_colwidth'}\NormalTok{, }\OperatorTok{-}\DecValTok{1}\NormalTok{)}
\AttributeTok{@app.route}\NormalTok{(}\StringTok{"/"}\NormalTok{)}
\KeywordTok{def}\NormalTok{ show_tables():}
\NormalTok{    data }\OperatorTok{=}\NormalTok{ pd.read_csv(}\StringTok{'squads.csv'}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{ data.to_html(escape }\OperatorTok{=} \VariableTok{False}\NormalTok{, index_names }\OperatorTok{=} \VariableTok{False}\NormalTok{)}
\ControlFlowTok{if} \VariableTok{__name__} \OperatorTok{==} \StringTok{"__main__"}\NormalTok{:}
\NormalTok{    app.run(debug}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Masking Array}\label{masking-array}

You can mask your array using the numpy.ma.array function and
subsequently apply any numpy operation:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{a }\OperatorTok{=}\NormalTok{ np.random.rand(}\DecValTok{10}\NormalTok{)            }\CommentTok{# Generate random data.}
\NormalTok{a }\OperatorTok{=}\NormalTok{ np.where(a }\OperatorTok{>} \FloatTok{0.8}\NormalTok{, np.nan, a)  }\CommentTok{# Set all data larger than 0.8 to NaN}
\NormalTok{a }\OperatorTok{=}\NormalTok{ np.ma.array(a, mask}\OperatorTok{=}\NormalTok{np.isnan(a)) }\CommentTok{# Use a mask to mark the NaNs}
\NormalTok{a_norm  }\OperatorTok{=}\NormalTok{ a }\OperatorTok{/}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(a) }\CommentTok{# The sum function ignores the masked values.}
\NormalTok{a_norm2 }\OperatorTok{=}\NormalTok{ a }\OperatorTok{/}\NormalTok{ np.std(a) }\CommentTok{# The std function ignores the masked values.}
\NormalTok{a.data}
\end{Highlighting}
\end{Shaded}

\subsection{Showing Data With Plotly}\label{showing-data-with-plotly}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plotly display data}
\ImportTok{from}\NormalTok{ plotly.tools }\ImportTok{import}\NormalTok{ FigureFactory }\ImportTok{as}\NormalTok{ ff}
\ImportTok{import}\NormalTok{ plotly}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\NormalTok{plotly.offline.init_notebook_mode()}
\NormalTok{df }\OperatorTok{=}\NormalTok{ pd.read_csv(}\StringTok{'https://raw.githubusercontent.com/plotly/datasets/master/2014_usa_states.csv'}\NormalTok{)}
\NormalTok{table }\OperatorTok{=}\NormalTok{ ff.create_table(df.iloc[}\DecValTok{1}\NormalTok{:}\DecValTok{10}\NormalTok{, }\DecValTok{1}\NormalTok{:}\DecValTok{5}\NormalTok{])}
\NormalTok{plotly.offline.plot(table, output_type }\OperatorTok{=} \StringTok{'div'}\NormalTok{, show_link }\OperatorTok{=} \VariableTok{False}\NormalTok{, include_plotlyjs }\OperatorTok{=} \VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Hyperband Cross
Validation}\label{hyperband-cross-validation}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ civismlext.hyperband }\ImportTok{import}\NormalTok{ HyperbandSearchCV}
\CommentTok{#https://github.com/civisanalytics/civisml-extensions/blob/master/civismlext/hyperband.py}
\end{Highlighting}
\end{Shaded}

\subsection{Pandas Display Options}\label{pandas-display-options}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Display floats with 2 decimal places}
\NormalTok{pd.options.display.float_format }\OperatorTok{=} \StringTok{'\{:,.2f\}'}\NormalTok{.}\BuiltInTok{format}
 
\CommentTok{# Expand display limits}
\NormalTok{pd.options.display.max_rows }\OperatorTok{=} \DecValTok{200}
\NormalTok{pd.options.display.max_columns }\OperatorTok{=} \DecValTok{100}
\end{Highlighting}
\end{Shaded}

\subsection{Infix Operators}\label{infix-operators}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Define new operator}
\KeywordTok{class}\NormalTok{ Infix:}
    \KeywordTok{def} \FunctionTok{__init__}\NormalTok{(}\VariableTok{self}\NormalTok{, function):}
        \VariableTok{self}\NormalTok{.function }\OperatorTok{=}\NormalTok{ function}
    \KeywordTok{def} \FunctionTok{__ror__}\NormalTok{(}\VariableTok{self}\NormalTok{, other):}
        \ControlFlowTok{return}\NormalTok{ Infix(}\KeywordTok{lambda}\NormalTok{ x, }\VariableTok{self} \OperatorTok{=} \VariableTok{self}\NormalTok{, other }\OperatorTok{=}\NormalTok{ other: }\VariableTok{self}\NormalTok{.function(other, x))}
    \KeywordTok{def} \FunctionTok{__or__}\NormalTok{(}\VariableTok{self}\NormalTok{, other):}
        \ControlFlowTok{return} \VariableTok{self}\NormalTok{.function(other)}
    \KeywordTok{def} \FunctionTok{__rlshift__}\NormalTok{(}\VariableTok{self}\NormalTok{, other):}
        \ControlFlowTok{return}\NormalTok{ Infix(}\KeywordTok{lambda}\NormalTok{ x, }\VariableTok{self}\OperatorTok{=}\VariableTok{self}\NormalTok{, other}\OperatorTok{=}\NormalTok{other: }\VariableTok{self}\NormalTok{.function(other, x))}
    \KeywordTok{def} \FunctionTok{__rshift__}\NormalTok{(}\VariableTok{self}\NormalTok{, other):}
        \ControlFlowTok{return} \VariableTok{self}\NormalTok{.function(other)}
    \KeywordTok{def} \FunctionTok{__call__}\NormalTok{(}\VariableTok{self}\NormalTok{, value1, value2):}
        \ControlFlowTok{return} \VariableTok{self}\NormalTok{.function(value1, value2)}
\NormalTok{pipe }\OperatorTok{=}\NormalTok{ Infix(}\KeywordTok{lambda}\NormalTok{ x,y: y(x))}
\DecValTok{4} \OperatorTok{|}\NormalTok{pipe}\OperatorTok{|}\NormalTok{ (}\KeywordTok{lambda}\NormalTok{ z: z}\OperatorTok{**}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Generators}\label{generators}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Generators Error}
\NormalTok{iterator }\OperatorTok{=} \BuiltInTok{iter}\NormalTok{(iterable)}
\ControlFlowTok{try}\NormalTok{:}
    \ControlFlowTok{while} \VariableTok{True}\NormalTok{:}
\NormalTok{        item }\OperatorTok{=} \BuiltInTok{next}\NormalTok{(iterator)}
\NormalTok{        do_stuff(item)}
\ControlFlowTok{except} \PreprocessorTok{StopIteration}\NormalTok{:}
    \ControlFlowTok{pass}
\ControlFlowTok{finally}\NormalTok{:}
    \KeywordTok{del}\NormalTok{ iterator}
\end{Highlighting}
\end{Shaded}

\subsection{Rowwise Operations In
Pandas}\label{rowwise-operations-in-pandas}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Apply a function to every row in a pandas dataframe}
\NormalTok{rectangles }\OperatorTok{=}\NormalTok{ [}
\NormalTok{    \{ }\StringTok{'height'}\NormalTok{: }\DecValTok{40}\NormalTok{, }\StringTok{'width'}\NormalTok{: }\DecValTok{10}\NormalTok{ \},}
\NormalTok{    \{ }\StringTok{'height'}\NormalTok{: }\DecValTok{20}\NormalTok{, }\StringTok{'width'}\NormalTok{: }\DecValTok{9}\NormalTok{ \},}
\NormalTok{    \{ }\StringTok{'height'}\NormalTok{: }\FloatTok{3.4}\NormalTok{, }\StringTok{'width'}\NormalTok{: }\DecValTok{4}\NormalTok{ \}}
\NormalTok{]}
\NormalTok{rectangles_df }\OperatorTok{=}\NormalTok{ pd.DataFrame(rectangles)}
\KeywordTok{def}\NormalTok{ calculate_area(row):}
    \ControlFlowTok{return}\NormalTok{ row[}\StringTok{'height'}\NormalTok{] }\OperatorTok{*}\NormalTok{ row[}\StringTok{'width'}\NormalTok{]}
\NormalTok{rectangles_df.}\BuiltInTok{apply}\NormalTok{(calculate_area, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\CommentTok{# Apply to single column}
\NormalTok{df[}\StringTok{'SAL-RATE'}\NormalTok{].}\BuiltInTok{apply}\NormalTok{(money_to_float)}
\end{Highlighting}
\end{Shaded}

\subsection{Additional Functions Args In
Map}\label{additional-functions-args-in-map}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Extra arguments to function}
\ImportTok{import}\NormalTok{ functools}
\KeywordTok{def}\NormalTok{ add(x, y):}
    \ControlFlowTok{return}\NormalTok{ x }\OperatorTok{+}\NormalTok{ y}
    
\NormalTok{a }\OperatorTok{=}\NormalTok{ [}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{]}
\BuiltInTok{map}\NormalTok{(functools.partial(add, y}\OperatorTok{=}\DecValTok{2}\NormalTok{), a)}
\end{Highlighting}
\end{Shaded}

\subsection{Check CSV Encoding}\label{check-csv-encoding}

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(}\StringTok{"../input/kickstarter-projects/ks-projects-201801.csv"}\NormalTok{, }\StringTok{'rb'}\NormalTok{) }\ImportTok{as}\NormalTok{ rawdata:}
\NormalTok{    result }\OperatorTok{=}\NormalTok{ chardet.detect(rawdata.read(}\DecValTok{10000}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\subsection{MNIST Example}\label{mnist-example}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{from}\NormalTok{ sklearn }\ImportTok{import}\NormalTok{ datasets}
\ImportTok{from}\NormalTok{ sklearn }\ImportTok{import}\NormalTok{ svm}
\NormalTok{digits }\OperatorTok{=}\NormalTok{ datasets.load_digits()}
\CommentTok{#print(digits.data)}
\CommentTok{#print(digits.target)}
\BuiltInTok{print}\NormalTok{(digits.images[}\DecValTok{0}\NormalTok{])}
\NormalTok{clf }\OperatorTok{=}\NormalTok{ svm.SVC(gamma}\OperatorTok{=}\NormalTok{.}\DecValTok{001}\NormalTok{,C}\OperatorTok{=}\DecValTok{100}\NormalTok{)}
\NormalTok{x, y }\OperatorTok{=}\NormalTok{ digits.data[:}\OperatorTok{-}\DecValTok{10}\NormalTok{], digits.target[:}\OperatorTok{-}\DecValTok{10}\NormalTok{]}
\NormalTok{clf.fit(x,y)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Prediction: "}\NormalTok{, clf.predict(digits.data[}\OperatorTok{-}\DecValTok{2}\NormalTok{].reshape(}\DecValTok{1}\NormalTok{, }\OperatorTok{-}\DecValTok{1}\NormalTok{)))}
\NormalTok{plt.imshow(digits.images[}\OperatorTok{-}\DecValTok{2}\NormalTok{], cmap}\OperatorTok{=}\NormalTok{plt.cm.gray_r, interpolation}\OperatorTok{=}\StringTok{"nearest"}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\section{R}\label{r}

\subsection{General}\label{general-4}

\begin{itemize}
\item
  ctrl + shift + a: reformat code in rstudio
\item
  library(rethinking) \textbar{} data(UCBadmit) \textbar{} d
  \textless{}- UCBadmit
\item
  purrr::p/map\_dfr/c
\item
  e \textless{}- matrix(NA\_real\_, nrow = 1000, ncol = 8)
\item
  rowwise in R: purrr::pmap
\item
  YearMonth = firstScheduledDay \%\textgreater{}\%
  strftime(format=``\%Y-\%m'')
\item
  str\_split + unnest()
\item
  \url{https://stat.ethz.ch/R-manual/R-devel/library/base/html/Last.value.html}
\item
  na\_if: Replace certain values by na
\item
  purrr::map\_df - map over columns
\item
  studio multiple cursors, snippets, addins
\item
  ctrl + shift + o: toggle table of contents
\item
  ctrl shift enter: run chunk
\item
  ctrl pageup/down: navigate chunks
\item
  atl shift k: shortcuts
\item
  ctrl + shift + R: label sections
\item
  ctrl + alt + R: run all chunks
\item
  df\_print: kable in yaml rmrkdown header
\item
  rm(list = ls()): Remove all objects in the current workspace
\item
  cut(): Transform a numeric variable into a categorical variable.
\item
  car: recode (good for dummying)
\item
  Examine the objects in the workspace: ls.str()
\item
  This is how you dynamically update a value in Rmarkdown: x =
  \emph{backtick} r x \emph{backtick}.
\item
  df{[}setdiff(names(df), ``z''){]}: remove column z
\item
  purrr::walk
\item
  purr::keep/discard for is.factor
\item
  choroplethr \textbar{} choroplethrMaps \textbar{}\textbar{} d3r
  \textbar{} d3treeR \textbar{} data.tree
\end{itemize}

\subsection{Knitting A GHub Doc}\label{knitting-a-ghub-doc}

output: github\_document: default html\_notebook: default

\section{Various}\label{various}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tapply}\NormalTok{(Summary_Variable, Group_Variable, Function)}

\KeywordTok{boxplot}\NormalTok{(frequency }\OperatorTok{~}\StringTok{ }\NormalTok{attitude}\OperatorTok{*}\NormalTok{gender,}
\DataTypeTok{col=}\KeywordTok{c}\NormalTok{(}\StringTok{"white"}\NormalTok{,}\StringTok{"lightgray"}\NormalTok{), politeness)}

\CommentTok{#corrplot package in r}
\KeywordTok{cor}\NormalTok{(Carseats[}\KeywordTok{sapply}\NormalTok{(Carseats, }\ControlFlowTok{function}\NormalTok{(x) }\OperatorTok{!}\KeywordTok{is.factor}\NormalTok{(x))])}

\KeywordTok{install.packages}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"caret"}\NormalTok{, }\StringTok{"pROC"}\NormalTok{), }\DataTypeTok{dependencies =} \KeywordTok{c}\NormalTok{(}\StringTok{"Depends"}\NormalTok{, }\StringTok{"Imports"}\NormalTok{, }\StringTok{"Suggests"}\NormalTok{))}

\KeywordTok{library}\NormalTok{(knitr)}
\KeywordTok{kable}\NormalTok{(pause_sum, }\DataTypeTok{caption =} \StringTok{"Summary Statistics Table for Exit Slip Dataset"}\NormalTok{,}
      \DataTypeTok{col.names =} \KeywordTok{c}\NormalTok{(}\StringTok{"Mean"}\NormalTok{, }\StringTok{"Median"}\NormalTok{, }\StringTok{"SD"}\NormalTok{, }\StringTok{"Min"}\NormalTok{, }\StringTok{"Max"}\NormalTok{, }\StringTok{"N"}\NormalTok{, }\StringTok{"NAs"}\NormalTok{))}

\KeywordTok{typeof}\NormalTok{(}\DecValTok{1}\NormalTok{) }\CommentTok{#"double"}
\KeywordTok{typeof}\NormalTok{(1L) }\CommentTok{#"integer"}

\CommentTok{#Prints out the first 4 multiples of 3.}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{4}\OperatorTok{*}\DecValTok{3}\NormalTok{) \{}\KeywordTok{cat}\NormalTok{(i)\}}
\end{Highlighting}
\end{Shaded}

\subsection{Create Package}\label{create-package}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#https://hilaryparker.com/2014/04/29/writing-an-r-package-from-scratch/}

\KeywordTok{library}\NormalTok{(devtools)}
\KeywordTok{library}\NormalTok{(roxygen2)}

\CommentTok{# Navigate to parent folder}
\KeywordTok{setwd}\NormalTok{(}\StringTok{'../Desktop/me'}\NormalTok{)}

\CommentTok{# Name and create package}
\KeywordTok{create}\NormalTok{(}\StringTok{"sansor"}\NormalTok{)}

\CommentTok{# Add file with function}
\KeywordTok{setwd}\NormalTok{(}\StringTok{'sansor/R'}\NormalTok{)}
\KeywordTok{file.create}\NormalTok{(}\StringTok{'test.R'}\NormalTok{) }\CommentTok{# test <- function()\{print('This is a test')\}}

\CommentTok{# Back to package dir}
\KeywordTok{setwd}\NormalTok{(}\StringTok{".."}\NormalTok{)}

\CommentTok{# Generate auto docs}
\KeywordTok{document}\NormalTok{()}

\CommentTok{# Install package for use}
\KeywordTok{setwd}\NormalTok{(}\StringTok{'..'}\NormalTok{)}
\KeywordTok{install}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\subsection{Create Operator}\label{create-operator}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(stringr)}
\StringTok{`}\DataTypeTok{%[]%}\StringTok{`}\NormalTok{ <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(string, subset)\{}
  \KeywordTok{return}\NormalTok{(}\KeywordTok{str_sub}\NormalTok{(string, subset[}\DecValTok{1}\NormalTok{], subset[}\DecValTok{2}\NormalTok{]))}
\NormalTok{\}}
\StringTok{'test'}\OperatorTok{%[]%}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Useful Date Manipulation}\label{useful-date-manipulation}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{year =}\StringTok{ }\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{format}\NormalTok{(}\KeywordTok{as.Date}\NormalTok{(TestEndTime, }\DataTypeTok{format=}\StringTok{"%Y-%m-%d"}\NormalTok{),}\StringTok{"%Y"}\NormalTok{))}
\NormalTok{date <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\KeywordTok{as.Date}\NormalTok{(}\StringTok{"2017/09/01"}\NormalTok{), }\KeywordTok{as.Date}\NormalTok{(}\StringTok{"2017/09/25"}\NormalTok{), }\DataTypeTok{by=}\StringTok{"day"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{R for Everything Talk}\label{r-for-everything-talk}

\begin{itemize}
\item
  dir.exists \textbar{} dir.create \textbar{} download.file
\item
  untar \textbar{} unlink \textbar{} file.info
\item
  dir \textbar{} file.rename \textbar{} file.copy
\item
  count.fields \textbar{} system
\item
  dplyr five main actions: select, filter, arrange, mutate, summarize.
\end{itemize}

\subsection{Multi Character To Numeric
Casting}\label{multi-character-to-numeric-casting}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{char_to_num_cols <-}\StringTok{ }\NormalTok{df }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\KeywordTok{starts_with}\NormalTok{(}\StringTok{'MMR'}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }\NormalTok{names}
\NormalTok{df[, char_to_num_cols] <-}\StringTok{ }\KeywordTok{sapply}\NormalTok{(df[, char_to_num_cols], as.numeric)}
\end{Highlighting}
\end{Shaded}

\subsection{Grid Plot}\label{grid-plot}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gridExtra}\OperatorTok{::}\KeywordTok{grid.arrange}\NormalTok{(}
\NormalTok{  plots[[}\DecValTok{1}\NormalTok{]],plots[[}\DecValTok{2}\NormalTok{]],plots[[}\DecValTok{3}\NormalTok{]],}
\NormalTok{  plots[[}\DecValTok{4}\NormalTok{]],plots[[}\DecValTok{5}\NormalTok{]],plots[[}\DecValTok{6}\NormalTok{]], }\DataTypeTok{nrow=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Classification Target
Comparison}\label{classification-target-comparison}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{chisq.test}\NormalTok{(}\KeywordTok{table}\NormalTok{(}\KeywordTok{cut}\NormalTok{(iris}\OperatorTok{$}\NormalTok{Sepal.Length, }\DecValTok{3}\NormalTok{), iris}\OperatorTok{$}\NormalTok{Species))}
\end{Highlighting}
\end{Shaded}

\subsection{Pairwise Scatterplot}\label{pairwise-scatterplot}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{GGally}\OperatorTok{::}\KeywordTok{ggpairs}\NormalTok{(data, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{colour =}\NormalTok{ category))}
\end{Highlighting}
\end{Shaded}

\subsection{Deploy Shiny App}\label{deploy-shiny-app}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(shinyapps)}
\KeywordTok{library}\NormalTok{(shiny)}

\NormalTok{working_dir <-}\StringTok{ ''}
\KeywordTok{setwd}\NormalTok{(working_dir)}

\NormalTok{shinyapps}\OperatorTok{::}\KeywordTok{setAccountInfo}\NormalTok{(}\DataTypeTok{name=}\StringTok{"<ACCOUNT>"}\NormalTok{, }
                          \DataTypeTok{token=}\StringTok{"<TOKEN>"}\NormalTok{, }
                          \DataTypeTok{secret=}\StringTok{"<SECRET>"}\NormalTok{)}

\KeywordTok{runApp}\NormalTok{()}
\KeywordTok{deployApp}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\subsection{Shut Up, R}\label{shut-up-r}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#chunk options: echo = F, warning=F, error=F, message=F, results='hide'}
\end{Highlighting}
\end{Shaded}

\subsection{Plotting}\label{plotting}

Stuff I did while testing Bundesliga xG data.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(openxlsx)}
\KeywordTok{library}\NormalTok{(tidyr)}
\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{library}\NormalTok{(dplyr)}
\KeywordTok{library}\NormalTok{(ggvis)}
\KeywordTok{library}\NormalTok{(rCharts)}

\NormalTok{bund_xg <-}\StringTok{ }\KeywordTok{read.xlsx}\NormalTok{(}\StringTok{"data/bundesliga_xg_2015_2016.xlsx"}\NormalTok{)}

\NormalTok{bund_xg.long <-}\StringTok{ }\KeywordTok{gather}\NormalTok{(bund_xg, Status, Team, Home}\OperatorTok{:}\NormalTok{Away)}
\NormalTok{bund_xg.long}\OperatorTok{$}\NormalTok{xG <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(bund_xg.long}\OperatorTok{$}\NormalTok{Status }\OperatorTok{==}\StringTok{ 'Home'}\NormalTok{, }
\NormalTok{                          bund_xg.long}\OperatorTok{$}\NormalTok{xG_H, }
\NormalTok{                          bund_xg.long}\OperatorTok{$}\NormalTok{xG_A)}
\NormalTok{bund_xg.long.team <-}\StringTok{ }\KeywordTok{group_by}\NormalTok{(bund_xg.long, Team)}

\NormalTok{xg.plot <-}\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ bund_xg.long, }
       \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ GW, }\DataTypeTok{y =}\NormalTok{ xG, }\DataTypeTok{group =}\NormalTok{ Status, }\DataTypeTok{color =}\NormalTok{ Status)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_line}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title =} \StringTok{'Bundesliga 15/16'}\NormalTok{, }\DataTypeTok{x =} \StringTok{'Game Week'}\NormalTok{, }\DataTypeTok{y =}\StringTok{'Expected Goals For'}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{(}\OperatorTok{~}\NormalTok{Team)}

\KeywordTok{ggsave}\NormalTok{(}\DataTypeTok{plot =}\NormalTok{ xg.plot, }\DataTypeTok{filename =} \StringTok{'bund_xg_plot.png'}\NormalTok{)}

\NormalTok{bund_xg.long[bund_xg.long}\OperatorTok{$}\NormalTok{Team }\OperatorTok{==}\StringTok{ 'Bayern'}\NormalTok{,] }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggvis}\NormalTok{(}\OperatorTok{~}\NormalTok{GW, }\OperatorTok{~}\NormalTok{xG) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{layer_points}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\KeywordTok{layer_lines}\NormalTok{()}
  \CommentTok{#add_tooltip(function(df) df$GW)}

\NormalTok{p =}\StringTok{ }\KeywordTok{rPlot}\NormalTok{(xG }\OperatorTok{~}\StringTok{ }\NormalTok{GW, }
      \DataTypeTok{data =}\NormalTok{ bund_xg.long[bund_xg.long}\OperatorTok{$}\NormalTok{Team }\OperatorTok{==}\StringTok{ 'Bayern'}\NormalTok{,], }
      \DataTypeTok{color =} \StringTok{'Status'}\NormalTok{, }
      \DataTypeTok{type =} \StringTok{'line'}\NormalTok{)}

\NormalTok{p}\OperatorTok{$}\KeywordTok{set}\NormalTok{(}\DataTypeTok{title =} \StringTok{'Bayern Munich (Bundesliga 15/16)'}\NormalTok{)}
\NormalTok{p}\OperatorTok{$}\KeywordTok{guides}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{title =} \StringTok{"Game Week"}\NormalTok{))}
\NormalTok{p}\OperatorTok{$}\KeywordTok{guides}\NormalTok{(}\DataTypeTok{y =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{title =} \StringTok{"Expected Goals"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\subsection{Nested Tree}\label{nested-tree}

Stuff I did while testing tree viz for ODSC DS tools.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(googleVis)}
\NormalTok{test.data <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{'C:/Users/Gordon/Downloads/test.csv'}\NormalTok{, }\DataTypeTok{header =}\NormalTok{ T)}

\NormalTok{Org <-}\StringTok{ }\KeywordTok{gvisOrgChart}\NormalTok{(test.data, }
                    \DataTypeTok{options=}\KeywordTok{list}\NormalTok{(}\DataTypeTok{width=}\DecValTok{600}\NormalTok{, }\DataTypeTok{height=}\DecValTok{250}\NormalTok{,}
                                 \DataTypeTok{size=}\StringTok{'large'}\NormalTok{, }\DataTypeTok{allowCollapse=}\OtherTok{TRUE}\NormalTok{))}
\NormalTok{Org}\OperatorTok{$}\NormalTok{html}\OperatorTok{$}\NormalTok{footer  <-}\StringTok{ }\OtherTok{NULL}
\NormalTok{Org}\OperatorTok{$}\NormalTok{html}\OperatorTok{$}\NormalTok{caption  <-}\StringTok{ }\OtherTok{NULL}
\KeywordTok{plot}\NormalTok{(Org)}

\NormalTok{test.data}\OperatorTok{$}\NormalTok{Val  <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\KeywordTok{nrow}\NormalTok{(test.data), }\DecValTok{1}\NormalTok{)}
\NormalTok{test.data}\OperatorTok{$}\NormalTok{Fac  <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\KeywordTok{nrow}\NormalTok{(test.data), }\DecValTok{1}\NormalTok{)}

\NormalTok{Tree <-}\StringTok{ }\KeywordTok{gvisTreeMap}\NormalTok{(test.data,  }
                    \StringTok{"Node"}\NormalTok{, }\StringTok{"Parent"}\NormalTok{, }
                    \CommentTok{#"Val", "Fac", }
                    \DataTypeTok{options=}\KeywordTok{list}\NormalTok{(}\DataTypeTok{fontSize=}\DecValTok{16}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(Tree)}
\end{Highlighting}
\end{Shaded}

\subsection{Setup Project}\label{setup-project}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(knitr)}

\CommentTok{#Create top level folder}
\KeywordTok{dir.create}\NormalTok{(}\StringTok{'my-data-project'}\NormalTok{)}
\KeywordTok{setwd}\NormalTok{(}\StringTok{'./my-data-project/'}\NormalTok{)}

\CommentTok{#Create child folders}
\KeywordTok{dir.create}\NormalTok{(}\StringTok{'code'}\NormalTok{)}
\KeywordTok{dir.create}\NormalTok{(}\StringTok{'visualizations'}\NormalTok{)}
\KeywordTok{dir.create}\NormalTok{(}\StringTok{'reports'}\NormalTok{)}
\KeywordTok{dir.create}\NormalTok{(}\StringTok{'data'}\NormalTok{)}
\KeywordTok{dir.create}\NormalTok{(}\StringTok{'deploy_maintain'}\NormalTok{)}

\CommentTok{#Create trackers}
\KeywordTok{file.create}\NormalTok{(}\StringTok{'README.md'}\NormalTok{)}
\KeywordTok{file.create}\NormalTok{(}\StringTok{'CHANGELOG.md'}\NormalTok{)}

\CommentTok{#Create grandchildren folders for data}
\KeywordTok{setwd}\NormalTok{(}\StringTok{'./data'}\NormalTok{)}
\KeywordTok{dir.create}\NormalTok{(}\StringTok{'raw'}\NormalTok{)}
\KeywordTok{dir.create}\NormalTok{(}\StringTok{'refined'}\NormalTok{)}

\CommentTok{#Create grandchildren folders for visualizations}
\KeywordTok{setwd}\NormalTok{(}\StringTok{'../visualizations'}\NormalTok{)}
\KeywordTok{dir.create}\NormalTok{(}\StringTok{'exploratory'}\NormalTok{)}
\KeywordTok{dir.create}\NormalTok{(}\StringTok{'communication'}\NormalTok{)}

\CommentTok{#Initialize git repo}
\NormalTok{git2r}\OperatorTok{::}\KeywordTok{init}\NormalTok{(}\StringTok{'.'}\NormalTok{)}

\CommentTok{# CHANGELOG.md Example}

\NormalTok{## Logs}

\NormalTok{## Excluded processes should be assumed to be the same as version 0.1.}

\NormalTok{## Current Best Model:}
\NormalTok{## Current Best Score / Metric:}

\NormalTok{## 0.1}

\NormalTok{## 1_import_tidy.Rmd}

\NormalTok{## 2_transform_visualize.Rmd}

\NormalTok{## 3_modeling.ipynb}

\CommentTok{# Layout}

\NormalTok{## libs, functions, imports, preprocessing, feature engineering, feature selection, model building}
\end{Highlighting}
\end{Shaded}

\subsection{MAP Model}\label{map-model}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}

\NormalTok{model <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(df) \{}
  \KeywordTok{lm}\NormalTok{(mpg }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ df)}
\NormalTok{\}}

\NormalTok{df <-}\StringTok{ }\NormalTok{mtcars}
\NormalTok{df }\OperatorTok{%>%}\StringTok{ }\KeywordTok{group_by}\NormalTok{(cyl) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{split}\NormalTok{(.}\OperatorTok{$}\NormalTok{cyl) }\OperatorTok{%>%}\StringTok{ }\NormalTok{purrr}\OperatorTok{::}\KeywordTok{map}\NormalTok{(}\OperatorTok{~}\StringTok{ }\KeywordTok{lm}\NormalTok{(mpg }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ .))}
\NormalTok{df }\OperatorTok{%>%}\StringTok{ }\KeywordTok{group_by}\NormalTok{(cyl) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{nest}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{model =} \KeywordTok{map}\NormalTok{(data, model))}

\NormalTok{chickweight }\OperatorTok{%>%}\StringTok{ }\KeywordTok{group_by}\NormalTok{(Diet) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{nest}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{mod =}\NormalTok{ data }\OperatorTok{%>%}\StringTok{ }\KeywordTok{map}\NormalTok{(}\OperatorTok{~}\StringTok{ }\KeywordTok{lm}\NormalTok{(weight }\OperatorTok{~}\StringTok{ }\NormalTok{Time, }\DataTypeTok{data=}\NormalTok{.)))}
\end{Highlighting}
\end{Shaded}

\subsection{DT Option}\label{dt-option}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{datatable}\NormalTok{(}
\NormalTok{  df_version_check,}
  \DataTypeTok{options =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{filter =} \StringTok{'top'}\NormalTok{,}
                 \DataTypeTok{searching =}\NormalTok{ T,}
                 \DataTypeTok{pageLength =} \DecValTok{10}\NormalTok{, }\DataTypeTok{lengthChange =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{ordering =} \OtherTok{FALSE}\NormalTok{,}
                 \DataTypeTok{scrollCollapse =}\NormalTok{ T,}
                 \DataTypeTok{scrollY =} \StringTok{'1500'}\NormalTok{,}
                 \DataTypeTok{paging =}\NormalTok{ T}
\NormalTok{                 )}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\subsection{Plotly MAP}\label{plotly-map}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplotly}\NormalTok{(}
    \KeywordTok{ggplot}\NormalTok{(df_master2 }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(skl_group}\OperatorTok{==}\NormalTok{x),}
       \KeywordTok{aes}\NormalTok{(SkillId, correct_rate, }\DataTypeTok{label =}\NormalTok{ Master, }\DataTypeTok{color =}\NormalTok{ SkillId)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_jitter}\NormalTok{(}\DataTypeTok{width =}\NormalTok{ .}\DecValTok{2}\NormalTok{),}
    \DataTypeTok{tooltip =} \KeywordTok{c}\NormalTok{(}\StringTok{'y'}\NormalTok{, }\StringTok{'label'}\NormalTok{)}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\subsection{Pipe Anonymous Function}\label{pipe-anonymous-function}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OperatorTok{%>%}
\StringTok{  }\NormalTok{(}\ControlFlowTok{function}\NormalTok{(input.object) \{}
\NormalTok{    complex}
\NormalTok{    logic}
\NormalTok{    goes}
\NormalTok{    here}
\NormalTok{    output.object}
\NormalTok{  \})}
\end{Highlighting}
\end{Shaded}

\subsection{Binarize Predictions}\label{binarize-predictions}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{clf_lr_final <-}\StringTok{ }\NormalTok{clf_lr}\OperatorTok{$}\NormalTok{finalModel}
\NormalTok{model_tidy <-}\StringTok{ }\NormalTok{broom}\OperatorTok{::}\KeywordTok{tidy}\NormalTok{(clf_lr_final)}
\NormalTok{df2}\OperatorTok{$}\NormalTok{pass_predicted <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(}
  \KeywordTok{ifelse}\NormalTok{(clf_lr_final}\OperatorTok{$}\NormalTok{fitted.values }\OperatorTok{>=}\StringTok{ }\NormalTok{.}\DecValTok{5}\NormalTok{, }\StringTok{'pass'}\NormalTok{, }\StringTok{'fail'}\NormalTok{)}
\NormalTok{  )}
\KeywordTok{metrics}\NormalTok{(df2, pass, pass_predicted)}

\NormalTok{var_imp <-}\StringTok{ }\KeywordTok{varImp}\NormalTok{(clf_lr_final) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{cols =} \KeywordTok{row.names}\NormalTok{(.)) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{arrange}\NormalTok{(}\OperatorTok{-}\NormalTok{Overall) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(cols, Overall)}
\NormalTok{var_imp}
\end{Highlighting}
\end{Shaded}

\subsection{Make Formula}\label{make-formula}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{make_formula <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(target, additions, }\DataTypeTok{subtractions =} \OtherTok{NULL}\NormalTok{, }\DataTypeTok{powers =} \OtherTok{NULL}\NormalTok{)\{}
  
\NormalTok{  add_sub_sep =}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(}\KeywordTok{is.null}\NormalTok{(subtractions), }\StringTok{''}\NormalTok{, }\StringTok{'-'}\NormalTok{)}
\NormalTok{  add <-}\StringTok{ }\KeywordTok{paste0}\NormalTok{(additions, }\DataTypeTok{collapse =} \StringTok{' + '}\NormalTok{)}
\NormalTok{  subtract <-}\StringTok{ }\KeywordTok{paste0}\NormalTok{(subtractions, }\DataTypeTok{collapse =} \StringTok{' - '}\NormalTok{)}
\NormalTok{  add_subtract <-}\StringTok{ }\KeywordTok{paste}\NormalTok{(add, subtract, }\DataTypeTok{sep =}\NormalTok{ add_sub_sep)}
  
  \KeywordTok{return}\NormalTok{(}\KeywordTok{as.formula}\NormalTok{(}\KeywordTok{paste}\NormalTok{(target, add_subtract, }\DataTypeTok{sep =} \StringTok{'~'}\NormalTok{)))}
  
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\subsection{Shiny Reactive}\label{shiny-reactive}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df_labeled =}\StringTok{ }\KeywordTok{eventReactive}\NormalTok{(\{}\KeywordTok{c}\NormalTok{(input}\OperatorTok{$}\NormalTok{good, input}\OperatorTok{$}\NormalTok{bad)\}, \{    }
\NormalTok{    df }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(value }\OperatorTok{==}\StringTok{ }\NormalTok{input}\OperatorTok{$}\NormalTok{value)}
\NormalTok{  \})}
\end{Highlighting}
\end{Shaded}

\subsection{Custom Theme}\label{custom-theme}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{theme}\NormalTok{(}
     \DataTypeTok{axis.text.y =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{angle =} \DecValTok{65}\NormalTok{, }\DataTypeTok{vjust =} \FloatTok{0.6}\NormalTok{),}
     \DataTypeTok{axis.title.x =} \KeywordTok{element_blank}\NormalTok{(),}
     \DataTypeTok{panel.background =} \KeywordTok{element_blank}\NormalTok{()}
\NormalTok{     )}
\end{Highlighting}
\end{Shaded}

\subsection{Create Factor}\label{create-factor}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mons =}\StringTok{ }\KeywordTok{factor}\NormalTok{(}
   \KeywordTok{c}\NormalTok{(}\StringTok{"August"}\NormalTok{,}\StringTok{"September"}\NormalTok{, }\StringTok{"October"}\NormalTok{,}\StringTok{"November"}\NormalTok{,}
           \StringTok{"December"}\NormalTok{, }\StringTok{"January"}\NormalTok{,}\StringTok{"February"}\NormalTok{,}\StringTok{"March"}\NormalTok{,}
               \StringTok{"April"}\NormalTok{,}\StringTok{"May"}\NormalTok{,}\StringTok{"June"}\NormalTok{,}\StringTok{"July"}\NormalTok{),}
  \DataTypeTok{levels =} \KeywordTok{c}\NormalTok{(}\StringTok{"August"}\NormalTok{,}\StringTok{"September"}\NormalTok{, }\StringTok{"October"}\NormalTok{,}\StringTok{"November"}\NormalTok{,}
           \StringTok{"December"}\NormalTok{, }\StringTok{"January"}\NormalTok{,}\StringTok{"February"}\NormalTok{,}\StringTok{"March"}\NormalTok{,}
               \StringTok{"April"}\NormalTok{,}\StringTok{"May"}\NormalTok{,}\StringTok{"June"}\NormalTok{,}\StringTok{"July"}\NormalTok{),}
  \DataTypeTok{ordered =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Extract Date}\label{extract-date}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{LessonStartTime }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as.POSIXct}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\KeywordTok{strftime}\NormalTok{(}\DataTypeTok{format=}\StringTok{"%Y-%m-%d"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Geom\_Path}\label{geom_path}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(ilo_data) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_path}\NormalTok{(}
            \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ working_hours, }\DataTypeTok{y =}\NormalTok{ country),}
            \DataTypeTok{arrow =} \KeywordTok{arrow}\NormalTok{(}\DataTypeTok{length =} \KeywordTok{unit}\NormalTok{(}\FloatTok{1.5}\NormalTok{, }\StringTok{"mm"}\NormalTok{), }\DataTypeTok{type =} \StringTok{"closed"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Theme Template}\label{theme-template}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{theme_ilo <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{() \{}
  \KeywordTok{theme_minimal}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}
    \DataTypeTok{text =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{family =} \StringTok{"Bookman"}\NormalTok{, }\DataTypeTok{color =} \StringTok{"gray25"}\NormalTok{),}
    \DataTypeTok{plot.subtitle =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size =} \DecValTok{12}\NormalTok{),}
    \DataTypeTok{plot.caption =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{color =} \StringTok{"gray30"}\NormalTok{),}
    \DataTypeTok{plot.background =} \KeywordTok{element_rect}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"gray95"}\NormalTok{),}
    \DataTypeTok{plot.margin =} \KeywordTok{unit}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{), }\DataTypeTok{units =} \StringTok{"mm"}\NormalTok{)}
\NormalTok{  )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\subsection{Heatmap}\label{heatmap}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(corrplot)}
\NormalTok{df_fltrd }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select_if}\NormalTok{(is.numeric) }\OperatorTok{%>%}\StringTok{  }
\KeywordTok{cor}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\KeywordTok{corrplot}\NormalTok{(}\DataTypeTok{method =} \StringTok{"circle"}\NormalTok{, }\DataTypeTok{is.corr =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Calculate Accuracy}\label{calculate-accuracy}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{calc_accuracy <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(labels, preds)\{}
  
\NormalTok{  results <-}\StringTok{ }\KeywordTok{table}\NormalTok{(labels, preds) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\NormalTok{data.frame }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{correct =} \KeywordTok{ifelse}\NormalTok{(labels}\OperatorTok{==}\NormalTok{preds, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{group_by}\NormalTok{(correct) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{freq =} \KeywordTok{sum}\NormalTok{(Freq)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{ungroup}\NormalTok{() }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{prop =}\NormalTok{ freq}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(freq)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{filter}\NormalTok{(correct }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{)}
  
  \KeywordTok{return}\NormalTok{(results)}

\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\subsection{Replace NA}\label{replace-na}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\OperatorTok{%>%}\StringTok{ }\KeywordTok{replace_na}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\DataTypeTok{x =} \DecValTok{0}\NormalTok{, }\DataTypeTok{y =} \StringTok{"unknown"}\NormalTok{))}
\NormalTok{df2 }\OperatorTok{%>%}\StringTok{ }\KeywordTok{replace}\NormalTok{(., }\KeywordTok{is.na}\NormalTok{(.), }\StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Filter By Row}\label{filter-by-row}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}
    \OperatorTok{!}\KeywordTok{grepl}\NormalTok{(}\StringTok{'Schedules'}\NormalTok{, issues)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\subsection{DB Functions}\label{db-functions}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# DB Functions}

\NormalTok{data <-}\StringTok{ }\KeywordTok{dbGetQuery}\NormalTok{(con, }\StringTok{'select * from iris'}\NormalTok{)}
\KeywordTok{dbListTables}\NormalTok{(con) }\OperatorTok{%>%}\StringTok{ }\NormalTok{sort}
\KeywordTok{dbExistsTable}\NormalTok{(con, }\StringTok{"iris"}\NormalTok{)}
\KeywordTok{dbRemoveTable}\NormalTok{(con, }\StringTok{"iris"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Knitr}\label{knitr}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\OperatorTok{::}\KeywordTok{include_graphics}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\StringTok{"./assets/ml_trading/portfolio.png"}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\section{Bash}\label{bash}

script to do joining of dummied csvs

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{paste}\NormalTok{ -d=}\StringTok{','}\NormalTok{ file1.csv file2.csv file3.csv }\OperatorTok{>}\NormalTok{ file_final.csv}
\end{Highlighting}
\end{Shaded}

\chapter{Time Series}\label{time-series}

\section{Notes}\label{notes}

A stationary time series is one whose properties do not depend on the
time at which the series is observed.

\section{Packages}\label{packages}

\url{https://robjhyndman.com/seminars/user-fable/}
\url{https://github.com/robjhyndman/forecast}
\url{https://facebook.github.io/prophet/docs/quick_start.html}
\url{https://keras.rstudio.com/articles/examples/index.html}
\url{http://www.business-science.io/code-tools/2017/05/02/timekit-0-2-0.html}
\url{https://www.rdocumentation.org/packages/bsts/versions/0.8.0}

\section{Data Conversion}\label{data-conversion}

\url{https://robjhyndman.com/hyndsight/seasonal-periods/}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt =}\StringTok{ }\KeywordTok{as.Date}\NormalTok{(StockData}\OperatorTok{$}\NormalTok{Date, }\DataTypeTok{format=}\StringTok{"%m/%d/%Y"}\NormalTok{) }\OperatorTok{>}\NormalTok{Stockdataz =}\StringTok{ }\KeywordTok{zoo}\NormalTok{(}\DataTypeTok{x=}\KeywordTok{cbind}\NormalTok{(StockData}\OperatorTok{$}\NormalTok{Volume,StockData}\OperatorTok{$}\NormalTok{Adj.Close), }\DataTypeTok{order.by=}\NormalTok{dt)}

\NormalTok{ts}\OperatorTok{:}\StringTok{ }\NormalTok{frequency =}\StringTok{ }\DecValTok{5}\NormalTok{ means that data is at daily level business days}
\KeywordTok{ts}\NormalTok{(completed_example_data}\OperatorTok{$}\NormalTok{Wind,}\DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\DecValTok{1973}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{1}\NormalTok{), }\DataTypeTok{frequency =} \FloatTok{365.25}\NormalTok{)}

\KeywordTok{xts}\NormalTok{(df[,}\OperatorTok{-}\DecValTok{1}\NormalTok{], }\DataTypeTok{order.by=}\KeywordTok{as.Date}\NormalTok{(df[,}\DecValTok{1}\NormalTok{], }\StringTok{"%m/%d/%Y"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\section{Models}\label{models}

Exponential smoothing and ARIMA models are the two most widely used
approaches to time series forecasting, and provide complementary
approaches to the problem. While exponential smoothing models are based
on a description of the trend and seasonality in the data, ARIMA models
aim to describe the autocorrelations in the data.

Benchmark models: Mean, naive, seasonal naive

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{-1}
\item
  Mean
\item
  Naive: Set all forecasts to be the value of the last observation. This
  method works remarkably well for many economic and financial time
  series. Because a naive forecast is optimal when data follow a random
  walk, these are also called random walk forecasts. naive()
\item
  Seasonal Naive: Assumes magnitude of the seasonal pattern is constant.
  snaive()
\end{enumerate}

Linear

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{9}
\item
  regression: tslm(): trend and season are not objects in the R
  workspace; they are created automatically by tslm() when specified in
  this way: tslm(beer2 \textasciitilde{} trend + season). It is not
  recommended that quadratic or higher order trends be used in
  forecasting. When they are extrapolated, the resulting forecasts are
  often unrealistic.
\item
  harmonic regression: An alternative to using seasonal dummy variables,
  especially for long seasonal periods, is to use Fourier terms. ith
  Fourier terms, we often need fewer predictors than with dummy
  variables, especially when m is large. This makes them useful for
  weekly data, for example, where m ≈ 52. For short seasonal periods
  (e.g., quarterly data), there is little advantage in using Fourier
  terms over seasonal dummy variables. fourier() - tslm(beer2
  \textasciitilde{} trend + fourier(beer2, K=2))
\item
  Dynamic Regression
\end{enumerate}

The R function Arima() will fit a regression model with ARIMA errors if
the argument xreg is used. The order argument specifies the order of the
ARIMA error model. If differencing is specified, then the differencing
is applied to all variables in the regression model before the model is
estimated.

Arima(y, xreg=x, order=c(1,1,0))
auto.arima(uschange{[},``Consumption''{]},
xreg=uschange{[},``Income''{]})

There are two different ways of modelling a linear trend: deterministic
and stochastic.

deterministic: fit1 \textless{}- auto.arima(austa, d=0, xreg=trend)

stochastic: fit2 \textless{}- auto.arima(austa, d=1)

There is an implicit assumption with deterministic trends that the slope
of the trend is not going to change over time. On the other hand,
stochastic trends can change, and the estimated growth is only assumed
to be the average growth over the historical period, not necessarily the
rate of growth that will be observed into the future. Consequently, it
is safer to forecast with stochastic trends, especially for longer
forecast horizons, as the prediction intervals allow for greater
uncertainty in future growth.

When there are long seasonal periods, a dynamic regression with Fourier
terms is often better than other models we have considered in this book.

Seasonal differencing of high order does not make a lot of sense --- for
daily data it involves comparing what happened today with what happened
exactly a year ago and there is no constraint that the seasonal pattern
is smooth.

So for such time series, we prefer a harmonic regression approach where
the seasonal pattern is modelled using Fourier terms with short-term
time series dynamics handled by an ARMA error.

fit \textless{}- auto.arima(cafe04, xreg = fourier(cafe04, K = i),
seasonal = FALSE, lambda = 0)

ETS (Error-Trend-Seasonality) Models

This label can also be thought of as ExponenTial Smoothing. The
possibilities for each component are: Error = \{a.m\}, Trend =
\{n,a,a\_d\}, and Seasonal = \{n,a,m\}. Therefore, for each component in
the ETS system, we can assign None, Multiplicative, or Additive (or N,
M, A) for each of the three components in our time series.

Three of the combinations of (Error, Trend, Seasonal) can lead to
numerical difficulties. Specifically, the models that can cause such
instabilities are ETS(A,N,M), ETS(A,A,M), and ETS(A,A d ,M), due to
division by values potentially close to zero in the state equations. We
normally do not consider these particular combinations when selecting a
model.

Models with multiplicative errors are useful when the data are strictly
positive, but are not numerically stable when the data contain zeros or
negative values. Therefore, multiplicative error models will not be
considered if the time series is not strictly positive. In that case,
only the six fully additive models will be applied.

The ets() function does not produce forecasts. Rather, it estimates the
model parameters and returns information about the fitted model. By
default it uses the AICc to select an appropriate model

ETS point forecasts are equal to the medians of the forecast
distributions. For models with only additive components, the forecast
distributions are normal, so the medians and means are equal. For ETS
models with multiplicative errors, or with multiplicative seasonality,
the point forecasts will not be equal to the means of the forecast
distributions.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Simple Exponential Smoothing Method: Time series does not have a trend
  line and does not have seasonality component. We would use a Simple
  Exponential Smoothing model.\\
  ETS(A,N,N): simple exponential smoothing with additive errors
  \textbar{} ETS(M,N,N): simple exponential smoothing with
  multiplicative errors
\item
  Holt's Linear Trend Method: Damped Holt's method is best whether you
  compare MAE or MSE values. So we will proceed with using the damped
  Holt's method and apply it to the whole data set to get forecasts for
  future years. Forecasts account for trend \& continue at the same
  trend indefinitely into the future: holt(damped = T) . A damped trend
  makes it level off after time: holt(damped = F) (phi = 1 is Holt's
  method).
\end{enumerate}

holt() \textbar{} ETS(A,A,N): Holt's linear method with additive errors
\textbar{} ETS(M,A,N): Holt's linear method with multiplicative errors
\textbar{} (a,a) Additive Holt-Winters' method \textbar{} (a,m)
Multiplicative Holt-Winters' method \textbar{} (a\_d,m) Holt-Winters'
damped method \textbar{} (a,n) Holt's linear method \textbar{} (a\_d,n)
Additive damped trend method

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\tightlist
\item
  Holt-Winters Seasonal Method: hw(aust,seasonal=``additive''). Accounts
  for trend and seasonality. In the additive version the seasonality
  averages to 0 but this changes to 1 in the multiplicative version
\end{enumerate}

ARIMA(p,d,q): non-seasonal: White noise: ARIMA(0,0,0) Random walk:
ARIMA(0,1,0) with no constant Random walk with drift: ARIMA(0,1,0) with
a constant Autoregression: ARIMA(p,0,0) Moving average: ARIMA(0,0,q)

Seasonal ARIMA(p,d,q)(P,D,Q)\_m

auto.arima() does an auto fit.

When fitting an ARIMA model to a set of (non-seasonal) time series data,
the following procedure provides a useful general approach.

Plot the data and identify any unusual observations.

If necessary, transform the data (using a Box-Cox transformation) to
stabilize the variance.

If the data are non-stationary, take first differences of the data until
the data are stationary.

Examine the ACF/PACF: Is an ARIMA(p,d,0) or ARIMA(0,d,q) model
appropriate?

Try your chosen model(s), and use the AICc to search for a better model.

Check the residuals from your chosen model by plotting the ACF of the
residuals, and doing a portmanteau test of the residuals. If they do not
look like white noise, try a modified model.

Once the residuals look like white noise, calculate forecasts.

auto.arima only does steps 3-5.

arima()

ggAcf() \textbar{} ggPacf()

If the data are from an ARIMA(p,d,0) or ARIMA(0,d,q) model, then the ACF
and PACF plots can be helpful in determining the value of p/q. If p and
q are both positive, then the plots do not help in finding suitable
values of p and q.

The data may follow an ARIMA(p,d,0) model if the ACF and PACF plots of
the differenced data show the following patterns:

the ACF is exponentially decaying or sinusoidal; there is a significant
spike at lag p in the PACF, but none beyond lag p.

The data may follow an ARIMA(0,d,q) model if the ACF and PACF plots of
the differenced data show the following patterns:

the PACF is exponentially decaying or sinusoidal; there is a significant
spike at lag q in the ACF, but none beyond lag q.

seaosnal arima: arima(p,d,q) (P,Q,D)\_m, m = num of observations per
year

The seasonal part of an AR or MA model will be seen in the seasonal lags
of the PACF and ACF. For example, an ARIMA(0,0,0)(0,0,1)\_12 model will
show:

a spike at lag 12 in the ACF but no other significant spikes;
exponential decay in the seasonal lags of the PACF (i.e., at lags 12,
24, 36, \ldots{}).

Arima(euretail, order=c(0,1,3), seasonal=c(0,1,1))

auto.arima(euretail, stepwise=FALSE, approximation=FALSE)

comparing information criteria is only valid for ARIMA models of the
same orders of differencing.

Other

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  ma
\item
  wma
\item
  X11 method: seasonal::seas(x11 = ''). from this output seasonal() will
  extract the seasonal component, trendcycle() will extract the
  trend-cycle component, remainder() will extract the remainder
  component, and seasadj() will compute the seasonally adjusted time
  series.
\item
  SEATS: seasonal::seas()
\item
  STL: best so far but only uses additive decomposition.
  stl(t.window=13, s.window=``periodic'', robust=TRUE). automate picking
  of first 2 params with mstl(). A short-cut approach is to use the
  stlf() function. The following code will decompose the time series
  using STL, forecast the seasonally adjusted series, and return
  reseasonalize the forecasts: stlf(elecequip, method=`naive')
\end{enumerate}

msts \textbar{} mstsl \textbar{} tbats \textbar{} bld.mbb.bootstrap

Auto Model:

forecast(): You can use this directly if you have no idea which model to
use, or use it to produce forecasts after fitting a model.

Hierarchical:

gts() \textbar{} hts() \textbar{} aggts()

\begin{itemize}
\tightlist
\item
  Feature Engineering
\end{itemize}

\url{https://github.com/robjhyndman/tsfeatures}

There are several useful predictors that occur frequently when using
regression for time series data:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  A trend variable can be specified in the tslm() function using the
  trend predictor.
\item
  dummy variables for time like day of week. The tslm() function will
  automatically handle this situation if you specify the predictor
  season.
\item
  interventions, eg. competitor activity
\item
  trading days: bizdays()
\item
  distirbuted lags
\item
  easter()
\end{enumerate}

Use lagged values of predictors

A log-log functional form is specified as log(y) = beta\_0 +
beta\_1*log(x) + epsilon

scaling by monthday() by population or by inflation for money

Logarithms are useful because they are interpretable: changes in a log
value are relative (or percentage) changes on the original scale. So if
log base 10 is used, then an increase of 1 on the log scale corresponds
to a multiplication of 10 on the original scale. Another useful feature
of log transformations is that they constrain the forecasts to stay
positive on the original scale. A log transformation can address
heteroskedasticity.

boxCox()/boxCox.lambda(), power transforms,

bias adjustments: mean of forecast distribution instead of median for
back-transformed forecast. Bias adjustment is not done by default in the
forecast package. If you want your forecasts to be means rather than
medians, use the argument biasadj=TRUE when you select your Box-Cox
transformation parameter.

\begin{itemize}
\tightlist
\item
  Model Evaluation
\end{itemize}

train-test split: window() / subset()

accuracy()

tsCV()

A great advantage of the ETS statistical framework is that information
criteria can be used for model selection - AIC, AIC\_c, and BIC

leave-one-out cross-validation statistic : CV(fit.consMR) for model
selection

we recommend that one of the AICc, AIC, or CV statistics be used, each
of which has forecasting as their objective.

When comparing forecast methods applied to a single time series, or to
several time series with the same units, the MAE is popular as it is
easy to both understand and compute. A forecast method that minimizes
the MAE will lead to forecasts of the median, while minimizing the RMSE
will lead to forecasts of the mean. Consequently, the RMSE is also
widely used, despite being more difficult to interpret. Percentage
errors have the advantage of being unit-free, and so are frequently used
to compare forecast performances between data sets. The most commonly
used measure is Mean Absolute Percentage Error (MAPE) or symmetric MAPE.
forecast errors are different from residuals in two ways. First,
residuals are calculated on the training set while forecast errors are
calculated on the test set. Second, residuals are based on one-step
forecasts while forecast errors can involve multi-step forecasts.

A good forecasting method will yield residuals with the following
properties:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  The residuals are uncorrelated. If there are correlations between
  residuals, then there is information left in the residuals which
  should be used in computing forecasts.
\item
  The residuals have zero mean. If the residuals have a mean other than
  zero, then the forecasts are biased. (Adjusting for bias is easy: if
  the residuals have mean m then add m to all forecasts and the bias
  problem is solved.)
\item
  It is useful (but not necessary) for the residuals to also have the
  following two properties: the residuals have constant variance and are
  normally distributed.
\end{enumerate}

use the Box-ljung test to test for autocorrelations: h\_0: no
autocorrelation, h\_a: autocorrelation.

checkresiduals() which will produce a time plot, ACF plot and histogram
of the residuals (with an overlayed normal distribution for comparison),
and do a Ljung-Box test with the correct degrees of freedom.

-Notes

Fourier terms in lm good for intraday data.

Methods with multiplicative trend produce poor forecasts.

For stock market prices and indexes, the best forecasting method is
often the naïve method. Use also dynamic regression.

Create time series with ts(mydata{[}-1{]}, start = c(1981, 1), frequency
= 4)

Time series that show no auto-correlation are called white noise. For
white noise series, we expect each auto-correlation to be close to zero.
For a white noise series, we expect 95\% of the spikes in the ACF to lie
within +/- 2 * sqrt(T) where T is the length of the time series.

The possible time series (TS) scenarios can be recognized by asking the
following questions: 1) TS has a trend? If yes, is the trend increasing
linearly or exponentially? 2) TS has seasonality? If yes, do the
seasonal components increase in magnitude over time?

Additive model for linear trend and multiplicative model for exponential
trend. Additive for trend and Multiplicative and Additive for seasonal
components. For trends that are exponential, we would need to use a
multiplicative model. For increasing seasonality components, we would
need to use a multiplicative model model as well.

Therefore we can generalize all of these models using a naming system
for ETS:

Error is the error line we saw in the time series decomposition part
earlier in the course. If the error is increasing similar to an
increasing seasonal components, we would need to consider a
multiplicative design for the exponential model.

A time series model that has a constant error, linear trend, and
increasing seasonal components means we would need to use an ETS model
of ETS(N,A,M)

A time series model that has increasing error, exponential trend, and no
seasonality means we would need to use an ETS model of:

non-stationary: This plot shows an upward trend and seasonality.

stationary: This plot revolves around a constant mean of 0 and shows
contained variance.

-Metrics

Percentage errors, like MAPE, are useful because they are scale
independent, so they can be used to compare forecasts between different
data series, unlike scale dependent errors. The disadvantage is that it
cannot be used if the series has zero values.

Mean Absolute Percentage Error (MAPE) is also often useful for purposes
of reporting, because it is expressed in generic percentage terms it
will make sense even to someone who has no idea what constitutes a
``big'' error in terms of dollars spent or widgets sold.

Mean Absolute Scaled Error (MASE) is another relative measure of error
that is applicable only to time series data. It is defined as the mean
absolute error of the model divided by the the mean absolute value of
the first difference of the series. Thus, it measures the relative
reduction in error compared to a naive model. Ideally its value will be
significantly less than 1 but is relative to comparison across other
models for the same series. Since this error measurement is relative and
can be applied across models, it is accepted as one of the best metrics
for error measurement.

AIC\_c for model selection.

\subsection{Intro}\label{intro-18}

\subsection{Assumptions}\label{assumptions-18}

\subsection{Characteristics}\label{characteristics-13}

\subsection{Evaluation}\label{evaluation-17}

\subsection{Pros/Cons}\label{proscons-18}

\textbf{Pros}

\textbf{Cons}

\begin{itemize}
\item
  Seasonal Decomposition; Use additive model when the magnitude of the
  seasonal fluctuations surrounding the general trend doesn't vary over
  time. Use the multiplicative model when the magnitude of the seasonal
  fluctuations surrounding the general trend appear to change in a
  proportional manner over time. Go from additive to multiplicative with
  a log transformation.
\item
  White noise: Describes the assumption that each element in the time
  series is a random draw from N(0, constant variance). Also called a
  stationary series. Time series with trends or seasonality are not
  stationary.
\end{itemize}

\textbf{ARIMA(p,d,q) (Auto-Regressive Integrated Moving Average) Models}

\begin{itemize}
\item
  AR(p): auto-regressive component for lags on the stationary series.
  The AR models sayd that the value of a variable at a specific time is
  related to the value of the variable at previous times.
\item
  I(d): Integrated component for a series that needs to be differenced.
\item
  MA(q): Moving average component for lag of the forecast errors. In a
  moving average model of order q, each value in a time series is
  predicted from the linear combination of the previous q errors. The
  value of a variable at a specific time is related to the residuals of
  prediction at previous times.
\item
  In an auto-regressive model of order p, each valeu in a time series is
  predicted from a linear combination of the previous p values.
\item
  Procedure: Make stationary if necessary by differencing. Determine
  possible values of p and q. Assess model fit and try other values of p
  and q to overfit. Make forecasts with the final model.
\item
  Augmented Dicky-Fuller Test: This tests whether or not a time series
  is stationary. h0: series is not stationary, ha: the series is
  stationary.
\item
  Determine p and q: Look at autocorrelation (AC) and partial
  correlation (PAC) functions. AC measures the way observations relate
  to each other. PAC measure the way observations relate to each other
  after accounting for all other intervening observations. Plot of the
  autocorrelation function (ACF) displays correlation of the series with
  itself at different lags. A plot of the PAC displays the amount of
  autocorrelation not explained by lower order correlations. Spikes in
  the PACF will choose for AR(p). Spikes in the ACF will choose MA(q).
\end{itemize}

\textbf{Assess Model Fit}

\begin{itemize}
\item
  Appropriate model should resemble white noise. Check scatterplot of
  residuals vs fit to check constant variance and qqplot to check
  normality. Autocorrelations should be zero to check for violation of
  independent errors.
\item
  Box-Ljung Test: Check if all autocorrelations are zero, i.e the series
  is of white noise. H0: autocorrelations are all 0. ha: At least one is
  nonzero.
\item
  Overfit model with extra AR/MA terms and compare using AIC/BIC.
\item
  Interpretation: AR coefficient closer to 1 means series returns to
  mean slowly, vice versa for closer to 0.
\item
  MA(1) coefficient indicates how much the shock of the previous time
  period is retained in the current time period. MA(2) refers to the
  previous two time periods.
\end{itemize}

\chapter{Stats}\label{stats}

\subsection{General}\label{general-5}

Bayes Rule: Posterior = Likelihood * Prior / Marginal Likelihood

Likelihood P(Data\textbar{}theta): Probability of the date could be
generated by a model with the given parameter(s)

Prior P(theta): Probability of parameter value

Posterior P(theta\textbar{}D): Credibility of the parameter value with
the data taken into account.

Marginal Likelihood (Evidence): Probability of evidence

Variance: Spread of the data \textbar{} Distance from the mean

Standard Deviation: Square root of the variance \textbar{} Same units as
the data

Mean: Average/Expected Value

Covariance: Measure of how two random values change together. Negative -
variables show opposite behavior ( As A increases B decreases). Positive
- variables show similar behavior (As A increases B increases)

Pearson Correlation: Normalized version of covariance

Median: Number at the halfway point of the dataset, found through
sorting data into ascending order and selecting middle point

Mode: Most frequent value in dataset

Note about Populations and Samples: Populations have parameters. Samples
have statistics.

Distributions:

Normal/Gaussian: Parameters are mean and standard deviation. Bell
shaped. z-score: indicates how many standard deviations an element is
from the mean

Student t: used to estimate the mean of a normally distributed
population where the sample size is small and the standard deviation is
unknown

A t test (t.test in R) is a special case of an ANOVA and a paired t test
(parameter paired = T) is a special case of a repeated measures ANOVA.

Sample size: A good formula is: if your sample is picked uniformly at
random and is of size at least:

\(\frac{-log(\frac{d}{2})}{2e^2}\)

then with probability at least 1-d your sample measured proportion q is
no further away than e from the unknown true population proportion p
(what you are trying to estimate). Need to estimate with 99\% certainty
the unknown true population rate to +- 10\% precision? That is d=0.01,
e=0.1 and a sample of size 265 should be enough. A stricter precision of
+-0.05 causes the estimated sample size to increase to 1060, but
increasing the certainty requirement to 99.5\% only increases the
estimated sample size to 300.

\begin{itemize}
\item
  Type I Error: Saying there is an effect when there isn't one. Type II
  error: concluding there is no effect when, in fact, there is one.
\item
  The MWW RankSum test is a useful test to determine if two
  distributions are significantly different or not. Unlike the t-test,
  the RankSum test does not assume that the data are normally
  distributed, potentially providing a more accurate assessment of the
  data sets.
\item
  Probably the most useful types of statistics for skewed probability
  distributions are quantiles.
\item
  Hypothesis testing requires an inferential-statistical approach.
  Crucial are meaningful distributions of test statistics, on which
  p-values for hypothesis tests can be based. It is not trivial to
  construct such ``meaningful distributions'' for complete network data!
\item
  Random sampling \& random assignment: generalizable \& causal
\item
  Get 95\% ci around p-value
\item
  There is no one rule about when a number is not accurate enough to
  use, but as a rule of thumb, you should be cautious about using any
  number with a MOE (Margin-of-error) over 10\%.
\item
  Probability is calibrated if it is empirically correct, i.e.~the
  empirical probability matches the theoretical one.
\item
  \href{https://artax.karlin.mff.cuni.cz/r-help/library/lsr/html/cohensD.html}{Effect
  size}
\item
  jarque.bera.test: Tests the null of normaility for the series using
  the Jarque-Bera test statistics
\item
  box.test: Compute the Box-Pierce or Ljung-Box test statistics for
  examining the null of independence in a given series
\item
  shapiro.test: Test for normality
\item
  Sum of normally distributed random variables: sum of means \& sum of
  variances
\item
  Zero correlation doesn't mean independent variables. Covariance only
  determines linear relationships.
\item
  test heteroscedaticity: breusch-pagan or ncv test
\item
  Test Normality: Shapiro--Wilk test is a test of normality
\item
  Bypass inferential stats if features at least 1/10 of data points.
\item
  The probability (p-value) of observing results at least as extreme as
  what is present in your data sample. P-value less than .05 retain h\_0
  else reject h\_0.
\item
  One Sample T-test: To examine the average difference between a sample
  and the known value of the population mean. Assumes the population
  from which the sample is drawn is normally distributed and the sample
  observations are randomly drawn and independent.
\item
  Two Sample T-test: To examine the average difference between two
  samples drawn from two different populations. Assumes the populations
  from which the samples are drawn are normally dist, the standard
  deviations of the two populations are equal, and sample observations
  are randomly drawn and independent.
\item
  One-Way ANOVA: To assess the equality of means of two or more groups.
  Assumes the populations from which the samples are drawn are normally
  dist, the standard deviations of the populations are equal, and sample
  observations are randomly drawn and independent.
\item
  Chi-Squared Test of Independence: To test whether two categorical
  variables are independent. Assumes the sample observations are
  randomly drawn and independent.
\item
  F-Test: To assess whether the variances of two different populations
  are equal. Assumes the population from which the sample is drawn is
  normally distributed and the sample observations are randomly drawn
  and independent.
\item
  Barlett Test: F-Test for more than two populations.
\item
  If we take a larger and larger sample from a population, its
  distribution will tend to become normal no matter what it is
  initially. It won't. The Central Limit Theorem, the misreading of
  which is the cause of this mistake, refers to the distribution of
  standardized sums of random variables as their number grow, not to the
  distribution of a collection of random variables.
\item
  Frequentists: Probability is a measure of the the frequency of
  repeated events. The parameters are fixed (but unknown) and data are
  random.
\item
  Bayesians: Probability is a measure of the degree of certainty about
  values, so the interpretation is that rameters are random and data are
  fixed.
\item
  Independent rvs are uncorrelated. That is Cor (X,Y)=0
\item
  Don't sum sd's, sum variances.
\item
  One Sample T-Test? To examine the average difference between a sample
  and the known value of the population mean. Assumptions: The
  population from which the sample is drawn is normally distributed.
  Sample observations are randomly drawn and independent.
\item
  When do we use the Two Sample T-Test? To examine the average
  difference between two samples drawn from two different populations.
  Assumptions: The populations from which the samples are drawn are
  normally dist. The standard deviations of the two populations are
  equal. Sample observations are randomly drawn and independent.
\item
  When do we use the F-Test? To assess whether the variances of two
  different populations are equal. Assumptions: The populations from
  which the samples are drawn are normally dist. Sample observations are
  randomly drawn and independent.
\item
  When do we use One-Way ANOVA? To assess the equality of means of two
  or more groups. NB: When there are exactly two groups, this is
  equivalent to a Two Sample T-Test. Assumptions: The populations from
  which the samples are drawn are normally dist. The standard deviations
  of the populations are equal. Sample observations are randomly drawn
  and independent.
\item
  When do we use the chisq2 Test of Independence? To test whether two
  categorical variables are independent. Assumptions: Sample
  observations are randomly drawn and independent.
\item
  To perform the one-way ANOVA, we can use the f\_oneway() function of
  the SciPy package.
\item
  The degrees of freedom is the number of data rows minus the number of
  coefficients fit.
\item
  Hypothesis tests aim to describe the plausibility of a parameter
  taking on a specific value. Confidence intervals aim to describe a
  range of plausible values a parameter can take on. If the value of the
  parameter specified by H0 is contained within the 95\% confidence
  interval, then H0 cannot be rejectedat the 0.05 p-value threshold. If
  the value of the parameter specified by H0 is not contained within the
  95\% confidence interval, then H0 can be rejectedat the 0.05 p-value
  threshold.
\end{itemize}

\subsection*{Introduction to Data}\label{introduction-to-data}
\addcontentsline{toc}{subsection}{Introduction to Data}

There is no random sampling since the subjects of the study were
volunteers, so the results cannot be generalized to all people. However,
due to random assignment, we are able to infer a causal link between the
belief information is stored and the ability to recall that same
information.

Explanatory variables are conditions you can impose on the experimental
units, while blocking variables are characteristics that the
experimental units come with that you would like to control for.

In random sampling, you use stratifying to control for a variable. In
random assignment, you use blocking to achieve the same goal.

Control: compare treatment of interest to a control group

Randomize: randomly assign subjects to treatments

Replicate: collect a sufficiently large sample within a study, or
replicate the entire study

Block: account for the potential effect of confounding variables

Group subjects into blocks based on these variations, and randomize
within each block to treatment groups

Random Sampling + Random Assignment: Causal + generalizable

No Random Sampling + Random Assignment: Causal + not generalizable

Random Sampling + No Random Assignment: Not causal + generalizable

No Random Sampling + No Random Assignment: Not causal + not
generalizable

\subsection{Statistical Tests}\label{statistical-tests}

The appropriate sample size depends on many things, chiefly the
complexity of the analysis and the expected effect size. The ``30'' rule
comes from one of the simplest cases: Whether to use a z-test or t-test
for comparing two means.

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  for large n (sample size), many distributions can be approximated to
  normal. {[} courtesy : Central Limit Theorems {]}
\item
  The approximation becomes better with larger n, given other things
  remaining the same.
\end{enumerate}

However if you are wondering why 30 ? why not 25 or 40 or something else
? Note that actual answer will depend:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  how much error are you going to tolerate
\item
  size of the effect
\item
  exact distribution family (like t vs chi-square vs gamma)
\end{enumerate}

The power of any test of statistical significance is defined as the
probability that it will reject a false null hypothesis. Statistical
power is inversely related to beta or the probability of making a Type
II error. In short, power = 1 -- β. In plain English, statistical power
is the likelihood that a study will detect an effect when there is an
effect there to be detected. If statistical power is high, the
probability of making a Type II error, or concluding there is no effect
when, in fact, there is one, goes down. Statistical power is affected
chiefly by the size of the effect and the size of the sample used to
detect it. Bigger effects are easier to detect than smaller effects,
while large samples offer greater test sensitivity than small samples.

The following four quantities have an intimate relationship:

\begin{itemize}
\item
  sample size
\item
  effect size
\item
  significance level = P(Type I error) = probability of finding an
  effect that is not there
\item
  power = 1 - P(Type II error) = probability of finding an effect that
  is there
\end{itemize}

Given any three, we can determine the fourth.

\textbf{Other}

\begin{itemize}
\item
  The power of any test of statistical significance is defined as the
  probability that it will reject a false null hypothesis. Statistical
  power is inversely related to beta or the probability of making a Type
  II error. In short, power = 1 -- beta.
\item
  For quantitative explanatory variables, effect sizes always have the
  unit of the response variable divided by the unit of the explanatory
  variable.
\item
  \href{http://www.evanmiller.org/ab-testing/sample-size.html}{Power
  Analysis Calculator}
\end{itemize}

\subsection{Stats For Hackers}\label{stats-for-hackers}

\begin{itemize}
\item
  Methods: Simluation, Boostrapping (with replacement, not good with
  ranks), Shuffling (Works when assumption is that the data are the
  same, representative samples, non-longitudinal data), \& Cross
  Validation.
\item
  Assumes iid.
\end{itemize}

\subsection{Think Stats}\label{think-stats}

\begin{itemize}
\item
  Is it possible that the apparent effect is due to selection bias or
  some other error in the experimental setup? If so, then we might
  conclude that the effect is an artifact; that is, something we created
  (by accident) rather than found.
\item
  The sampling distribution is the distribution of the samples mean.
\item
  The CDF is the function that maps values to their percentile rank in a
  distribution.
\item
  I'll start with the exponential distribution because it is easy to
  work with. In the real world, exponential distributions come up when
  we look at a series of events and measure the times between events,
  which are called interarrival times. If the events are equally likely
  to occur at any time, the distribution of inter-arrival times tends to
  look like an exponential distribution.
\item
  CLT: More specifically, if the distribution of the values has mean and
  standard deviation μ and σ, the distribution of the sum is
  approximately N (nμ, nσ\^{}2).
\item
  Another option is to report just the Bayes Factor or likelihood ratio,
  P(E \textbar{} H A ) / P(E\textbar{}H 0 ), rather than the posterior
  probability.
\item
  Statistical power is the probability that the test will be positive if
  the null hypothesis is false. In general, the power of a test depends
  on the sample size, the magnitude of the effect, and the threshold α.
\item
  If there are no outliers, the sample mean minimizes the mean squared
  error
\item
  An estimator is unbiased if the expected total (or mean) error, after
  many iterations of the estimation game, is 0.
\item
  A challenge in measuring correlation is that the variables we want to
  compare might not be expressed in the same units. For example, height
  might be in centimeters and weight in kilograms. And even if they are
  in the same units, they come from different distributions. There are
  two common solutions to these problems:
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Transform all values to standard scores. This leads to the Pearson
  coefficient of correlation.
\item
  Transform all values to their percentile ranks. This leads to the
  Spearman coefficient.
\end{enumerate}

\subsection{AB Testing Overview}\label{ab-testing-overview}

Guide: Choose metric (CTR), review stats (Hypothesis Testing), design,
analyse

Need to have clear control and thing to test. Also need time. Frequency
of customer interaqction is important.

shopping and searching not independent events.

Point estimate of CTR (users who clicked)/users. Need margin of error
for estimate. Approximate binomial by normal if N*p \& N(1-p)
\textgreater{} 5.

marin of error = Z*(stndad error)

se = sqrt(p(1-p)/n)

Z for 95\% s 1.96 and 2.58 for 99\%.

for Two samples calculate a pooled CTR point estimate and standard
error.

Need to be substantive (a worthy difference) in addition to being
statistically significant. Some changes might not be useful due to the
resources one might want to allocate.

Result are repeatable (stat sig) bar should be lower than business
interesting (practically sig)

Stat Power: How many page view necessary to get a stat sig result.

Size v Power: The smaller the effect or increased confidence you want to
detect/have (greater power of experiment), the larger the sample you
need to use.

alpha = P(reject null when true) beta = P(accept null when false)

small sample: low alpha, high beta larger sample: low alpha, low beta

1- alpha = confidence level 1 - beta = sensitivity (often 80\%)

practical significance: 2\%, alpha = 0.05, beta = 0.2

sample size dp\_to ctr (increase in se), confidence level, sensitivity
sample size invp\_to practical sig (larger changes easier to detect)

above is design of experiment. below is analysis:

estimate diff: experimental prop - control prop margin of error = z*se

conf int: estimated diff +/- margin of error

\textbf{Policy \& Ethics}

In the study, what risk is the participant undertaking? The main
threshold is whether the risk exceeds that of ``minimal risk''.

Next, what benefits might result from the study? Even if the risk is
minimal, how might the results help?

Third, what other choices do participants have? For example, if you are
testing out changes to a search engine, participants always have the
choice to use another search engine.

Finally, what data is being collected, and what is the expectation of
privacy and confidentiality? This last question is quite nuanced,
encompassing numerous questions: Do participants understand what data is
being collected about them? What harm would befall them should that data
be made public? Would they expect that data to be considered private and
confidential?

Our recommendation is that there should be internal reviews of all
proposed studies by experts regarding the questions:

Are participants facing more than minimal risk? Do participants
understand what data is being gathered? Is that data identifiable? How
is the data handled?

Internal process recommendations Finally, regarding internal process of
data handling, we recommend that:

Every employee who might be involved in A/B test be educated about the
ethics and the protection of the participants. Clearly there are other
areas of ethics beyond what we've covered that discuss integrity,
competence, and responsibility, but those generally are broader than
protecting participants of A/B tests (cite ACM code of ethics).

All data, identified or not, be stored securely, with access limited to
those who need it to complete their job. Access should be time limited.
There should be clear policies of what data usages are acceptable and
not acceptable. Moreover, all usage of the data should be logged and
audited regularly for violations. You create a clear escalation path for
how to handle cases where there is even possibly more than minimal risk
or data sensitivity issues.

rate better for usability test than prob

\textbf{Choosing \& Characterizing Metrics}

\textbf{Define}

Make sure to granularize funnel

\textbf{Intuition}

NA

\textbf{Characterize}

Focus groups, UER (user experience study), survey, experiments,
retrospective analysis, external data

temporal effects across hours, days, weeks are important

different browsers screw with the CTR depending on how they deal with JS

Can use cookies to track users

Metric definitions

Def \#1 (Cookie probability): For each , number of cookies that click
divided by number of cookies

Def \#2 (Pageview probability): Number of pageviews with a click within
divided by number of pageviews

Def \#3 (Rate): Number of clicks divided by number of pageviews

CTR vs CTP: The first can be \textgreater{}1.

Take into consideration mean and median. Median is robust but might not
be useful so look at percentiles.

Measuring sensitiviyt and robustness: 1) Running experiments to see if
metrics move as predicted. A vs A experiments: Measure people who saw
the same thing to see if there is a difference between them according to
the metric or its too sensitive. Look at previous experiments. 2)
Retrospective analysis of logs to see how the metrics responded in the
past.

Metric For Loading Vid: Look at distribution of load times for vid. Want
a robust metric that doesn't really change for comparable vids. But wnat
a metric that is sensitive to a change that you care about like
resolution.

How to compare experiment vs control: difference or relative change.
Relative change allows you to keep same significance boundary.

Check that praticial sig level is good for metric. Need to have handle
on variability for confidence interval.

sign test: Good for looking to implement a change, but doesn't give
effect size.

empirical vs analytic: underlying distribution could be weird so do
empirical. Analyticcal for simple metrics. If empirical and anlytical
don't agree then run AvA test.

A vs A test: Std proportional to square root of number of samples.
Diminishing returns for many tests. Can use bootstrap if don't have
enough traffic for a big A v A test.

\textbf{Designing An Experiment}

\textbf{Choose Subject}

Unit of Diversion: How to indentify a person. Cookie (could clear so
change), userid, deviceid, and address (changes all the time).

Intra-user vs inter-user experiment.

\textbf{Choose Population}

Size

Duration

Cohort: People who enter the experiment at the same time.

Cohort \textgreater{} Population: Learning effects, user retention

Can reduce sample if just targeting english speaking traffic

When going to run experiment: holiday or not? How many people to put
through experiment and control?

\textbf{Analyzing Results}

Sanity Checks:

Pass all before move on

Good invariants: \# of events, video load time (user has no control over
it). Want these to be about the same for experiment and control.

Single Metric:

sign test

Multiple Metrics:

More metrics increase false positive chance. Use Bonferroni assumption:
makes no independence assumption and is conservative.

margin of error smaller than observed diff so the confidence interval
won't include 0.

Different strategies: family-wise error rate, false discovery rate,

Gotchas:

Simpson's Paradox

Usually A/B testing works for testing changes in elements in the web
page. A/B testing framework is following sequence:

Design a research question.

Choose test statistics method or metrics to evaluate experiment.

Designing the control group and experiment group.

Analyzing results, and draw valid conclusions.

\chapter{Deep Learning}\label{deep-learning}

\begin{itemize}
\item
  restnet, attention \textgreater{} RNN / LSTM
\item
  ``Training with large minibatches is bad for your health. More
  importantly, it's bad for your test error. Friends dont let friends
  use minibatches larger than 32.'' - Yann Le Cun
\item
  \href{https://hackernoon.com/deep-learning-cheat-sheet-25421411e460}{DL
  Cheatsheet}
\item
  By reducing learning rate (for example, to 0.001), Test loss drops to
  a value much closer to Training loss. In most runs, increasing Batch
  size does not influence Training loss or Test loss significantly.
  However, in a small percentage of runs, increasing Batch size to 20 or
  greater causes Test loss to drop slightly below Training loss.
\item
  Reducing the ratio of training to test data from 50\% to 10\%
  dramatically lowers the number of data points in the training set.
  With so little data, high batch size and high learning rate cause the
  training model to jump around chaotically (jumping repeatedly over the
  minimum point).
\item
  There are several activation functions you may encounter in practice:

  \begin{itemize}
  \item
    Sigmoid: Takes a real-valued input and squashes it to range between
    0 and 1 (\(σ(x) = 1 / (1 + exp(−x))\))
  \item
    Softmax: Same end result as sigmoid, but different function.
  \item
    tanh: Takes a real-valued input and squashes it to the range {[}-1,
    1{]} (\(tanh(x) = 2σ(2x) − 1\))
  \end{itemize}
\item
  ReLU: The rectified linear activation function (called ReLU) has been
  shown to lead to very high-performance networks. This function takes a
  single number as an input, returning 0 if the input is negative, and
  the input if the input is positive. \(f(x) = max(0, x)\)
\item
  softmax doesn't like multi-label classification
\item
  Embeddings vs One-Hot Encoding: Embeddings are better than One-Hot
  Encodings because it allows for relationships to be shown between
  days.
\item
  Advised to scale features
\item
  \href{https://www.youtube.com/watch?v=nqEYVzJLR_c\&feature=youtu.be\&t=31}{Rule
  of 30}: A change that affects at least 30 data points in your
  validation set is usually significant and not just noise.
\item
  Gradient descent takes about 3 times longer than the loss function.
  Stochastic gradient descent works off an estimate of the loss function
  from a sample of the training set. Scales better than normal gradient
  descent.
\item
  Momentum and learning rate decay are good for knowing which way to go
  in gradient descent.
\item
  Always try to lower your learning rate for improvement.
\item
  ADAGRAD is another optimization option that takes care of some of the
  hyperparameters for you.
\item
  n inputs and k outputs gives you \((n+1)*k\) parameters.
\item
  Back propogation takes about twice the memory as forward propogation.
\item
  Stop model from overoptimizing on training set: early termination
  (stop as soon as performance on validation set drops) and
  regularization which applies artifical constraints to reduce the
  number of free parameters while making it difficult to optimize. Uses
  l2 regularization or dropout. Dropout works by randomly setting
  activations from one layer to the next to 0 repeatly. Forces the
  network to learn redundant representations, and takes average
  consensus for final prediction. Makes things more robust and prevents
  overfitting. Scale the non-zero activates by 2 to get the right
  average. If it doesn't work, go deeper.
\item
  Two ways are to average the yellow, blue, and green channels or to use
  YUV representation. If position on the screen doesn't matter then use
  translation invariance. Use weight sharing if this occurs in text.
\item
  Things that don't chnage across time, space, etc are called
  statistical invariants.
\item
  RNN's use shared parameters over time to extract patterns. Uses a
  recurrent connection to provide a summary of the past and pass this
  info to the classifier.
\item
  Backpropagation occurs throw time to the beginning. All derivative
  applied to same parameters, so very correlated. Bad for stochastic
  gradient descent and makes math very unstable. Gradients are either
  zero (doesn't remember the past well) or infinity. The latter is fixed
  by gradient clipping and the former by LSTM (long-short term memory).
  LSTM replaced the rectified linear units by continuous functions. You
  can use dropout or L2 regularization with an LSTM.
\item
  Generally dealing with images, Convolutional Neural Network is used
  mostly because of its better accuracy results.
\item
  Model capacity: Same as underfitting and overfitting in bias-variance.
  Less nodes and hidden layers corresponds to simpler model and vice
  versa.
\item
  The perceptron is the simplest neural network. The perceptron is an
  iterative classification method.The perceptron starts with a random
  hyperplane then adjust its weights to separate the data.
\item
  BackProp Algorithm: Initially all the edge weights are randomly
  assigned. For every input in the training dataset, the ANN is
  activated and its output is observed. This output is compared with the
  desired output that we already know, and the error is ``propagated''
  back to the previous layer. This error is noted and the weights are
  ``adjusted'' accordingly. This process is repeated until the output
  error is below a predetermined threshold.
\item
  The last layer of a neural network captures the most complex
  interactions.
\item
  When plotting the mean-squared error loss function against
  predictions, the slope is \(2x(y-xb)\), or \(2input_data error\).
\item
  weights\_updated = weights - slope*learning\_rate
\item
  This is exactly what's happening in the vanishing gradient problem --
  the gradients of the network's output with respect to the parameters
  in the early layers become extremely small. That's a fancy way of
  saying that even a large change in the value of parameters for the
  early layers doesn't have a big effect on the output.
\item
  Batch: Subset of the data used to calculate slopes during back
  propagation. Different batches are used to calculate different
  updates.
\item
  Epoch: One full pass through all the batches in the training data.
\item
  Stochastic gradient descent calculates slopes one batch at a time.
\item
  This network again uses the ReLU activation function, so the slope of
  the activation function is 1 for any node receiving a positive value
  as input.
\item
  Few people use kfold cv in deep learning because of the large datasets
  in play.
\item
  Dying neuron + vanishing gradient: Change activation function
\end{itemize}

\chapter{Probabilistic Programming}\label{probabilistic-programming}

\url{https://towardsdatascience.com/introduction-to-bayesian-linear-regression-e66e60791ea7}

Bayes Rule:

old\_prob {[}P(h)/p(\textasciitilde{}h){]} * strength\_of\_evidence
{[}p(e\textbar{}h)/p(e\textbar{}\textasciitilde{}h){]} = new\_prob
{[}p(h\textbar{}e)/p(\textasciitilde{}h\textbar{}e){]}

\begin{itemize}
\item
  Models have a lot of parameters classical methods that use simple
  point estimates of the parameters don't adequately capture uncertainty
  so we turn to Bayesian methods because Bayesian inference is one way
  of finding a large model with metadata.
\item
  The second reason we use Bayesian inference is for combining
  information you might have experimental data on the drug under one
  condition and aggregate data under another condition we can combine
  polls and election forecast of public opinion data from multiple poles
  and environmental statistics we have measurements of different quality
  Bayesian methods are particularly adapted tocombining information.
\item
  The third appeal of Bayesian methods and the sort of applications that
  I work on is that the inferences can map directly to decisions based
  on inferences express not its estimates and standard errors but rather
  as probability distributions we can take these probability
  distributions and pipe them directly into decision analysis for these
  reasons.
\item
  An advantage of recognizing that the prior distribution is a testable
  part of a Bayesian model is that it clarifies the role of the prior
  inference, and where it comes from.
\item
  Approximate Bayesian Computation
\item
  Laplace approximation: Approximate the posterior of a non-conjugate
  model
\item
  When one uses likelihood to get point estimates of model parameters,
  it's called maximum-likelihood estimation, or MLE. If one also takes
  the prior into account, then it's maximum a posteriori estimation
  (MAP). MLE and MAP are the same if the prior is uniform.
\item
  Beta conjugacy: Beta for prior \& binomial for likelihoood implies
  beta for posterior. Update: beta(alpha + successes, beta + failures).
  Mean: alpha/(alpha + beta)
\item
  Small alpha and beta means the prior is less informative
\item
  Empirical Bayes is an approximation to more exact Bayesian methods.
  With a lot of data it is a very good approximation.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ numpy.random }\ImportTok{import}\NormalTok{ beta }\ImportTok{as}\NormalTok{ beta_dist}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{N_samp }\OperatorTok{=} \DecValTok{10000} \CommentTok{#number of samples to draw}
\NormalTok{clicks_A }\OperatorTok{=} \DecValTok{450}
\NormalTok{views_A }\OperatorTok{=} \DecValTok{56000}
\NormalTok{clicks_B }\OperatorTok{=} \DecValTok{345}
\NormalTok{views_B }\OperatorTok{=} \DecValTok{49000}
\NormalTok{alpha }\OperatorTok{=} \FloatTok{1.1}
\NormalTok{beta }\OperatorTok{=} \FloatTok{14.2}
\NormalTok{A_samples }\OperatorTok{=}\NormalTok{ beta_dist(clicks_A }\OperatorTok{+}\NormalTok{ alpha, views_A }\OperatorTok{-}\NormalTok{ clicks_A }\OperatorTok{+}\NormalTok{ beta, N_samp)}
\NormalTok{B_samples }\OperatorTok{=}\NormalTok{ beta_dist(clicks_B }\OperatorTok{+}\NormalTok{ alpha, views_B }\OperatorTok{-}\NormalTok{ clicks_B }\OperatorTok{+}\NormalTok{ beta, N_samp)}
\end{Highlighting}
\end{Shaded}

Posterior prob that CTR\_A \textgreater{} CTR\_B given data is
np.mean(A\_samples \textgreater{} B\_samples). Prob that lift of A
relative to B is \textgreater{}=3\%:
np.mean(100*(A\_samples-B\_samples)/B\_samples \textgreater{} 3)

\chapter{NLP}\label{nlp}

\begin{itemize}
\tightlist
\item
  TF-IDF: A feature vectorization method widely used in text mining to
  reflect the importance of a term to a document in the corpus. Denote a
  term by t, a document by d, and the corpus by D. TF(i,d) is the number
  of times the term t appears in document d while document frequency
  DF(t,D) is the number of documents which contain the term t. If a term
  appears very often across the corpus it doesn't carry any special info
  about the document. Inverse document frequency is a numerical measure
  of how much info a term provides. If a term appears in all documents
  its IDF value is 0. TF-IDF is the product of TF and IDF. HashingTF
  uses a hash function to transform input into text.
\end{itemize}

\textbf{Basic Py NLP Pipeline}

\begin{itemize}
\item
  Converting text to numerical data is vectorization.
\item
  Tokenize: A token is an individual word (or group of words) extracted
  from a document, using whitespace or punctuation as separators.
\item
  Create bag of words: Count tokens within a document and normalised to
  de-emphasise tokens that appear frequently within a document.
\item
  Vectorization: Corpus represented as a large matrix, each row of which
  represents one of the documents and each column represents token
  occurance within that document. CountVectorizer or TF-IDF.
\item
  tf-idf \& cosine similarity
\end{itemize}

\subsection{Topic Modeling}\label{topic-modeling}

\begin{itemize}
\item
  Pointwise mutual information is any way to evaluate topic models. Uses
  joint probabilities to evaluate how much a word tells you a label.
\item
  Mutual information generalized pointwise mutual information averaged
  over all events.
\item
  If you're just using the probability vectors that come out of a topic
  model you are selecting a probability metric. If you want to do
  post-processing you something like mutual information or point was
  mutual information you're selecting another metric. Metrics always
  have a point of view.
\item
  They're just like human writers. Probability says I like really common
  words, pointwise mutual information as I like really really really
  discerning words and I don't care how common they are. Mutual
  information is sort of a balance between the two.
\item
  In topic modeling using latent Dirichlet distribution, how are
  ``topics'' represented? As distributions over words.
\end{itemize}

\chapter{Hierarchical Models}\label{hierarchical-models}

``Across counties, as the average mother's age increases the birth rate
decreases while correcting for state-level impacts'' BirthRate
\textasciitilde{} AverageAgeofMother + (1 \textbar{} State)

lmerTest

Analysis of Variance, or ANOVA, can be used to compare two different
lmer models and test if one model explains more variability than the
other model. We can use this to compare two lmer models. For example, if
we wanted to see if Year is important for predicting Crime in Maryland,
we can build a null model with only County as a random-effect and a year
model that includes Year.

\begin{itemize}
\item
  One far out suggestion in one of the links is to do PCA and then
  regress the original variables against the new axis.
\item
  Account for longitudinal data: random selection of one event from each
  student, partition cross validation by dates, pca finds major trends
  hidden within the data
\end{itemize}

\bibliography{book.bib,packages.bib}


\end{document}
