<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Data Science Cribsheeet</title>
  <meta name="description" content="A collection of quick notes on the field.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Data Science Cribsheeet" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A collection of quick notes on the field." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Data Science Cribsheeet" />
  
  <meta name="twitter:description" content="A collection of quick notes on the field." />
  




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="model.html">
<link rel="next" href="cdm.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">DS Cribsheet</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Workflow</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#preamble"><i class="fa fa-check"></i><b>1.1</b> Preamble</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#annotated-checklist"><i class="fa fa-check"></i><b>1.2</b> Annotated Checklist</a><ul>
<li class="chapter" data-level="1.2.1" data-path="index.html"><a href="index.html#preparation"><i class="fa fa-check"></i><b>1.2.1</b> Preparation</a></li>
<li class="chapter" data-level="1.2.2" data-path="index.html"><a href="index.html#import"><i class="fa fa-check"></i><b>1.2.2</b> Import</a></li>
<li class="chapter" data-level="1.2.3" data-path="index.html"><a href="index.html#tidy"><i class="fa fa-check"></i><b>1.2.3</b> Tidy</a></li>
<li class="chapter" data-level="1.2.4" data-path="index.html"><a href="index.html#utransform"><i class="fa fa-check"></i><b>1.2.4</b> uTransform</a></li>
<li class="chapter" data-level="1.2.5" data-path="index.html"><a href="index.html#uvisualize"><i class="fa fa-check"></i><b>1.2.5</b> uVisualize</a></li>
<li class="chapter" data-level="1.2.6" data-path="index.html"><a href="index.html#umodel"><i class="fa fa-check"></i><b>1.2.6</b> uModel</a></li>
<li class="chapter" data-level="1.2.7" data-path="index.html"><a href="index.html#communicate"><i class="fa fa-check"></i><b>1.2.7</b> Communicate</a></li>
<li class="chapter" data-level="1.2.8" data-path="index.html"><a href="index.html#deployment-maintenance"><i class="fa fa-check"></i><b>1.2.8</b> Deployment / Maintenance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="import-tidy.html"><a href="import-tidy.html"><i class="fa fa-check"></i><b>2</b> Import &amp; Tidy</a><ul>
<li class="chapter" data-level="2.1" data-path="import-tidy.html"><a href="import-tidy.html#general"><i class="fa fa-check"></i><b>2.1</b> General</a></li>
<li class="chapter" data-level="2.2" data-path="import-tidy.html"><a href="import-tidy.html#sql"><i class="fa fa-check"></i><b>2.2</b> SQL</a></li>
<li class="chapter" data-level="2.3" data-path="import-tidy.html"><a href="import-tidy.html#scraping"><i class="fa fa-check"></i><b>2.3</b> Scraping</a></li>
<li class="chapter" data-level="2.4" data-path="import-tidy.html"><a href="import-tidy.html#exploration"><i class="fa fa-check"></i><b>2.4</b> Exploration</a></li>
<li class="chapter" data-level="2.5" data-path="import-tidy.html"><a href="import-tidy.html#general-1"><i class="fa fa-check"></i><b>2.5</b> General</a></li>
<li class="chapter" data-level="2.6" data-path="import-tidy.html"><a href="import-tidy.html#missingness-imputation"><i class="fa fa-check"></i><b>2.6</b> Missingness &amp; Imputation</a></li>
<li class="chapter" data-level="2.7" data-path="import-tidy.html"><a href="import-tidy.html#data-table"><i class="fa fa-check"></i><b>2.7</b> Data Table</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="transform-visualize.html"><a href="transform-visualize.html"><i class="fa fa-check"></i><b>3</b> Transform &amp; Visualize</a><ul>
<li class="chapter" data-level="3.1" data-path="transform-visualize.html"><a href="transform-visualize.html#transform"><i class="fa fa-check"></i><b>3.1</b> Transform</a></li>
<li class="chapter" data-level="3.2" data-path="transform-visualize.html"><a href="transform-visualize.html#visualize"><i class="fa fa-check"></i><b>3.2</b> Visualize</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="model.html"><a href="model.html"><i class="fa fa-check"></i><b>4</b> Model</a><ul>
<li class="chapter" data-level="4.1" data-path="model.html"><a href="model.html#pre-processing"><i class="fa fa-check"></i><b>4.1</b> Pre-processing</a></li>
<li class="chapter" data-level="4.2" data-path="model.html"><a href="model.html#feature-engineering"><i class="fa fa-check"></i><b>4.2</b> Feature Engineering</a></li>
<li class="chapter" data-level="4.3" data-path="model.html"><a href="model.html#feature-selection"><i class="fa fa-check"></i><b>4.3</b> Feature Selection</a></li>
<li class="chapter" data-level="4.4" data-path="model.html"><a href="model.html#linear-regression"><i class="fa fa-check"></i><b>4.4</b> Linear Regression</a><ul>
<li class="chapter" data-level="4.4.1" data-path="model.html"><a href="model.html#intro"><i class="fa fa-check"></i><b>4.4.1</b> Intro</a></li>
<li class="chapter" data-level="4.4.2" data-path="model.html"><a href="model.html#assumptions"><i class="fa fa-check"></i><b>4.4.2</b> Assumptions</a></li>
<li class="chapter" data-level="4.4.3" data-path="model.html"><a href="model.html#evaluation"><i class="fa fa-check"></i><b>4.4.3</b> Evaluation</a></li>
<li class="chapter" data-level="4.4.4" data-path="model.html"><a href="model.html#proscons"><i class="fa fa-check"></i><b>4.4.4</b> Pros/Cons</a></li>
<li class="chapter" data-level="4.4.5" data-path="model.html"><a href="model.html#other"><i class="fa fa-check"></i><b>4.4.5</b> Other</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="model.html"><a href="model.html#ridgelasso"><i class="fa fa-check"></i><b>4.5</b> Ridge/Lasso</a><ul>
<li class="chapter" data-level="4.5.1" data-path="model.html"><a href="model.html#intro-1"><i class="fa fa-check"></i><b>4.5.1</b> Intro</a></li>
<li class="chapter" data-level="4.5.2" data-path="model.html"><a href="model.html#assumptions-1"><i class="fa fa-check"></i><b>4.5.2</b> Assumptions</a></li>
<li class="chapter" data-level="4.5.3" data-path="model.html"><a href="model.html#evaluation-1"><i class="fa fa-check"></i><b>4.5.3</b> Evaluation</a></li>
<li class="chapter" data-level="4.5.4" data-path="model.html"><a href="model.html#proscons-1"><i class="fa fa-check"></i><b>4.5.4</b> Pros/Cons</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="model.html"><a href="model.html#logistic-regression"><i class="fa fa-check"></i><b>4.6</b> Logistic Regression</a><ul>
<li class="chapter" data-level="4.6.1" data-path="model.html"><a href="model.html#intro-2"><i class="fa fa-check"></i><b>4.6.1</b> Intro</a></li>
<li class="chapter" data-level="4.6.2" data-path="model.html"><a href="model.html#assumptions-2"><i class="fa fa-check"></i><b>4.6.2</b> Assumptions</a></li>
<li class="chapter" data-level="4.6.3" data-path="model.html"><a href="model.html#evaluation-2"><i class="fa fa-check"></i><b>4.6.3</b> Evaluation</a></li>
<li class="chapter" data-level="4.6.4" data-path="model.html"><a href="model.html#proscons-2"><i class="fa fa-check"></i><b>4.6.4</b> Pros/Cons</a></li>
<li class="chapter" data-level="4.6.5" data-path="model.html"><a href="model.html#code-example"><i class="fa fa-check"></i><b>4.6.5</b> Code Example</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="model.html"><a href="model.html#poisson-regression"><i class="fa fa-check"></i><b>4.7</b> Poisson Regression</a><ul>
<li class="chapter" data-level="4.7.1" data-path="model.html"><a href="model.html#intro-3"><i class="fa fa-check"></i><b>4.7.1</b> Intro</a></li>
<li class="chapter" data-level="4.7.2" data-path="model.html"><a href="model.html#assumptions-3"><i class="fa fa-check"></i><b>4.7.2</b> Assumptions</a></li>
<li class="chapter" data-level="4.7.3" data-path="model.html"><a href="model.html#evaluation-3"><i class="fa fa-check"></i><b>4.7.3</b> Evaluation</a></li>
<li class="chapter" data-level="4.7.4" data-path="model.html"><a href="model.html#proscons-3"><i class="fa fa-check"></i><b>4.7.4</b> Pros/Cons</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="model.html"><a href="model.html#generalized-additive-models"><i class="fa fa-check"></i><b>4.8</b> Generalized Additive Models</a><ul>
<li class="chapter" data-level="4.8.1" data-path="model.html"><a href="model.html#intro-4"><i class="fa fa-check"></i><b>4.8.1</b> Intro</a></li>
<li class="chapter" data-level="4.8.2" data-path="model.html"><a href="model.html#assumptions-4"><i class="fa fa-check"></i><b>4.8.2</b> Assumptions</a></li>
<li class="chapter" data-level="4.8.3" data-path="model.html"><a href="model.html#evaluation-4"><i class="fa fa-check"></i><b>4.8.3</b> Evaluation</a></li>
<li class="chapter" data-level="4.8.4" data-path="model.html"><a href="model.html#proscons-4"><i class="fa fa-check"></i><b>4.8.4</b> Pros/Cons</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="model.html"><a href="model.html#bayesian"><i class="fa fa-check"></i><b>4.9</b> Bayesian</a></li>
<li class="chapter" data-level="4.10" data-path="model.html"><a href="model.html#naive-bayes"><i class="fa fa-check"></i><b>4.10</b> Naïve Bayes</a><ul>
<li class="chapter" data-level="4.10.1" data-path="model.html"><a href="model.html#intro-5"><i class="fa fa-check"></i><b>4.10.1</b> Intro</a></li>
<li class="chapter" data-level="4.10.2" data-path="model.html"><a href="model.html#assumptions-5"><i class="fa fa-check"></i><b>4.10.2</b> Assumptions</a></li>
<li class="chapter" data-level="4.10.3" data-path="model.html"><a href="model.html#characteristics"><i class="fa fa-check"></i><b>4.10.3</b> Characteristics</a></li>
<li class="chapter" data-level="4.10.4" data-path="model.html"><a href="model.html#evaluation-5"><i class="fa fa-check"></i><b>4.10.4</b> Evaluation</a></li>
<li class="chapter" data-level="4.10.5" data-path="model.html"><a href="model.html#proscons-5"><i class="fa fa-check"></i><b>4.10.5</b> Pros/Cons</a></li>
</ul></li>
<li class="chapter" data-level="4.11" data-path="model.html"><a href="model.html#lda"><i class="fa fa-check"></i><b>4.11</b> LDA</a><ul>
<li class="chapter" data-level="4.11.1" data-path="model.html"><a href="model.html#intro-6"><i class="fa fa-check"></i><b>4.11.1</b> Intro</a></li>
<li class="chapter" data-level="4.11.2" data-path="model.html"><a href="model.html#assumptions-6"><i class="fa fa-check"></i><b>4.11.2</b> Assumptions</a></li>
<li class="chapter" data-level="4.11.3" data-path="model.html"><a href="model.html#characteristics-1"><i class="fa fa-check"></i><b>4.11.3</b> Characteristics</a></li>
<li class="chapter" data-level="4.11.4" data-path="model.html"><a href="model.html#evaluation-6"><i class="fa fa-check"></i><b>4.11.4</b> Evaluation</a></li>
<li class="chapter" data-level="4.11.5" data-path="model.html"><a href="model.html#proscons-6"><i class="fa fa-check"></i><b>4.11.5</b> Pros/Cons</a></li>
</ul></li>
<li class="chapter" data-level="4.12" data-path="model.html"><a href="model.html#qda"><i class="fa fa-check"></i><b>4.12</b> QDA</a><ul>
<li class="chapter" data-level="4.12.1" data-path="model.html"><a href="model.html#intro-7"><i class="fa fa-check"></i><b>4.12.1</b> Intro</a></li>
<li class="chapter" data-level="4.12.2" data-path="model.html"><a href="model.html#assumptions-7"><i class="fa fa-check"></i><b>4.12.2</b> Assumptions</a></li>
<li class="chapter" data-level="4.12.3" data-path="model.html"><a href="model.html#characteristics-2"><i class="fa fa-check"></i><b>4.12.3</b> Characteristics</a></li>
<li class="chapter" data-level="4.12.4" data-path="model.html"><a href="model.html#evaluation-7"><i class="fa fa-check"></i><b>4.12.4</b> Evaluation</a></li>
<li class="chapter" data-level="4.12.5" data-path="model.html"><a href="model.html#proscons-7"><i class="fa fa-check"></i><b>4.12.5</b> Pros/Cons</a></li>
</ul></li>
<li class="chapter" data-level="4.13" data-path="model.html"><a href="model.html#advanced-bayesian"><i class="fa fa-check"></i><b>4.13</b> Advanced Bayesian</a></li>
<li class="chapter" data-level="4.14" data-path="model.html"><a href="model.html#clustering"><i class="fa fa-check"></i><b>4.14</b> Clustering</a></li>
<li class="chapter" data-level="4.15" data-path="model.html"><a href="model.html#k-means"><i class="fa fa-check"></i><b>4.15</b> K-Means</a><ul>
<li class="chapter" data-level="4.15.1" data-path="model.html"><a href="model.html#intro-8"><i class="fa fa-check"></i><b>4.15.1</b> Intro</a></li>
<li class="chapter" data-level="4.15.2" data-path="model.html"><a href="model.html#assumptions-8"><i class="fa fa-check"></i><b>4.15.2</b> Assumptions</a></li>
<li class="chapter" data-level="4.15.3" data-path="model.html"><a href="model.html#characteristics-3"><i class="fa fa-check"></i><b>4.15.3</b> Characteristics</a></li>
<li class="chapter" data-level="4.15.4" data-path="model.html"><a href="model.html#evaluation-8"><i class="fa fa-check"></i><b>4.15.4</b> Evaluation</a></li>
<li class="chapter" data-level="4.15.5" data-path="model.html"><a href="model.html#proscons-8"><i class="fa fa-check"></i><b>4.15.5</b> Pros/Cons</a></li>
</ul></li>
<li class="chapter" data-level="4.16" data-path="model.html"><a href="model.html#hierarchical-clustering"><i class="fa fa-check"></i><b>4.16</b> Hierarchical Clustering</a><ul>
<li class="chapter" data-level="4.16.1" data-path="model.html"><a href="model.html#intro-9"><i class="fa fa-check"></i><b>4.16.1</b> Intro</a></li>
<li class="chapter" data-level="4.16.2" data-path="model.html"><a href="model.html#assumptions-9"><i class="fa fa-check"></i><b>4.16.2</b> Assumptions</a></li>
<li class="chapter" data-level="4.16.3" data-path="model.html"><a href="model.html#characteristics-4"><i class="fa fa-check"></i><b>4.16.3</b> Characteristics</a></li>
<li class="chapter" data-level="4.16.4" data-path="model.html"><a href="model.html#evaluation-9"><i class="fa fa-check"></i><b>4.16.4</b> Evaluation</a></li>
<li class="chapter" data-level="4.16.5" data-path="model.html"><a href="model.html#proscons-9"><i class="fa fa-check"></i><b>4.16.5</b> Pros/Cons</a></li>
</ul></li>
<li class="chapter" data-level="4.17" data-path="model.html"><a href="model.html#pca"><i class="fa fa-check"></i><b>4.17</b> PCA</a><ul>
<li class="chapter" data-level="4.17.1" data-path="model.html"><a href="model.html#intro-10"><i class="fa fa-check"></i><b>4.17.1</b> Intro</a></li>
<li class="chapter" data-level="4.17.2" data-path="model.html"><a href="model.html#assumptions-10"><i class="fa fa-check"></i><b>4.17.2</b> Assumptions</a></li>
<li class="chapter" data-level="4.17.3" data-path="model.html"><a href="model.html#characteristics-5"><i class="fa fa-check"></i><b>4.17.3</b> Characteristics</a></li>
<li class="chapter" data-level="4.17.4" data-path="model.html"><a href="model.html#evaluation-10"><i class="fa fa-check"></i><b>4.17.4</b> Evaluation</a></li>
<li class="chapter" data-level="4.17.5" data-path="model.html"><a href="model.html#proscons-10"><i class="fa fa-check"></i><b>4.17.5</b> Pros/Cons</a></li>
<li class="chapter" data-level="4.17.6" data-path="model.html"><a href="model.html#other-1"><i class="fa fa-check"></i><b>4.17.6</b> Other</a></li>
</ul></li>
<li class="chapter" data-level="4.18" data-path="model.html"><a href="model.html#knn"><i class="fa fa-check"></i><b>4.18</b> KNN</a><ul>
<li class="chapter" data-level="4.18.1" data-path="model.html"><a href="model.html#intro-11"><i class="fa fa-check"></i><b>4.18.1</b> Intro</a></li>
<li class="chapter" data-level="4.18.2" data-path="model.html"><a href="model.html#assumptions-11"><i class="fa fa-check"></i><b>4.18.2</b> Assumptions</a></li>
<li class="chapter" data-level="4.18.3" data-path="model.html"><a href="model.html#characteristics-6"><i class="fa fa-check"></i><b>4.18.3</b> Characteristics</a></li>
<li class="chapter" data-level="4.18.4" data-path="model.html"><a href="model.html#evaluation-11"><i class="fa fa-check"></i><b>4.18.4</b> Evaluation</a></li>
<li class="chapter" data-level="4.18.5" data-path="model.html"><a href="model.html#proscons-11"><i class="fa fa-check"></i><b>4.18.5</b> Pros/Cons</a></li>
</ul></li>
<li class="chapter" data-level="4.19" data-path="model.html"><a href="model.html#svm"><i class="fa fa-check"></i><b>4.19</b> SVM</a><ul>
<li class="chapter" data-level="4.19.1" data-path="model.html"><a href="model.html#intro-12"><i class="fa fa-check"></i><b>4.19.1</b> Intro</a></li>
<li class="chapter" data-level="4.19.2" data-path="model.html"><a href="model.html#assumptions-12"><i class="fa fa-check"></i><b>4.19.2</b> Assumptions</a></li>
<li class="chapter" data-level="4.19.3" data-path="model.html"><a href="model.html#characteristics-7"><i class="fa fa-check"></i><b>4.19.3</b> Characteristics</a></li>
<li class="chapter" data-level="4.19.4" data-path="model.html"><a href="model.html#evaluation-12"><i class="fa fa-check"></i><b>4.19.4</b> Evaluation</a></li>
<li class="chapter" data-level="4.19.5" data-path="model.html"><a href="model.html#proscons-12"><i class="fa fa-check"></i><b>4.19.5</b> Pros/Cons</a></li>
</ul></li>
<li class="chapter" data-level="4.20" data-path="model.html"><a href="model.html#decision-tree"><i class="fa fa-check"></i><b>4.20</b> Decision Tree</a><ul>
<li class="chapter" data-level="4.20.1" data-path="model.html"><a href="model.html#intro-13"><i class="fa fa-check"></i><b>4.20.1</b> Intro</a></li>
<li class="chapter" data-level="4.20.2" data-path="model.html"><a href="model.html#assumptions-13"><i class="fa fa-check"></i><b>4.20.2</b> Assumptions</a></li>
<li class="chapter" data-level="4.20.3" data-path="model.html"><a href="model.html#characteristics-8"><i class="fa fa-check"></i><b>4.20.3</b> Characteristics</a></li>
<li class="chapter" data-level="4.20.4" data-path="model.html"><a href="model.html#evaluation-13"><i class="fa fa-check"></i><b>4.20.4</b> Evaluation</a></li>
<li class="chapter" data-level="4.20.5" data-path="model.html"><a href="model.html#proscons-13"><i class="fa fa-check"></i><b>4.20.5</b> Pros/Cons</a></li>
</ul></li>
<li class="chapter" data-level="4.21" data-path="model.html"><a href="model.html#bagging"><i class="fa fa-check"></i><b>4.21</b> Bagging</a><ul>
<li class="chapter" data-level="4.21.1" data-path="model.html"><a href="model.html#intro-14"><i class="fa fa-check"></i><b>4.21.1</b> Intro</a></li>
<li class="chapter" data-level="4.21.2" data-path="model.html"><a href="model.html#assumptions-14"><i class="fa fa-check"></i><b>4.21.2</b> Assumptions</a></li>
<li class="chapter" data-level="4.21.3" data-path="model.html"><a href="model.html#characteristics-9"><i class="fa fa-check"></i><b>4.21.3</b> Characteristics</a></li>
<li class="chapter" data-level="4.21.4" data-path="model.html"><a href="model.html#evaluation-14"><i class="fa fa-check"></i><b>4.21.4</b> Evaluation</a></li>
<li class="chapter" data-level="4.21.5" data-path="model.html"><a href="model.html#proscons-14"><i class="fa fa-check"></i><b>4.21.5</b> Pros/Cons</a></li>
</ul></li>
<li class="chapter" data-level="4.22" data-path="model.html"><a href="model.html#random-forest"><i class="fa fa-check"></i><b>4.22</b> Random Forest</a><ul>
<li class="chapter" data-level="4.22.1" data-path="model.html"><a href="model.html#intro-15"><i class="fa fa-check"></i><b>4.22.1</b> Intro</a></li>
<li class="chapter" data-level="4.22.2" data-path="model.html"><a href="model.html#assumptions-15"><i class="fa fa-check"></i><b>4.22.2</b> Assumptions</a></li>
<li class="chapter" data-level="4.22.3" data-path="model.html"><a href="model.html#characteristics-10"><i class="fa fa-check"></i><b>4.22.3</b> Characteristics</a></li>
<li class="chapter" data-level="4.22.4" data-path="model.html"><a href="model.html#evaluation-15"><i class="fa fa-check"></i><b>4.22.4</b> Evaluation</a></li>
<li class="chapter" data-level="4.22.5" data-path="model.html"><a href="model.html#proscons-15"><i class="fa fa-check"></i><b>4.22.5</b> Pros/Cons</a></li>
</ul></li>
<li class="chapter" data-level="4.23" data-path="model.html"><a href="model.html#boosting"><i class="fa fa-check"></i><b>4.23</b> Boosting</a><ul>
<li class="chapter" data-level="4.23.1" data-path="model.html"><a href="model.html#intro-16"><i class="fa fa-check"></i><b>4.23.1</b> Intro</a></li>
<li class="chapter" data-level="4.23.2" data-path="model.html"><a href="model.html#assumptions-16"><i class="fa fa-check"></i><b>4.23.2</b> Assumptions</a></li>
<li class="chapter" data-level="4.23.3" data-path="model.html"><a href="model.html#characteristics-11"><i class="fa fa-check"></i><b>4.23.3</b> Characteristics</a></li>
<li class="chapter" data-level="4.23.4" data-path="model.html"><a href="model.html#evaluation-16"><i class="fa fa-check"></i><b>4.23.4</b> Evaluation</a></li>
<li class="chapter" data-level="4.23.5" data-path="model.html"><a href="model.html#proscons-16"><i class="fa fa-check"></i><b>4.23.5</b> Pros/Cons</a></li>
</ul></li>
<li class="chapter" data-level="4.24" data-path="model.html"><a href="model.html#nlp"><i class="fa fa-check"></i><b>4.24</b> NLP</a></li>
<li class="chapter" data-level="4.25" data-path="model.html"><a href="model.html#topic-modeling"><i class="fa fa-check"></i><b>4.25</b> Topic Modeling</a></li>
<li class="chapter" data-level="4.26" data-path="model.html"><a href="model.html#recommendation-systems"><i class="fa fa-check"></i><b>4.26</b> Recommendation Systems</a></li>
<li class="chapter" data-level="4.27" data-path="model.html"><a href="model.html#time-series"><i class="fa fa-check"></i><b>4.27</b> Time Series</a><ul>
<li class="chapter" data-level="4.27.1" data-path="model.html"><a href="model.html#intro-17"><i class="fa fa-check"></i><b>4.27.1</b> Intro</a></li>
<li class="chapter" data-level="4.27.2" data-path="model.html"><a href="model.html#assumptions-17"><i class="fa fa-check"></i><b>4.27.2</b> Assumptions</a></li>
<li class="chapter" data-level="4.27.3" data-path="model.html"><a href="model.html#characteristics-12"><i class="fa fa-check"></i><b>4.27.3</b> Characteristics</a></li>
<li class="chapter" data-level="4.27.4" data-path="model.html"><a href="model.html#evaluation-17"><i class="fa fa-check"></i><b>4.27.4</b> Evaluation</a></li>
<li class="chapter" data-level="4.27.5" data-path="model.html"><a href="model.html#proscons-17"><i class="fa fa-check"></i><b>4.27.5</b> Pros/Cons</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="udacity-ts-fundamentals.html"><a href="udacity-ts-fundamentals.html"><i class="fa fa-check"></i><b>5</b> Udacity: TS Fundamentals</a><ul>
<li class="chapter" data-level="5.1" data-path="udacity-ts-fundamentals.html"><a href="udacity-ts-fundamentals.html#dl"><i class="fa fa-check"></i><b>5.1</b> DL</a></li>
<li class="chapter" data-level="5.2" data-path="udacity-ts-fundamentals.html"><a href="udacity-ts-fundamentals.html#tensorflow"><i class="fa fa-check"></i><b>5.2</b> TensorFlow</a></li>
<li class="chapter" data-level="5.3" data-path="udacity-ts-fundamentals.html"><a href="udacity-ts-fundamentals.html#keras"><i class="fa fa-check"></i><b>5.3</b> Keras</a><ul>
<li class="chapter" data-level="5.3.1" data-path="udacity-ts-fundamentals.html"><a href="udacity-ts-fundamentals.html#regression"><i class="fa fa-check"></i><b>5.3.1</b> Regression</a></li>
<li class="chapter" data-level="5.3.2" data-path="udacity-ts-fundamentals.html"><a href="udacity-ts-fundamentals.html#classification"><i class="fa fa-check"></i><b>5.3.2</b> Classification</a></li>
<li class="chapter" data-level="5.3.3" data-path="udacity-ts-fundamentals.html"><a href="udacity-ts-fundamentals.html#another-example"><i class="fa fa-check"></i><b>5.3.3</b> Another Example</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="udacity-ts-fundamentals.html"><a href="udacity-ts-fundamentals.html#association-rule-mining"><i class="fa fa-check"></i><b>5.4</b> Association Rule Mining</a><ul>
<li class="chapter" data-level="5.4.1" data-path="udacity-ts-fundamentals.html"><a href="udacity-ts-fundamentals.html#intro-18"><i class="fa fa-check"></i><b>5.4.1</b> Intro</a></li>
<li class="chapter" data-level="5.4.2" data-path="udacity-ts-fundamentals.html"><a href="udacity-ts-fundamentals.html#assumptions-18"><i class="fa fa-check"></i><b>5.4.2</b> Assumptions</a></li>
<li class="chapter" data-level="5.4.3" data-path="udacity-ts-fundamentals.html"><a href="udacity-ts-fundamentals.html#characteristics-13"><i class="fa fa-check"></i><b>5.4.3</b> Characteristics</a></li>
<li class="chapter" data-level="5.4.4" data-path="udacity-ts-fundamentals.html"><a href="udacity-ts-fundamentals.html#evaluation-18"><i class="fa fa-check"></i><b>5.4.4</b> Evaluation</a></li>
<li class="chapter" data-level="5.4.5" data-path="udacity-ts-fundamentals.html"><a href="udacity-ts-fundamentals.html#proscons-18"><i class="fa fa-check"></i><b>5.4.5</b> Pros/Cons</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="udacity-ts-fundamentals.html"><a href="udacity-ts-fundamentals.html#model-selection"><i class="fa fa-check"></i><b>5.5</b> Model Selection</a></li>
<li class="chapter" data-level="5.6" data-path="udacity-ts-fundamentals.html"><a href="udacity-ts-fundamentals.html#other-2"><i class="fa fa-check"></i><b>5.6</b> Other</a><ul>
<li class="chapter" data-level="5.6.1" data-path="udacity-ts-fundamentals.html"><a href="udacity-ts-fundamentals.html#general-2"><i class="fa fa-check"></i><b>5.6.1</b> General</a></li>
<li class="chapter" data-level="5.6.2" data-path="udacity-ts-fundamentals.html"><a href="udacity-ts-fundamentals.html#unbalanced-classes"><i class="fa fa-check"></i><b>5.6.2</b> Unbalanced Classes</a></li>
<li class="chapter" data-level="5.6.3" data-path="udacity-ts-fundamentals.html"><a href="udacity-ts-fundamentals.html#longitudinal-data"><i class="fa fa-check"></i><b>5.6.3</b> Longitudinal Data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="cdm.html"><a href="cdm.html"><i class="fa fa-check"></i><b>6</b> CDM</a><ul>
<li class="chapter" data-level="6.1" data-path="cdm.html"><a href="cdm.html#shiny"><i class="fa fa-check"></i><b>6.1</b> Shiny</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="frameworks.html"><a href="frameworks.html"><i class="fa fa-check"></i>Frameworks</a><ul>
<li class="chapter" data-level="6.2" data-path="frameworks.html"><a href="frameworks.html#pyspark"><i class="fa fa-check"></i><b>6.2</b> PySpark</a></li>
<li class="chapter" data-level="6.3" data-path="frameworks.html"><a href="frameworks.html#sparklyr"><i class="fa fa-check"></i><b>6.3</b> Sparklyr</a></li>
<li class="chapter" data-level="6.4" data-path="frameworks.html"><a href="frameworks.html#caret"><i class="fa fa-check"></i><b>6.4</b> Caret</a></li>
<li class="chapter" data-level="6.5" data-path="frameworks.html"><a href="frameworks.html#preprocessing"><i class="fa fa-check"></i><b>6.5</b> Preprocessing</a></li>
<li class="chapter" data-level="6.6" data-path="frameworks.html"><a href="frameworks.html#lm"><i class="fa fa-check"></i><b>6.6</b> LM</a></li>
<li class="chapter" data-level="6.7" data-path="frameworks.html"><a href="frameworks.html#lassoridge"><i class="fa fa-check"></i><b>6.7</b> Lasso/Ridge</a></li>
<li class="chapter" data-level="6.8" data-path="frameworks.html"><a href="frameworks.html#logreg"><i class="fa fa-check"></i><b>6.8</b> LogReg</a></li>
<li class="chapter" data-level="6.9" data-path="frameworks.html"><a href="frameworks.html#train-test-split-folds-cv"><i class="fa fa-check"></i><b>6.9</b> Train-Test Split, Folds, &amp; CV</a></li>
<li class="chapter" data-level="6.10" data-path="frameworks.html"><a href="frameworks.html#random-forest-1"><i class="fa fa-check"></i><b>6.10</b> Random Forest</a></li>
<li class="chapter" data-level="6.11" data-path="frameworks.html"><a href="frameworks.html#svm-1"><i class="fa fa-check"></i><b>6.11</b> SVM</a></li>
<li class="chapter" data-level="6.12" data-path="frameworks.html"><a href="frameworks.html#model-selection-1"><i class="fa fa-check"></i><b>6.12</b> Model Selection</a></li>
<li class="chapter" data-level="6.13" data-path="frameworks.html"><a href="frameworks.html#stacking"><i class="fa fa-check"></i><b>6.13</b> Stacking</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="code-snippets.html"><a href="code-snippets.html"><i class="fa fa-check"></i>Code Snippets</a><ul>
<li class="chapter" data-level="6.14" data-path="code-snippets.html"><a href="code-snippets.html#python"><i class="fa fa-check"></i><b>6.14</b> Python</a><ul>
<li class="chapter" data-level="6.14.1" data-path="code-snippets.html"><a href="code-snippets.html#check-csv-encoding"><i class="fa fa-check"></i><b>6.14.1</b> Check CSV Encoding</a></li>
<li class="chapter" data-level="6.14.2" data-path="code-snippets.html"><a href="code-snippets.html#mnist-example"><i class="fa fa-check"></i><b>6.14.2</b> MNIST Example</a></li>
</ul></li>
<li class="chapter" data-level="6.15" data-path="code-snippets.html"><a href="code-snippets.html#r"><i class="fa fa-check"></i><b>6.15</b> R</a></li>
<li class="chapter" data-level="6.16" data-path="code-snippets.html"><a href="code-snippets.html#shiny-reactive"><i class="fa fa-check"></i><b>6.16</b> Shiny Reactive</a></li>
<li class="chapter" data-level="6.17" data-path="code-snippets.html"><a href="code-snippets.html#custom-theme"><i class="fa fa-check"></i><b>6.17</b> Custom Theme</a></li>
<li class="chapter" data-level="6.18" data-path="code-snippets.html"><a href="code-snippets.html#create-factor"><i class="fa fa-check"></i><b>6.18</b> Create Factor</a><ul>
<li class="chapter" data-level="6.18.1" data-path="code-snippets.html"><a href="code-snippets.html#extract-date"><i class="fa fa-check"></i><b>6.18.1</b> Extract Date</a></li>
<li class="chapter" data-level="6.18.2" data-path="code-snippets.html"><a href="code-snippets.html#geom_path"><i class="fa fa-check"></i><b>6.18.2</b> Geom_Path</a></li>
<li class="chapter" data-level="6.18.3" data-path="code-snippets.html"><a href="code-snippets.html#theme-template"><i class="fa fa-check"></i><b>6.18.3</b> Theme Template</a></li>
<li class="chapter" data-level="6.18.4" data-path="code-snippets.html"><a href="code-snippets.html#heatmap"><i class="fa fa-check"></i><b>6.18.4</b> Heatmap</a></li>
<li class="chapter" data-level="6.18.5" data-path="code-snippets.html"><a href="code-snippets.html#calculate-accuracy"><i class="fa fa-check"></i><b>6.18.5</b> Calculate Accuracy</a></li>
<li class="chapter" data-level="6.18.6" data-path="code-snippets.html"><a href="code-snippets.html#replace-na"><i class="fa fa-check"></i><b>6.18.6</b> Replace NA</a></li>
<li class="chapter" data-level="6.18.7" data-path="code-snippets.html"><a href="code-snippets.html#filter-by-row"><i class="fa fa-check"></i><b>6.18.7</b> Filter By Row</a></li>
<li class="chapter" data-level="6.18.8" data-path="code-snippets.html"><a href="code-snippets.html#db-functions"><i class="fa fa-check"></i><b>6.18.8</b> DB Functions</a></li>
</ul></li>
<li class="chapter" data-level="6.19" data-path="code-snippets.html"><a href="code-snippets.html#knitr"><i class="fa fa-check"></i><b>6.19</b> Knitr</a></li>
<li class="chapter" data-level="6.20" data-path="code-snippets.html"><a href="code-snippets.html#bash"><i class="fa fa-check"></i><b>6.20</b> Bash</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="general-3.html"><a href="general-3.html"><i class="fa fa-check"></i>General</a><ul>
<li class="chapter" data-level="6.21" data-path="general-3.html"><a href="general-3.html#python-1"><i class="fa fa-check"></i><b>6.21</b> Python</a></li>
<li class="chapter" data-level="6.22" data-path="general-3.html"><a href="general-3.html#r-1"><i class="fa fa-check"></i><b>6.22</b> R</a></li>
<li class="chapter" data-level="6.23" data-path="general-3.html"><a href="general-3.html#git"><i class="fa fa-check"></i><b>6.23</b> Git</a></li>
<li class="chapter" data-level="6.24" data-path="general-3.html"><a href="general-3.html#other-3"><i class="fa fa-check"></i><b>6.24</b> Other</a></li>
<li class="chapter" data-level="6.25" data-path="general-3.html"><a href="general-3.html#yaml"><i class="fa fa-check"></i><b>6.25</b> YAML</a></li>
<li class="chapter" data-level="6.26" data-path="general-3.html"><a href="general-3.html#ab-testing-overview"><i class="fa fa-check"></i><b>6.26</b> AB Testing Overview</a></li>
<li class="chapter" data-level="6.27" data-path="general-3.html"><a href="general-3.html#designing-an-experiment"><i class="fa fa-check"></i><b>6.27</b> Designing An Experiment</a></li>
<li class="chapter" data-level="6.28" data-path="general-3.html"><a href="general-3.html#networks"><i class="fa fa-check"></i><b>6.28</b> Networks</a></li>
<li class="chapter" data-level="6.29" data-path="general-3.html"><a href="general-3.html#networkx"><i class="fa fa-check"></i><b>6.29</b> Networkx</a></li>
<li class="chapter" data-level="6.30" data-path="general-3.html"><a href="general-3.html#stats"><i class="fa fa-check"></i><b>6.30</b> Stats</a><ul>
<li class="chapter" data-level="6.30.1" data-path="general-3.html"><a href="general-3.html#infer-package"><i class="fa fa-check"></i><b>6.30.1</b> Infer Package</a></li>
<li class="chapter" data-level="6.30.2" data-path="general-3.html"><a href="general-3.html#statistical-tests"><i class="fa fa-check"></i><b>6.30.2</b> Statistical Tests</a></li>
<li class="chapter" data-level="6.30.3" data-path="general-3.html"><a href="general-3.html#stats-for-hackers"><i class="fa fa-check"></i><b>6.30.3</b> Stats For Hackers</a></li>
<li class="chapter" data-level="6.30.4" data-path="general-3.html"><a href="general-3.html#think-stats"><i class="fa fa-check"></i><b>6.30.4</b> Think Stats</a></li>
</ul></li>
<li class="chapter" data-level="6.31" data-path="general-3.html"><a href="general-3.html#other-4"><i class="fa fa-check"></i><b>6.31</b> Other</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Science Cribsheeet</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="udacity-ts-fundamentals" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Udacity: TS Fundamentals</h1>
<p>Naive vs Seasonal Naive vs Exponential Smooothing</p>
<p>Seasonal Naive: Assumes magnitude of the seasonal pattern is constant.</p>
<p>cyclical vs seasonal patterns: longer, harder to predicit, don’t follow seasonl patterns</p>
<p>ETS: Error-Trend-Seasonality</p>
<p>The possible time series (TS) scenarios can be recognized by asking the following questions:</p>
<p>TS has a trend? If yes, is the trend increasing linearly or exponentially?</p>
<p>TS has seasonality? If yes, do the seasonal components increase in magnitude over time?</p>
<p>We are going to explore four ETS models that can help forecast these possible time-series scenarios.</p>
<p>Simple Exponential Smoothing Method</p>
<p>Holt’s Linear Trend Method</p>
<p>Exponential Trend Method</p>
<p>Holt-Winters Seasonal Method</p>
<p>Time series does not have a trend line and does not have seasonality component. We would use a Simple Exponential Smoothing model.</p>
<p>Time Seies: level, trend, seasonal compoent</p>
<p>Methods</p>
<p>There are several methods we need to pick in order to model any given time series appropriately:</p>
<p>Simple Exponential Smoothing: Finds the level of the time series</p>
<p>Holt’s Linear Trend: Finds the level of the time series</p>
<p>Additive model for linear trend</p>
<p>Exponential Trend: Finds the level of the time series</p>
<p>Multiplicative model for exponential trend</p>
<p>Holt-Winters Seasonal: Finds the level of the time series</p>
<p>Additive for trend</p>
<p>Multiplicative and Additive for seasonal components</p>
<p>These methods help deal with different scenarios in our time series involving:</p>
<p>Linear or exponential trend</p>
<p>Constant or increasing seasonality components</p>
<p>For trends that are exponential, we would need to use a multiplicative model.</p>
<p>For increasing seasonality components, we would need to use a multiplicative model model as well.</p>
<p>ETS</p>
<p>Therefore we can generalize all of these models using a naming system for ETS:</p>
<p>ETS (Error, Trend, Seasonality)</p>
<p>Error is the error line we saw in the time series decomposition part earlier in the course. If the error is increasing similar to an increasing seasonal components, we would need to consider a multiplicative design for the exponential model.</p>
<p>Therefore, for each component in the ETS system, we can assign None, Multiplicative, or Additive (or N, M, A) for each of the three components in our time series.</p>
<p>Examples</p>
<p>A time series model that has a constant error, linear trend, and increasing seasonal components means we would need to use an ETS model of:</p>
<p>ETS(N,A,M)</p>
<p>A time series model that has increasing error, exponential trend, and no seasonality means we would need to use an ETS model of:</p>
<p>ARIMA; Seasnoanl:ARIMA(p,d,q)(P,D,Q)M vs non-seasonal: ARIMA(p,d,q)</p>
<p>non-=statipjary: This plot shows an upward trend and seasonality.</p>
<p>stationaey: This plot revolves around a constant mean of 0 and shows contained variance.</p>
<p>Evluated withhold set and residual plots, should have mean of 0.</p>
<p>is less sensitive to the occasional very large error because it does not square the errors in the calculation.</p>
<p>Percentage Errors</p>
<p>Percentage errors, like MAPE, are useful because they are scale independent, so they can be used to compare forecasts between different data series, unlike scale dependent errors. The disadvantage is that it cannot be used if the series has zero values.</p>
<p>Mean Absolute Percentage Error (MAPE) is also often useful for purposes of reporting, because it is expressed in generic percentage terms it will make sense even to someone who has no idea what constitutes a “big” error in terms of dollars spent or widgets sold.</p>
<p>Scale-Free Errors</p>
<p>Scale-free errors were introduced more recently to offer a scale-independent measure that doesn’t have many of the problems of other errors like percentage errors.</p>
<p>Mean Absolute Scaled Error (MASE) is another relative measure of error that is applicable only to time series data. It is defined as the mean absolute error of the model divided by the the mean absolute value of the first difference of the series. Thus, it measures the relative reduction in error compared to a naive model. Ideally its value will be significantly less than 1 but is relative to comparison across other models for the same series. Since this error measurement is relative and can be applied across models, it is accepted as one of the best metrics for error measurement.</p>
<p>Can use AIC for model selection. Also use confidence intervals.</p>
<div id="dl" class="section level2">
<h2><span class="header-section-number">5.1</span> DL</h2>
<ul>
<li><p><a href="https://hackernoon.com/deep-learning-cheat-sheet-25421411e460">DL Cheatsheet</a></p></li>
<li><p>By reducing learning rate (for example, to 0.001), Test loss drops to a value much closer to Training loss. In most runs, increasing Batch size does not influence Training loss or Test loss significantly. However, in a small percentage of runs, increasing Batch size to 20 or greater causes Test loss to drop slightly below Training loss.</p></li>
<li><p>Reducing the ratio of training to test data from 50% to 10% dramatically lowers the number of data points in the training set. With so little data, high batch size and high learning rate cause the training model to jump around chaotically (jumping repeatedly over the minimum point).</p></li>
<li><p>There are several activation functions you may encounter in practice:</p>
<ul>
<li><p>Sigmoid: Takes a real-valued input and squashes it to range between 0 and 1 (<span class="math inline">\(σ(x) = 1 / (1 + exp(−x))\)</span>)</p></li>
<li><p>Softmax: Same end result as sigmoid, but different function.</p></li>
<li><p>tanh: Takes a real-valued input and squashes it to the range [-1, 1] (<span class="math inline">\(tanh(x) = 2σ(2x) − 1\)</span>)</p></li>
</ul></li>
<li><p>ReLU: The rectified linear activation function (called ReLU) has been shown to lead to very high-performance networks. This function takes a single number as an input, returning 0 if the input is negative, and the input if the input is positive. <span class="math inline">\(f(x) = max(0, x)\)</span></p></li>
<li><p>softmax doesn’t like multi-label classification</p></li>
<li><p>Embeddings vs One-Hot Encoding: Embeddings are better than One-Hot Encodings because it allows for relationships to be shown between days.</p></li>
<li><p>Advised to scale features</p></li>
<li><p><a href="https://www.youtube.com/watch?v=nqEYVzJLR_c&amp;feature=youtu.be&amp;t=31">Rule of 30</a>: A change that affects at least 30 data points in your validation set is usually significant and not just noise.</p></li>
<li><p>Gradient descent takes about 3 times longer than the loss function. Stochastic gradient descent works off an estimate of the loss function from a sample of the training set. Scales better than normal gradient descent.</p></li>
<li><p>Momentum and learning rate decay are good for knowing which way to go in gradient descent.</p></li>
<li><p>Always try to lower your learning rate for improvement.</p></li>
<li><p>ADAGRAD is another optimization option that takes care of some of the hyperparameters for you.</p></li>
<li><p>n inputs and k outputs gives you <span class="math inline">\((n+1)*k\)</span> parameters.</p></li>
<li><p>Back propogation takes about twice the memory as forward propogation.</p></li>
<li><p>Stop model from overoptimizing on training set: early termination (stop as soon as performance on validation set drops) and regularization which applies artifical constraints to reduce the number of free parameters while making it difficult to optimize. Uses l2 regularization or dropout. Dropout works by randomly setting activations from one layer to the next to 0 repeatly. Forces the network to learn redundant representations, and takes average consensus for final prediction. Makes things more robust and prevents overfitting. Scale the non-zero activates by 2 to get the right average. If it doesn’t work, go deeper.</p></li>
<li><p>Two ways are to average the yellow, blue, and green channels or to use YUV representation. If position on the screen doesn’t matter then use translation invariance. Use weight sharing if this occurs in text.</p></li>
<li><p>Things that don’t chnage across time, space, etc are called statistical invariants.</p></li>
<li><p>RNN’s use shared parameters over time to extract patterns. Uses a recurrent connection to provide a summary of the past and pass this info to the classifier.</p></li>
<li><p>Backpropagation occurs throw time to the beginning. All derivative applied to same parameters, so very correlated. Bad for stochastic gradient descent and makes math very unstable. Gradients are either zero (doesn’t remember the past well) or infinity. The latter is fixed by gradient clipping and the former by LSTM (long-short term memory). LSTM replaced the rectified linear units by continuous functions. You can use dropout or L2 regularization with an LSTM.</p></li>
<li><p>Generally dealing with images, Convolutional Neural Network is used mostly because of its better accuracy results.</p></li>
<li><p>Model capacity: Same as underfitting and overfitting in bias-variance. Less nodes and hidden layers corresponds to simpler model and vice versa.</p></li>
<li><p>The perceptron is the simplest neural network. The perceptron is an iterative classification method.The perceptron starts with a random hyperplane then adjust its weights to separate the data.</p></li>
<li><p>BackProp Algorithm: Initially all the edge weights are randomly assigned. For every input in the training dataset, the ANN is activated and its output is observed. This output is compared with the desired output that we already know, and the error is “propagated” back to the previous layer. This error is noted and the weights are “adjusted” accordingly. This process is repeated until the output error is below a predetermined threshold.</p></li>
<li><p>The last layer of a neural network captures the most complex interactions.</p></li>
<li><p>When plotting the mean-squared error loss function against predictions, the slope is <span class="math inline">\(2x(y-xb)\)</span>, or <span class="math inline">\(2input_data error\)</span>.</p></li>
<li><p>weights_updated = weights - slope*learning_rate</p></li>
<li><p>This is exactly what’s happening in the vanishing gradient problem – the gradients of the network’s output with respect to the parameters in the early layers become extremely small. That’s a fancy way of saying that even a large change in the value of parameters for the early layers doesn’t have a big effect on the output.</p></li>
<li><p>Batch: Subset of the data used to calculate slopes during back propagation. Different batches are used to calculate different updates.</p></li>
<li><p>Epoch: One full pass through all the batches in the training data.</p></li>
<li><p>Stochastic gradient descent calculates slopes one batch at a time.</p></li>
<li><p>This network again uses the ReLU activation function, so the slope of the activation function is 1 for any node receiving a positive value as input.</p></li>
<li><p>Few people use kfold cv in deep learning because of the large datasets in play.</p></li>
<li><p>Dying neuron + vanishing gradient: Change activation function</p></li>
</ul>
</div>
<div id="tensorflow" class="section level2">
<h2><span class="header-section-number">5.2</span> TensorFlow</h2>
<ul>
<li><p>In TensorFlow, we indicate a feature’s data type using a construct called a feature column. Feature columns store only a description of the feature data; they do not contain the feature data itself.</p></li>
<li><p>Amazingly enough, performing gradient descent on a small batch or even a batch of one example is usually more efficient than the full batch. After all, finding the gradient of one example is far cheaper than finding the gradient of millions of examples. To ensure a good representative sample, the algorithm scoops up another random small batch (or batch of one) on every iteration.</p></li>
<li><p>SGD &amp; Mini-Batch Gradient Descent</p></li>
</ul>
<p>Could compute gradient over entire data set on each step, but this turns out to be unnecessary. Computing gradient on small data samples works well. On every step, get a new random sample</p>
<p>Stochastic Gradient Descent: one example at a time</p>
<p>Mini-Batch Gradient Descent: batches of 10-1000. Loss &amp; gradients are averaged over the batch</p>
</div>
<div id="keras" class="section level2">
<h2><span class="header-section-number">5.3</span> Keras</h2>
<p>Here’s a <a href="https://github.com/fastforwardlabs/keras-hello-world/blob/master/kerashelloworld.ipynb">Hello World</a> and the general framework:</p>
<ul>
<li><p>declare: sequential</p></li>
<li><p>add layers input-hidden-output</p></li>
<li><p>compile</p></li>
<li><p>fit</p></li>
</ul>
<p>Keras only uses numpy arrays.</p>
<ul>
<li>keras.callbacks.EarlyStopping: Stop training when validation score stop improving after a certain number of epochs (baches?)</li>
</ul>
<div id="regression" class="section level3">
<h3><span class="header-section-number">5.3.1</span> Regression</h3>
<ul>
<li><p>loss = mean_squared_error</p></li>
<li><p>metric: rmse</p></li>
<li><p>activation_function = relu</p></li>
</ul>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> keras
<span class="im">from</span> keras.layers <span class="im">import</span> Dense
<span class="im">from</span> keras.models <span class="im">import</span> Sequential
<span class="co"># Specify the model: Two hidden layers</span>
model <span class="op">=</span> Sequential()
<span class="co">## Input</span>
n_cols <span class="op">=</span> predictors.shape[<span class="dv">1</span>]
model.add(Dense(<span class="dv">50</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, input_shape <span class="op">=</span> (n_cols,)))
<span class="co"># Hidden</span>
model.add(Dense(<span class="dv">32</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>))
<span class="co"># Output</span>
model.add(Dense(<span class="dv">1</span>))
<span class="co"># Compile the model</span>
model.<span class="bu">compile</span>(optimizer <span class="op">=</span> <span class="st">&#39;adam&#39;</span>, loss <span class="op">=</span> <span class="st">&#39;mean_squared_error&#39;</span>) 
<span class="co"># Fit the model</span>
model.fit(predictors, target)
<span class="co">#Look at summary</span>
model.summary()
<span class="co"># Calculate predictions: predictions</span>
predictions <span class="op">=</span> model.predict(pred_data)</code></pre></div>
</div>
<div id="classification" class="section level3">
<h3><span class="header-section-number">5.3.2</span> Classification</h3>
<ul>
<li><p>loss = categorical_crossentropy</p></li>
<li><p>metric = accuracy</p></li>
<li><p>activation_function = softmax</p></li>
<li><p>output layer with stuff equal to number of categorical groups</p></li>
</ul>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> keras
<span class="im">from</span> keras.layers <span class="im">import</span> Dense
<span class="im">from</span> keras.models <span class="im">import</span> Sequential
<span class="co"># Specify the model: Two hidden layers</span>
n_cols <span class="op">=</span> predictors.shape[<span class="dv">1</span>]
model <span class="op">=</span> Sequential()
<span class="co">## Input</span>
model.add(Dense(<span class="dv">50</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, input_shape <span class="op">=</span> (n_cols,)))
<span class="co"># Hidden</span>
model.add(Dense(<span class="dv">32</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>))
<span class="co"># Output</span>
model.add(Dense(<span class="dv">2</span>, activation <span class="op">=</span> <span class="st">&#39;softmax&#39;</span>))
<span class="co"># Compile the model</span>
model.<span class="bu">compile</span>(optimizer <span class="op">=</span> <span class="st">&#39;sgd&#39;</span>, loss <span class="op">=</span> <span class="st">&#39;categorical_crossentropy&#39;</span>, metrics <span class="op">=</span> [<span class="st">&#39;accuracy&#39;</span>])
<span class="co"># Fit the model</span>
model.fit(predictors, target)
<span class="co">#Look at summary</span>
model.summary()
<span class="co">#Calculate predictions: predictions</span>
predictions <span class="op">=</span> model.predict(pred_data)
<span class="co">#Calculate predicted probability of survival</span>
predicted_prob_true <span class="op">=</span> predictions[:, <span class="dv">1</span>]</code></pre></div>
</div>
<div id="another-example" class="section level3">
<h3><span class="header-section-number">5.3.3</span> Another Example</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># Import the SGD optimizer</span>
<span class="im">from</span> keras.optimizers <span class="im">import</span> SGD
<span class="co"># Create list of learning rates: lr_to_test</span>
lr_to_test <span class="op">=</span> [.<span class="dv">000001</span>, <span class="fl">0.01</span>, <span class="dv">1</span>]
<span class="co"># Loop over learning rates</span>
<span class="cf">for</span> lr <span class="kw">in</span> lr_to_test:
    <span class="bu">print</span>(<span class="st">&#39;Testing model with learning rate: &#39;</span>)
    
    <span class="co"># Build new model to test, unaffected by previous models</span>
    model <span class="op">=</span> get_new_model()
    
    <span class="co"># Create SGD optimizer with specified learning rate: my_optimizer</span>
    my_optimizer <span class="op">=</span> SGD(lr <span class="op">=</span> lr)
    
    <span class="co"># Compile the model</span>
    model.<span class="bu">compile</span>(optimizer <span class="op">=</span> my_optimizer, loss <span class="op">=</span> <span class="st">&#39;categorical_crossentropy&#39;</span>)
    
    <span class="co"># Fit the model</span>
    model.fit(predictors, 
              target, 
              validation_split <span class="op">=</span> <span class="fl">0.3</span>, 
              epochs <span class="op">=</span> <span class="dv">20</span>, 
              callbacks <span class="op">=</span> [early_stopping_monitor], 
              verbose <span class="op">=</span> <span class="va">False</span>)</code></pre></div>
</div>
</div>
<div id="association-rule-mining" class="section level2">
<h2><span class="header-section-number">5.4</span> Association Rule Mining</h2>
<div id="intro-18" class="section level3">
<h3><span class="header-section-number">5.4.1</span> Intro</h3>
<ul>
<li><p>Used in Genetics, Fraud, and Market Basket Analysis, etc. A typical rule might be: if someone buys peanut butter and jelly, then that person is likely to buy bread as well.</p></li>
<li><p>Incredibly big feature space (2^k-1).</p></li>
<li><p>The Apriori algorithm employs a simple a priori belief as a guideline for reducing the association rule space.</p></li>
<li><p>Support: the fraction of which each item appears within the dataset as a whole. Support(item) = count(item)/N. Higher support is better.</p></li>
<li><p>nfidence: the likelihood that a constructed rule is correct given the items on the left hand side of the transaction. A higher level of confidence implies a higher likelihood that Y appears alongside transactions in which X appears.</p></li>
<li><p>ft: the ratio by which the confidence of a rule exceeds the expected outcome. When lift &gt; 1, the presence of X seems to have increasedthe probability of Y occurring in the transaction. When lift &lt; 1, the presence of X seems to have decreasedthe probability of Y occurring in the transaction. When lift = 1, X and Y are independent.</p></li>
<li><p>t thresholds for support and confidence and then the algorithm goes from all 1-combinations to 2-combinations and up. Those subset below the threshold don’t make it to the higher iterations.</p></li>
</ul>
</div>
<div id="assumptions-18" class="section level3">
<h3><span class="header-section-number">5.4.2</span> Assumptions</h3>
</div>
<div id="characteristics-13" class="section level3">
<h3><span class="header-section-number">5.4.3</span> Characteristics</h3>
</div>
<div id="evaluation-18" class="section level3">
<h3><span class="header-section-number">5.4.4</span> Evaluation</h3>
</div>
<div id="proscons-18" class="section level3">
<h3><span class="header-section-number">5.4.5</span> Pros/Cons</h3>
<p><strong>Pros</strong></p>
<ul>
<li><p>Ideally suited for working with very large amounts of transactional data.</p></li>
<li><p>The results are rules that are generally easy to understand and have a high amount of interpretability.</p></li>
<li><p>The process is useful for data mining and uncovering unexpected knowledge within a dataset.</p></li>
</ul>
<p><strong>Cons</strong></p>
<ul>
<li><p>The outcome is usually not interesting when applied to smaller datasets.</p></li>
<li><p>It is difficult to separate actual insights from common sense notions.</p></li>
<li><p>The analyst might be compelled to draw spurious conclusions–remember that correlation doesn’t imply causation!</p></li>
</ul>
</div>
</div>
<div id="model-selection" class="section level2">
<h2><span class="header-section-number">5.5</span> Model Selection</h2>
<ul>
<li><p><a href="http://hunch.net/?p=224" class="uri">http://hunch.net/?p=224</a></p></li>
<li><p>Linear SVM often outperforms logistic regression and is explainable as well.</p></li>
<li><p>However, the problem with cross-validation is that it is rarely applicable to real world problems, for all the reasons described in the above sections. Cross-validation only works in the same cases where you can randomly shuffle your data to choose a validation set.</p></li>
<li><p>If the performance of a classification model on the test set (out-of-sample) error is poor, you can’t just re-calibrate your model parameters to achieve a better model. This is cheating.</p></li>
<li><p>When the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suffer from this problem. If n is small and the distribution of the predictors X is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model.</p></li>
<li><p>Roughly speaking, LDA tends to be a better bet than QDA if there are relatively few training observations and so reducing variance is crucial. In contrast, QDA is recommended if the training set is very large, so that the variance of the classifier is not a major concern, or if the assumption of a common covariance matrix for the K classes is clearly untenable.</p></li>
<li><p>Two features interact if the effect on the prediction of one feature depends on the value of the other feature</p></li>
<li><p>“No Free Lunch” theorem: no single classifier works best across all possible scenarios</p></li>
<li><p>Warning PolynomialFeatures(degree=d) transforms an array containing n features into an array containing features, where n! is the factorial of n, equal to 1 × 2 × 3 × ⋯ × n. Beware of the combinatorial explosion of the number of features!</p></li>
<li><p>If a model suffers from overfitting, we also say that the model has a high variance, which can be caused by having too many parameters that lead to a model that is too complex given the underlying data. Similarly, our model can also suffer from underfitting (high bias), which means that our model is not complex enough to capture the pattern in the training data well and therefore also suffers from low performance on unseen data.</p></li>
<li><p>It appears that each feature has modestly improved our model. There are certainly more features that we could add to our model. For example, we could add the day of the week and the hour of the posting, we could determine if the article is a listicle by running a regex on the headline, or we could examine the sentiment of each article. This only begins to touch on the features that could be important to model virality. We would certainly need to go much further to continue reducing the error in our model.</p></li>
<li><p>Suppose Fixitol reduces symptoms by 20% over a placebo, but the trial you’re using to test it is too small to have adequate statistical power to detect this difference reliably. We know that small trials tend to have varying results; it’s easy to get 10 lucky patients who have shorter colds than usual but much harder to get 10,000 who all do.</p></li>
<li><p>As I mentioned earlier, one drawback of the Bonferroni correction is that it greatly decreases the statistical power of your experiments, making it more likely that you’ll miss true effects. More sophisticated procedures than Bonferroni correction exist, ones with less of an impact on statistical power, but even these are not magic bullets. Worse, they don’t spare you from the base rate fallacy. You can still be misled by your p threshold and falsely claim there’s “only a 5% chance I’m wrong.” Procedures like the Bonferroni correction only help you eliminate some false positives.</p></li>
<li><p>In step 1, we find where missing values are located. The md.pattern() function from the mice package is a really useful function. It gives you a clear view of where missing values are located, helping you in decisions regarding exclusions or substitution. You can refer to the next recipe for missing value substitution.</p></li>
<li><p>“Product classification is an example of multicategory or multinomial classification. Most classification problems and most classification algorithms are specialized for two-category, or binomial, classification. There are tricks to using binary classifiers to solve multicategory problems (for example, building one classifier for each category, called a “one versus rest” classifier). But in most cases it’s worth the effort to find a suitable multiple-category implementation, as they tend to work better than multiple binary classifiers (for example, using the package mlogit instead of the base method glm() for logistic regression).”</p></li>
<li><p>Another possible indication of collinearity in the inputs is seeing coefficients with an unexpected sign: for example, seeing that income is negatively correlated with years in the workforce.</p></li>
<li><p>The overall model can still predict income quite well, even when the inputs are correlated; it just can’t determine which variable deserves the credit for the prediction. Using regularization (especially ridge regression as found in lm.ridge() in the package MASS) is helpful in collinear situations (we prefer it to “x-alone” variable preprocessing, such as principal components analysis). If you want to use the coefficient values as advice as well as to make good predictions.</p></li>
<li><p>The degrees of freedom is thought of as the number of training data rows you have after correcting for the number of coefficients you tried to solve for. You want the degrees of freedom to be large compared to the number of coefficients fit to avoid overfitting.</p></li>
<li><p>Overfitting is when you find chance relations in your training data that aren’t present in the general population. Overfitting is bad: you think you have a good model when you don’t.</p></li>
<li><p>The probable reason for nonconvergence is separation or quasi-separation: one of the model variables or some combination of the model variables predicts the outcome perfectly for at least a subset of the training data. You’d think this would be a good thing, but ironically logistic regression fails when the variables are too powerful.</p></li>
<li><p>For categorical variables (male/female, or small/medium/large), you can define the distance as 0 if two points are in the same category, and 1 otherwise. If all the variables are categorical, then you can use Hamming distance, which counts the number of mismatches.</p></li>
<li><p>Your next step is to select a performance measure. A typical performance measure for regression problems is the Root Mean Square Error (RMSE). It measures the standard deviation of the errors the system makes in its predictions.</p></li>
<li><p>You should save every model you experiment with, so you can come back easily to any model you want. Make sure you save both the hyperparameters and the trained parameters, as well as the cross-validation scores and perhaps the actual predictions as well. This will allow you to easily compare scores across model types, and compare the types of errors they make. You can easily save Scikit-Learn models by using Python’s pickle module, or using sklearn.externals.joblib, which is more efficient at serializing large NumPy arrays.</p></li>
<li><p>If all classifiers are able to estimate class probabilities (i.e., they have a predict_proba() method), then you can tell Scikit-Learn to predict the class with the highest class probability, averaged over all the individual classifiers. This is called soft voting. It often achieves higher performance than hard voting because it gives more weight to highly confident votes. All you need to do is replace voting=“hard” with voting=“soft” and ensure that all classifiers can estimate class probabilities.</p></li>
<li><p>The standard value for k in k-fold cross-validation is 10, which is typically a reasonable choice for most applications. However, if we are working with relatively small training sets, it can be useful to increase the number of folds. If we increase the value of k, more training data will be used in each iteration, which results in a lower bias towards estimating the generalization performance by averaging the individual model estimates. However, large values of k will also increase the runtime of the cross-validation algorithm and yield estimates with higher variance since the training folds will be more similar to each other. On the other hand, if we are working with large datasets, we can choose a smaller value for k.</p></li>
<li><p>Distance Metrics: Hamming distance for categorical variables, Cosine distance for text data, Gower distance for categorical data</p></li>
<li><p>AIC x BIC: Explanatory power x Parsimonious</p></li>
<li><p>Bias is how far offon the average the model is from the truth. Variance is how much the estimate varies about its average. With low model complexity bias is high because predictions are more likely to stray from the truth, and variance is low because there are only few parameters being fit. With high model complexity bias is low because the model can adapt to more subtleties in the data, and variance is high because we have more parameters to estimate from the same amount of data.</p></li>
<li><p>The mean of highly correlated quantities has higher variance than the mean of uncorrelated quantities. The smaller the number of folds, the more biased the error estimates (they will be biased to be conservative indicating higher error than there is in reality) but the less variable they will be. On the extreme end you can have one fold for each data point which is known as Leave-One-Out-Cross-Validation. In this case, your error estimate is essentially unbiased but it could potentially have high variance.</p></li>
<li><p>LDA vs Logistic Regression: When the classes are well-separated parameters of logistic regression are unstable and LDA doesn’t have this problem. If the distribution of X is approximately normal then LDA is more stable again. LDA doesn’t need to fit multiple models for multiclass classification.</p></li>
<li><p>SVM vs Logistic Regression: SVM is likely to overfit the data and can dominated LR on the training set. Kernel method makes SVM more flexible.</p></li>
<li><p>Decision Tree vs Linear Model: Tree has non-linear boundary, selects important features, easier to intepret but unstable especially for small data.</p></li>
<li><p>Naive Bayes vs LDA: Both use Bayes theorem, both assume gaussian distribution, but LDA takes care of the correlations. LDA might out-perform Naive Bayes when the features are highly correlated.</p></li>
<li><p>KNN vs Other Classification Methods: KNN is completely non-parametic, doesn’t tell which features are important. Dominates LDA and Logistic Regression when the decision boundary is highly non-linear.</p></li>
<li><p>When building, testing, and validating a suite of models, the optimal model (i.e., the optimal point in the ROC curve) is the spot where the overall accuracy of the model is essentially unchanged as we make small adjustments in the choice of model. That is to say, the change (from one model to the next) in the model’s precision (specificity) is exactly offset by the change in the model’s recall (sensitivity). Consequently, allowing for some imperfection, where the false positive rate and the false negative rate are in proper balance (so that neither one has too great of an impact on the overall accuracy and effectiveness of the model), is good fruit from your big data labors. The metric I used to guide my cross-validation is the F-score. This is a good metric when we have a lot more samples from one category than from the other categories.</p></li>
<li><p>The cost of the holdout method comes in the amount of data that is removed from the model training process. For instance, in the illustrative example here, we removed 30% of our data. This means that our model is trained on a smaller data set and its error is likely to be higher than if we trained it on the full data set. The standard procedure in this case is to report your error using the holdout set, and then train a final model using all your data.</p></li>
<li><p>A metric that minimizes false positives, by rarely flagging players and teams that fail to achieve the desired outcome, is specific. A metric that minimizes false negatives, by rarely failing to flag players and teams that achieve the desired outcome, is sensitive.</p></li>
<li><p>Although the previous code example was useful to illustrate how k-fold cross-validation works, scikit-learn also implements a k-fold cross-validation scorer, which allows us to evaluate our model using stratified k-fold cross-validation more efficiently</p></li>
<li><p>Sometimes it may be more useful to report the coefficient of determination (), which can be understood as a standardized version of the MSE, for better interpretability of the model performance. In other words, is the fraction of response variance that is captured by the model. The value is defined as follows:Here, SSE is the sum of squared errors and SST is the total sum of squares , or in other words, it is simply the variance of the response.</p></li>
<li><p>Most people start working with data from exactly the wrong end. They begin with a data set, then apply their favorite tools and techniques to it. The result is narrow questions and shallow arguments. Starting with data, without first doing a lot of thinking, without having any structure, is a short road to simple questions and unsurprising results. We don’t want unsurprising—we want knowledge.</p></li>
<li><p>Remember that the positive class in scikit-learn is the class that is labeled as class 1. If we want to specify a different positive label, we can construct our own scorer via the make_scorer function, which we can then directly provide as an argument to the scoring parameter in GridSearchCV</p></li>
<li><p>While the weighted macro-average is the default for multiclass problems in scikit-learn, we can specify the averaging method via the average parameter inside the different scoring functions that we import from the sklean.metrics module, for example, the precision_score or make_scorer functions</p></li>
<li><p>Transforming words into feature vectorsTo construct a bag-of-words model based on the word counts in the respective documents, we can use the CountVectorizer class implemented in scikit-learn. As we will see in the following code section, the CountVectorizer class takes an array of text data, which can be documents or just sentences, and constructs the bag-of-words model for us:</p></li>
<li><p>Scikit-learn implements yet another transformer, the TfidfTransformer, that takes the raw term frequencies from CountVectorizer as input and transforms them into tf-idfs.</p></li>
<li><p>Naive Bayes is a linear classifier, while k-NN is not. The curse of dimensionality and large feature sets are a problem for k-NN, while Naive Bayes performs well. k-NN requires no training (just load in the dataset), whereas Naive Bayes does.</p></li>
</ul>
</div>
<div id="other-2" class="section level2">
<h2><span class="header-section-number">5.6</span> Other</h2>
<div id="general-2" class="section level3">
<h3><span class="header-section-number">5.6.1</span> General</h3>
<ul>
<li><p>R uses class encoded as 1 in classification to make predictions. Use levels() function on factor to determine what 1 is.</p></li>
<li><p><a href="http://ml-cheatsheet.readthedocs.io/en/latest/index.html" class="uri">http://ml-cheatsheet.readthedocs.io/en/latest/index.html</a></p></li>
<li><p>For example, multilevel models themselves may be referred to as hierarchical linear models, random effects models, multilevel models, random intercept models, random slope models, or pooling models.</p></li>
<li><p><a href="https://www.listendata.com/2018/03/regression-analysis.html" class="uri">https://www.listendata.com/2018/03/regression-analysis.html</a></p></li>
<li><p>In general, raising the classification threshold reduces false positives, thus raising precision.</p></li>
<li><p>Multi-Modal Classification: In the machine learning community, the term Multi-Modal is used to refer to multiple kinds of data. For example, consider a YouTube video. It can be thought to contain 3 different modalities: 1) The video frames (visual modality), 2) The audio clip of what’s being spoken (audio modality), 3) Some videos also come with the transcription of the words spoken in the form of subtitles (textual modality)</p></li>
<li><p>Less samples and more features increase the chance of overfitting.</p></li>
<li><p>You can choose either of the following inference strategies: offline inference, meaning that you make all possible predictions in a batch, using a MapReduce or something similar. You then write the predictions to an SSTable or Bigtable, and then feed these to a cache/lookup table. online inference, meaning that you predict on demand, using a server.</p></li>
<li><p>A static model is trained offline. That is, we train the model exactly once and then use that trained model for a while. A dynamic model is trained online. That is, data is continually entering the system and we’re incorporating that data into the model through continuous updates.</p></li>
<li><p>How would multiplying all of the predictions from a given model by 2.0 (for example, if the model predicts 0.4, we multiply by 2.0 to get a prediction of 0.8) change the model’s performance as measured by AUC? No change. AUC only cares about relative prediction scores. Yes, AUC is based on the relative predictions, so any transformation of the predictions that preserves the relative ranking has no effect on AUC. This is clearly not the case for other metrics such as squared error, log loss, or prediction bias (discussed later).</p></li>
<li><p>AUC is desirable for the following two reasons: AUC is scale-invariant. It measures how well predictions are ranked, rather than their absolute values. UC is classification-threshold-invariant. It measures the quality of the model’s predictions irrespective of what classification threshold is chosen. However, both these reasons come with caveats, which may limit the usefulness of AUC in certain use cases: Scale invariance is not always desirable. For example, sometimes we really do need well calibrated probability outputs, and AUC won’t tell us about that. Classification-threshold invariance is not always desirable. In cases where there are wide disparities in the cost of false negatives vs. false positives, it may be critical to minimize one type of classification error. For example, when doing email spam detection, you likely want to prioritize minimizing false positives (even if that results in a significant increase of false negatives). AUC isn’t a useful metric for this type of optimization.</p></li>
<li><p>Raising our classification threshold will cause the number of true positives to decrease or stay the same and will cause the number of false negatives to increase or stay the same. Thus, recall will either stay constant or decrease.</p></li>
<li><p><a href="https://www.dataquest.io/blog/learning-curves-machine-learning/" class="uri">https://www.dataquest.io/blog/learning-curves-machine-learning/</a></p></li>
<li><p>A feature cross is a synthetic feature formed by multiplying (crossing) two or more features. Crossing combinations of features can provide predictive abilities beyond what those features can provide individually.</p></li>
<li><p>Bias: Error Introduced by approximating real-life problem by using a simple method</p></li>
<li><p>Imagine a linear model with two strongly correlated features; that is, these two features are nearly identical copies of one another but one feature contains a small amount of random noise. If we train this model with L2 regularization, what will happen to the weights for these two features? L2 regularization will force the features towards roughly equivalent weights that are approximately half of what they would have been had only one of the two features been in the model.</p></li>
<li><p>L2 regularization will encourage many of the non-informative weights to be nearly (but not exactly) 0.0.</p></li>
<li><p>F1 Score = TP/(TP + FP + FN). Essentially how good you are at identifying the positive class.</p></li>
<li><p>Active Learning: Finding the optimal data to manually label</p></li>
<li><p>Simulated Annealing: Gradient descent extension</p></li>
<li><p>A variable that is completely useless by itself can provide a significant performance improvement when taken with others.</p></li>
<li><p>So, if your problem involves kind of searching a needle in the haystack when for ex: the positive class samples are very rare compared to the negative classes, use a precision recall curve. Otherwise use a ROC curve because a ROC curve remains the same regardless of the baseline prior probability of your positive class (the important rare class).</p></li>
<li><p>Variance: The amount the model output will change if the model was trained using a different data</p></li>
<li><p>Flexible Methods: higher variance, low bias, Example: KNN (k = 1)</p></li>
<li><p>Inflexible Methods: low variance, high bias, Example: Linear regression</p></li>
<li><p>Sequence mining Algorithms: spade, gsp, freespan; hmmlearn</p></li>
<li><p>Network models: graphical lasso, ising model, granger causality test, network hypothesis testing, bayesian networks</p></li>
<li><p>Parametric: Model has a function shape and form, Model is trained/fit using training data to determine parameter values, reduces the problem of training a model down to training a set of parameter values, Examples: linear regression</p></li>
<li><p>Non-Parametric: No assumption about model form is made, Example: KNN</p></li>
<li><p>Set Random state by hand in sklearn.</p></li>
<li><p>Generally need 3 rows of data per variable (to prevent model from seeing signal where there is only noise) [Nina Zumel]</p></li>
<li><p>Domain knowledge for feature selection and random forest feature importance (5-10) best variables. [Nina Zumel]</p></li>
<li><p>Because we have a couple thousand data points, even though salary can best be represented by a Poisson distribution, I’m going to use Gaussian distribution. (With bigger datasets, these two distribution start to become very similar).</p></li>
<li><p>We can also tell that our input, smart_1_normalized, doesn’t have a very strong relationship to our output because the standard error (0.009) is more than 1/10 of our estimate (0.02).</p></li>
<li><p>The less complex an ML model, the more likely that a good empirical result is not just due to the peculiarities of the sample.</p></li>
<li><p>The ML Fine Print: Three basic assumptions: 1) We draw examples independently and identically (i.i.d.) at random from the distribution, 2) The distribution is stationary: It doesn’t change over time, 3) We always pull from the same distribution: Including training, validation, and test sets. In practice, we sometimes violate these assumptions. For example: 1) Consider a model that chooses ads to display. The i.i.d. assumption would be violated if the model bases its choice of ads, in part, on what ads the user has previously seen. 2) Consider a data set that contains retail sales information for a year. User’s purchases change seasonally, which would violate stationarity. When we know that any of the preceding three basic assumptions are violated, we must pay careful attention to metrics.</p></li>
<li><p>empirical risk minimization: In supervised learning, a machine learning algorithm builds a model by examining many examples and attempting to find a model that minimizes loss.</p></li>
<li><p>Loss is the penalty for a bad prediction. That is, loss is a number indicating how bad the model’s prediction was on a single example. If the model’s prediction is perfect, the loss is zero; otherwise, the loss is greater.</p></li>
</ul>
</div>
<div id="unbalanced-classes" class="section level3">
<h3><span class="header-section-number">5.6.2</span> Unbalanced Classes</h3>
<p>That said, here is a rough outline of useful approaches. These are listed approximately in order of effort:</p>
<p>Do nothing. Sometimes you get lucky and nothing needs to be done. You can train on the so-called natural (or stratified) distribution and sometimes it works without need for modification.</p>
<p>Balance the training set in some way: Oversample the minority class. Undersample the majority class. Synthesize new minority classes.</p>
<p>Throw away minority examples and switch to an anomaly detection framework.</p>
<p>At the algorithm level, or after it: Adjust the class weight (misclassification costs). Adjust the decision threshold. Modify an existing algorithm to be more sensitive to rare classes.</p>
<p>Construct an entirely new algorithm to perform well on imbalanced data.</p>
<p>Use AUC, F1 Score, Cohen’s Kappa, ROC curve, a precision-recall curve, a lift curve, or a profit (gain) curve for evaluation.</p>
<p>Use probability estimates and not the default .5 threshold.</p>
<p>Undersampling, oversampling, increase weight of minority class</p>
<p>Decision trees often perform well on imbalanced datasets because their hierarchical structure allows them to learn signals from both classes.</p>
<p>Reframe as Anomaly Detection</p>
<ul>
<li>Sklearn Eg: wclf = svm.SVC(kernel=‘linear’, class_weight={1: 10}) &amp; svm.SVC(kernel=‘linear’, class_weight=‘balanced’)</li>
</ul>
</div>
<div id="longitudinal-data" class="section level3">
<h3><span class="header-section-number">5.6.3</span> Longitudinal Data</h3>
<ul>
<li><p>One far out suggestion in one of the links is to do PCA and then regress the original variables against the new axis.</p></li>
<li><p>Account for longitudinal data: random selection of one event from each student, partition cross validation by dates, pca finds major trends hidden within the data</p></li>
</ul>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="model.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="cdm.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/03-model.Rmd",
"text": "Edit"
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
