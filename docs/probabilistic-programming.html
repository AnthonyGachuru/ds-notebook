<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Probabilistic Programming | Data Science Cribsheet</title>
  <meta name="description" content="A collection of quick notes on the field." />
  <meta name="generator" content="bookdown 0.12 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Probabilistic Programming | Data Science Cribsheet" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A collection of quick notes on the field." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Probabilistic Programming | Data Science Cribsheet" />
  
  <meta name="twitter:description" content="A collection of quick notes on the field." />
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="deep-learning.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">DS Cribsheet</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Workflow</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#preamble"><i class="fa fa-check"></i><b>1.1</b> Preamble</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#checklist"><i class="fa fa-check"></i><b>1.2</b> Checklist</a><ul>
<li class="chapter" data-level="1.2.1" data-path="index.html"><a href="index.html#preparation"><i class="fa fa-check"></i><b>1.2.1</b> Preparation</a></li>
<li class="chapter" data-level="1.2.2" data-path="index.html"><a href="index.html#pre-modeling"><i class="fa fa-check"></i><b>1.2.2</b> Pre-Modeling</a></li>
<li class="chapter" data-level="1.2.3" data-path="index.html"><a href="index.html#model"><i class="fa fa-check"></i><b>1.2.3</b> Model</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#resources"><i class="fa fa-check"></i><b>1.3</b> Resources</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#pipelines"><i class="fa fa-check"></i><b>1.4</b> Pipelines</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="pre-modeling-1.html"><a href="pre-modeling-1.html"><i class="fa fa-check"></i><b>2</b> Pre-Modeling</a><ul>
<li class="chapter" data-level="2.1" data-path="pre-modeling-1.html"><a href="pre-modeling-1.html#import"><i class="fa fa-check"></i><b>2.1</b> Import</a><ul>
<li class="chapter" data-level="2.1.1" data-path="pre-modeling-1.html"><a href="pre-modeling-1.html#general"><i class="fa fa-check"></i><b>2.1.1</b> General</a></li>
<li class="chapter" data-level="2.1.2" data-path="pre-modeling-1.html"><a href="pre-modeling-1.html#sql"><i class="fa fa-check"></i><b>2.1.2</b> SQL</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="pre-modeling-1.html"><a href="pre-modeling-1.html#tidy-transform"><i class="fa fa-check"></i><b>2.2</b> Tidy &amp; Transform</a><ul>
<li class="chapter" data-level="2.2.1" data-path="pre-modeling-1.html"><a href="pre-modeling-1.html#general-1"><i class="fa fa-check"></i><b>2.2.1</b> General</a></li>
<li class="chapter" data-level="2.2.2" data-path="pre-modeling-1.html"><a href="pre-modeling-1.html#missingness-imputation"><i class="fa fa-check"></i><b>2.2.2</b> Missingness &amp; Imputation</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="pre-modeling-1.html"><a href="pre-modeling-1.html#visualize"><i class="fa fa-check"></i><b>2.3</b> Visualize</a></li>
<li class="chapter" data-level="2.4" data-path="pre-modeling-1.html"><a href="pre-modeling-1.html#pre-processing"><i class="fa fa-check"></i><b>2.4</b> Pre-processing</a><ul>
<li class="chapter" data-level="2.4.1" data-path="pre-modeling-1.html"><a href="pre-modeling-1.html#feature-engineering"><i class="fa fa-check"></i><b>2.4.1</b> Feature Engineering</a></li>
<li class="chapter" data-level="2.4.2" data-path="pre-modeling-1.html"><a href="pre-modeling-1.html#feature-selection"><i class="fa fa-check"></i><b>2.4.2</b> Feature Selection</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="model-1.html"><a href="model-1.html"><i class="fa fa-check"></i><b>3</b> Model</a><ul>
<li class="chapter" data-level="3.1" data-path="model-1.html"><a href="model-1.html#toolkit"><i class="fa fa-check"></i><b>3.1</b> Toolkit</a></li>
<li class="chapter" data-level="3.2" data-path="model-1.html"><a href="model-1.html#general-2"><i class="fa fa-check"></i><b>3.2</b> General</a></li>
<li class="chapter" data-level="3.3" data-path="model-1.html"><a href="model-1.html#model-selectionevaluation"><i class="fa fa-check"></i><b>3.3</b> Model Selection/Evaluation</a></li>
<li class="chapter" data-level="3.4" data-path="model-1.html"><a href="model-1.html#supervised"><i class="fa fa-check"></i><b>3.4</b> Supervised</a><ul>
<li class="chapter" data-level="3.4.1" data-path="model-1.html"><a href="model-1.html#glm"><i class="fa fa-check"></i><b>3.4.1</b> GLM</a></li>
<li class="chapter" data-level="3.4.2" data-path="model-1.html"><a href="model-1.html#generalized-additive-models"><i class="fa fa-check"></i><b>3.4.2</b> Generalized Additive Models</a></li>
<li class="chapter" data-level="3.4.3" data-path="model-1.html"><a href="model-1.html#knn"><i class="fa fa-check"></i><b>3.4.3</b> KNN</a></li>
<li class="chapter" data-level="3.4.4" data-path="model-1.html"><a href="model-1.html#svm"><i class="fa fa-check"></i><b>3.4.4</b> SVM</a></li>
<li class="chapter" data-level="3.4.5" data-path="model-1.html"><a href="model-1.html#decision-tree"><i class="fa fa-check"></i><b>3.4.5</b> Decision Tree</a></li>
<li class="chapter" data-level="3.4.6" data-path="model-1.html"><a href="model-1.html#bagging"><i class="fa fa-check"></i><b>3.4.6</b> Bagging</a></li>
<li class="chapter" data-level="3.4.7" data-path="model-1.html"><a href="model-1.html#random-forest"><i class="fa fa-check"></i><b>3.4.7</b> Random Forest</a></li>
<li class="chapter" data-level="3.4.8" data-path="model-1.html"><a href="model-1.html#boosting"><i class="fa fa-check"></i><b>3.4.8</b> Boosting</a></li>
<li class="chapter" data-level="3.4.9" data-path="model-1.html"><a href="model-1.html#naive-bayes"><i class="fa fa-check"></i><b>3.4.9</b> Na√Øve Bayes</a></li>
<li class="chapter" data-level="3.4.10" data-path="model-1.html"><a href="model-1.html#lda-qda"><i class="fa fa-check"></i><b>3.4.10</b> LDA &amp; QDA</a></li>
<li class="chapter" data-level="3.4.11" data-path="model-1.html"><a href="model-1.html#gams"><i class="fa fa-check"></i><b>3.4.11</b> GAMs</a></li>
<li class="chapter" data-level="3.4.12" data-path="model-1.html"><a href="model-1.html#hierarchical-models"><i class="fa fa-check"></i><b>3.4.12</b> Hierarchical Models</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="model-1.html"><a href="model-1.html#unsupervised"><i class="fa fa-check"></i><b>3.5</b> Unsupervised</a><ul>
<li class="chapter" data-level="3.5.1" data-path="model-1.html"><a href="model-1.html#k-means-clustering"><i class="fa fa-check"></i><b>3.5.1</b> K-Means Clustering</a></li>
<li class="chapter" data-level="3.5.2" data-path="model-1.html"><a href="model-1.html#hierarchical-clustering"><i class="fa fa-check"></i><b>3.5.2</b> Hierarchical Clustering</a></li>
<li class="chapter" data-level="3.5.3" data-path="model-1.html"><a href="model-1.html#dimensionality-reduction"><i class="fa fa-check"></i><b>3.5.3</b> Dimensionality Reduction</a></li>
<li class="chapter" data-level="3.5.4" data-path="model-1.html"><a href="model-1.html#multiple-correspondence-analysis"><i class="fa fa-check"></i><b>3.5.4</b> Multiple Correspondence Analysis</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="model-1.html"><a href="model-1.html#semi-supervised"><i class="fa fa-check"></i><b>3.6</b> Semi Supervised</a></li>
<li class="chapter" data-level="3.7" data-path="model-1.html"><a href="model-1.html#reinforcement-learning"><i class="fa fa-check"></i><b>3.7</b> Reinforcement Learning</a></li>
<li class="chapter" data-level="3.8" data-path="model-1.html"><a href="model-1.html#other-ml"><i class="fa fa-check"></i><b>3.8</b> Other ML</a><ul>
<li class="chapter" data-level="3.8.1" data-path="model-1.html"><a href="model-1.html#association-rule-mining"><i class="fa fa-check"></i><b>3.8.1</b> Association Rule Mining</a></li>
<li class="chapter" data-level="3.8.2" data-path="model-1.html"><a href="model-1.html#nlp"><i class="fa fa-check"></i><b>3.8.2</b> NLP</a></li>
<li class="chapter" data-level="3.8.3" data-path="model-1.html"><a href="model-1.html#geospatial"><i class="fa fa-check"></i><b>3.8.3</b> Geospatial</a></li>
<li class="chapter" data-level="3.8.4" data-path="model-1.html"><a href="model-1.html#ai"><i class="fa fa-check"></i><b>3.8.4</b> AI</a></li>
<li class="chapter" data-level="3.8.5" data-path="model-1.html"><a href="model-1.html#cv"><i class="fa fa-check"></i><b>3.8.5</b> CV</a></li>
<li class="chapter" data-level="3.8.6" data-path="model-1.html"><a href="model-1.html#online-learning"><i class="fa fa-check"></i><b>3.8.6</b> Online Learning</a></li>
<li class="chapter" data-level="3.8.7" data-path="model-1.html"><a href="model-1.html#sequences"><i class="fa fa-check"></i><b>3.8.7</b> Sequences</a></li>
<li class="chapter" data-level="3.8.8" data-path="model-1.html"><a href="model-1.html#federated-ml"><i class="fa fa-check"></i><b>3.8.8</b> Federated ML</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="communicate-deploy-maintain.html"><a href="communicate-deploy-maintain.html"><i class="fa fa-check"></i><b>4</b> Communicate, Deploy, Maintain</a><ul>
<li class="chapter" data-level="4.1" data-path="communicate-deploy-maintain.html"><a href="communicate-deploy-maintain.html#communicate"><i class="fa fa-check"></i><b>4.1</b> Communicate</a><ul>
<li class="chapter" data-level="4.1.1" data-path="communicate-deploy-maintain.html"><a href="communicate-deploy-maintain.html#outline"><i class="fa fa-check"></i><b>4.1.1</b> Outline</a></li>
<li class="chapter" data-level="4.1.2" data-path="communicate-deploy-maintain.html"><a href="communicate-deploy-maintain.html#other"><i class="fa fa-check"></i><b>4.1.2</b> Other</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="communicate-deploy-maintain.html"><a href="communicate-deploy-maintain.html#maintain"><i class="fa fa-check"></i><b>4.2</b> Maintain</a></li>
<li class="chapter" data-level="4.3" data-path="communicate-deploy-maintain.html"><a href="communicate-deploy-maintain.html#deploy"><i class="fa fa-check"></i><b>4.3</b> Deploy</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="stats.html"><a href="stats.html"><i class="fa fa-check"></i><b>5</b> Stats</a><ul>
<li class="chapter" data-level="5.1" data-path="stats.html"><a href="stats.html#general-3"><i class="fa fa-check"></i><b>5.1</b> General</a></li>
<li class="chapter" data-level="5.2" data-path="stats.html"><a href="stats.html#sample-size"><i class="fa fa-check"></i><b>5.2</b> Sample Size</a></li>
<li class="chapter" data-level="5.3" data-path="stats.html"><a href="stats.html#hypothesis-testing"><i class="fa fa-check"></i><b>5.3</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="5.4" data-path="stats.html"><a href="stats.html#ab-testing"><i class="fa fa-check"></i><b>5.4</b> AB Testing</a></li>
<li class="chapter" data-level="5.5" data-path="stats.html"><a href="stats.html#experimental-design"><i class="fa fa-check"></i><b>5.5</b> Experimental Design</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="sequences-1.html"><a href="sequences-1.html"><i class="fa fa-check"></i><b>6</b> Sequences</a><ul>
<li class="chapter" data-level="6.1" data-path="sequences-1.html"><a href="sequences-1.html#time-series"><i class="fa fa-check"></i><b>6.1</b> Time Series</a></li>
<li class="chapter" data-level="6.2" data-path="sequences-1.html"><a href="sequences-1.html#toolkit-1"><i class="fa fa-check"></i><b>6.2</b> Toolkit</a><ul>
<li class="chapter" data-level="6.2.1" data-path="sequences-1.html"><a href="sequences-1.html#notes"><i class="fa fa-check"></i><b>6.2.1</b> Notes</a></li>
<li class="chapter" data-level="6.2.2" data-path="sequences-1.html"><a href="sequences-1.html#workflow-1"><i class="fa fa-check"></i><b>6.2.2</b> Workflow</a></li>
<li class="chapter" data-level="6.2.3" data-path="sequences-1.html"><a href="sequences-1.html#preprocessing"><i class="fa fa-check"></i><b>6.2.3</b> Preprocessing</a></li>
<li class="chapter" data-level="6.2.4" data-path="sequences-1.html"><a href="sequences-1.html#feature-engineering-1"><i class="fa fa-check"></i><b>6.2.4</b> Feature Engineering</a></li>
<li class="chapter" data-level="6.2.5" data-path="sequences-1.html"><a href="sequences-1.html#packages-functions"><i class="fa fa-check"></i><b>6.2.5</b> Packages / Functions</a></li>
<li class="chapter" data-level="6.2.6" data-path="sequences-1.html"><a href="sequences-1.html#models"><i class="fa fa-check"></i><b>6.2.6</b> Models</a></li>
<li class="chapter" data-level="6.2.7" data-path="sequences-1.html"><a href="sequences-1.html#model-selection"><i class="fa fa-check"></i><b>6.2.7</b> Model Selection</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="sequences-1.html"><a href="sequences-1.html#dsp"><i class="fa fa-check"></i><b>6.3</b> DSP</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="deep-learning.html"><a href="deep-learning.html"><i class="fa fa-check"></i><b>7</b> Deep Learning</a><ul>
<li class="chapter" data-level="7.1" data-path="deep-learning.html"><a href="deep-learning.html#general-6"><i class="fa fa-check"></i><b>7.1</b> General</a></li>
<li class="chapter" data-level="7.2" data-path="deep-learning.html"><a href="deep-learning.html#pre-processing-1"><i class="fa fa-check"></i><b>7.2</b> Pre-processing</a></li>
<li class="chapter" data-level="7.3" data-path="deep-learning.html"><a href="deep-learning.html#defaults"><i class="fa fa-check"></i><b>7.3</b> Defaults</a></li>
<li class="chapter" data-level="7.4" data-path="deep-learning.html"><a href="deep-learning.html#rnnlstm"><i class="fa fa-check"></i><b>7.4</b> RNN/LSTM</a></li>
<li class="chapter" data-level="7.5" data-path="deep-learning.html"><a href="deep-learning.html#tuning"><i class="fa fa-check"></i><b>7.5</b> Tuning</a></li>
<li class="chapter" data-level="7.6" data-path="deep-learning.html"><a href="deep-learning.html#debugging"><i class="fa fa-check"></i><b>7.6</b> Debugging</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="probabilistic-programming.html"><a href="probabilistic-programming.html"><i class="fa fa-check"></i><b>8</b> Probabilistic Programming</a><ul>
<li class="chapter" data-level="8.1" data-path="probabilistic-programming.html"><a href="probabilistic-programming.html#bayes-rule"><i class="fa fa-check"></i><b>8.1</b> Bayes Rule</a></li>
<li class="chapter" data-level="8.2" data-path="probabilistic-programming.html"><a href="probabilistic-programming.html#other-2"><i class="fa fa-check"></i><b>8.2</b> Other</a></li>
<li class="chapter" data-level="8.3" data-path="probabilistic-programming.html"><a href="probabilistic-programming.html#doing-bayesian-da"><i class="fa fa-check"></i><b>8.3</b> Doing Bayesian DA</a></li>
<li class="chapter" data-level="8.4" data-path="probabilistic-programming.html"><a href="probabilistic-programming.html#statistical-rethinking"><i class="fa fa-check"></i><b>8.4</b> Statistical Rethinking</a></li>
<li class="chapter" data-level="8.5" data-path="probabilistic-programming.html"><a href="probabilistic-programming.html#causality"><i class="fa fa-check"></i><b>8.5</b> Causality</a><ul>
<li class="chapter" data-level="8.5.1" data-path="probabilistic-programming.html"><a href="probabilistic-programming.html#general-7"><i class="fa fa-check"></i><b>8.5.1</b> General</a></li>
<li class="chapter" data-level="8.5.2" data-path="probabilistic-programming.html"><a href="probabilistic-programming.html#causality-edx"><i class="fa fa-check"></i><b>8.5.2</b> Causality edX</a></li>
<li class="chapter" data-level="8.5.3" data-path="probabilistic-programming.html"><a href="probabilistic-programming.html#causality-coursera"><i class="fa fa-check"></i><b>8.5.3</b> Causality Coursera</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Science Cribsheet</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="probabilistic-programming" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Probabilistic Programming</h1>
<div id="bayes-rule" class="section level2">
<h2><span class="header-section-number">8.1</span> Bayes Rule</h2>
<p>old_prob [P(h)/p(~h)] * strength_of_evidence [p(e|h)/p(e|~h)] = new_prob [p(h|e)/p(~h|e)]</p>
<p>Posterior = Likelihood * Prior / Marginal Likelihood</p>
<p>Likelihood P(Data|theta): Probability of the date could be generated by a model with the given parameter(s)</p>
<p>Prior P(theta): Probability of parameter value</p>
<p>Posterior P(theta|D): Credibility of the parameter value with the data taken into account.</p>
<p>Marginal Likelihood (Evidence): Probability of evidence</p>
</div>
<div id="other-2" class="section level2">
<h2><span class="header-section-number">8.2</span> Other</h2>
<p><a href="http://brunaw.com/slides/SER2019/talk/pres.html#1" class="uri">http://brunaw.com/slides/SER2019/talk/pres.html#1</a> <a href="https://causal.app/" class="uri">https://causal.app/</a> <a href="https://docs.google.com/presentation/d/1buknIrG5b8u0twrwvlxcTudIOdx68AlqDiST_A_jJ9g/edit#slide=id.g4254d546f6_0_0">PyMc3 Quick Intro</a> <a href="https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations">Prior Guide</a></p>
<p><a href="http://daft-pgm.org">PGM Viz</a></p>
<p>Another option is to report just the Bayes Factor or likelihood ratio, rather than the posterior probability.</p>
<p>Models have a lot of parameters classical methods that use simple point estimates of the parameters don‚Äôt adequately capture uncertainty so we turn to Bayesian methods because Bayesian inference is one way of finding a large model with metadata.</p>
<p>The second reason we use Bayesian inference is for combining information you might have experimental data on the drug under one condition and aggregate data under another condition we can combine polls and election forecast of public opinion data from multiple poles and environmental statistics we have measurements of different quality Bayesian methods are particularly adapted tocombining information.</p>
<p>The third appeal of Bayesian methods and the sort of applications that I work on is that the inferences can map directly to decisions based on inferences express not its estimates and standard errors but rather as probability distributions we can take these probability distributions and pipe them directly into decision analysis for these reasons.</p>
<p>An advantage of recognizing that the prior distribution is a testable part of a Bayesian model is that it clarifies the role of the prior inference, and where it comes from.</p>
<p>Approximate Bayesian Computation</p>
<p>Laplace approximation: Approximate the posterior of a non-conjugate model</p>
<p>When one uses likelihood to get point estimates of model parameters, it‚Äôs called maximum-likelihood estimation, or MLE. If one also takes the prior into account, then it‚Äôs maximum a posteriori estimation (MAP). MLE and MAP are the same if the prior is uniform.</p>
<p>Beta conjugacy: Beta for prior &amp; binomial for likelihoood implies beta for posterior. Update: beta(alpha + successes, beta + failures). Mean: alpha/(alpha + beta)</p>
<p>Small alpha and beta means the prior is less informative</p>
<p>Empirical Bayes is an approximation to more exact Bayesian methods. With a lot of data it is a very good approximation.</p>
</div>
<div id="doing-bayesian-da" class="section level2">
<h2><span class="header-section-number">8.3</span> Doing Bayesian DA</h2>
<p>In general, Bayesian analysis of data follows these steps:</p>
<ol style="list-style-type: decimal">
<li><p>Identify the data relevant to the research questions. What are the measurement scales of the data? Which data variables are to be predicted, and which data variables are supposed to act as predictors?</p></li>
<li><p>Define a descriptive model for the relevant data. The mathematical form and its parameters should be meaningful and appropriate to the theoretical purposes of the analysis.</p></li>
<li><p>Specify a prior distribution on the parameters. The prior must pass muster with the audience of the analysis, such as skeptical scientists.</p></li>
<li><p>Use Bayesian inference to re-allocate credibility across parameter values. Interpret the posterior distribution with respect to theoretically meaningful issues (assuming that the model is a reasonable description of the data; see next step).</p></li>
<li><p>Check that the posterior predictions mimic the data with reasonable accuracy (i.e., conduct a ‚Äúposterior predictive check‚Äù). If not, then consider a different descriptive model.</p></li>
</ol>
</div>
<div id="statistical-rethinking" class="section level2">
<h2><span class="header-section-number">8.4</span> Statistical Rethinking</h2>
<p>bayesian: 1) likelihood: binomial 2) parameters of the likelihood 3) prior for each param</p>
<p>Data are measured and known; parameters are unknown and must be estimated from data.</p>
<p>Compared to MCMC, variational inference tends to be faster and easier to scale to large data</p>
<p>To combat under/overfitting use a regularizing prior or a scoring device like information criteria.</p>
<p>As you‚Äôll see once you start using AIC and related measures, it is true that predictor variables that do improve prediction are not always statistically significant. It is also possible for variables that are statistically significant to do nothing useful for prediction.</p>
<p>But the prior on Œ≤ is narrower and is meant to regularize. The prior Œ≤ ~ Normal(0,1) says that, before seeing the data, the machine should be very skeptical of values above 2 and below ‚àí2, as a Gaussian prior with a standard deviation of 1 assigns only 5% plausibility to values above and below 2 standard deviations. Because the predictor variable x is standardized, you can interpret this as meaning that a change of 1 standard deviation in x is very unlikely to produce 2 units of change in the outcome.</p>
<p>metrics: dic, waic</p>
<p>Sampling from the posterior now and plotting the model‚Äôs predictions would help immensely with interpretation. And that‚Äôs the course I want to encourage and provide code for. In general, it‚Äôs not safe to interpret interactions without plotting them.</p>
<p>bayesian model: generative model (binomial number of success) + prior (eg. sample of probabilities of success from uniform)</p>
<p>bayesian inference: bayesian model + data</p>
<p>bayesian inference is conditioning on the data to learn the parameter values.</p>
<p>The reason that we call it posterior is because it represents the uncertainty after (that is, posterior to) having included the information in the data.</p>
<p>Bayesian Inference Methods: Sampling, Grid Approximation</p>
</div>
<div id="causality" class="section level2">
<h2><span class="header-section-number">8.5</span> Causality</h2>
<div id="general-7" class="section level3">
<h3><span class="header-section-number">8.5.1</span> General</h3>
<p><a href="https://twitter.com/tdietterich/status/1034631407904018437">Causality &amp; DL</a> Instrumental variables propensity score matching <a href="http://causalinference.gitlab.io/icwsm-tutorial/" class="uri">http://causalinference.gitlab.io/icwsm-tutorial/</a> <a href="http://causalinference.gitlab.io/dowhy/do_why_simple_example.html" class="uri">http://causalinference.gitlab.io/dowhy/do_why_simple_example.html</a></p>
</div>
<div id="causality-edx" class="section level3">
<h3><span class="header-section-number">8.5.2</span> Causality edX</h3>
<p>Draw a complete DAG if your expert knowledge is insufficient to exclude any possible effect.
Knowledge is expressed in the form of missing arrows.</p>
<p>Causal Markov Condition: If two vars share a cause this cause is also on the graph.</p>
<p>An association exists b/w A &amp; Y if having info on A on average better allows us to predict Y.</p>
<p>An association for a -&gt; y exists if the proportion of individuals with y is different given having or not having a.</p>
<p>A -&gt; B -&gt; Y: B is a mediator which blocks the association b/w A &amp; Y when conditioning on it
even though A is causal for Y.</p>
<p>Systematic bias: Any assoication between A &amp; Y not causes by an effect of A on Y.</p>
<p>Bias due to a common cause is called confounding. We expect an association b/w two items with a commong cause.</p>
<p>a &lt;- t -&gt; y: t is a commong cause which blocks the association from a to y when conditioned upon.</p>
<p>Common effects ( a -&gt; y &lt;- b) don‚Äôt cause an association but common causes ( a &lt;- y -&gt; b) do.</p>
<p>For a -&gt; y &lt;- b, y is called a collider. It blocks association between a and b.</p>
<p>Selection bias: Conditioning on a collider introduces an association between its causes.</p>
<p>Bias stems from The existence of a shared cause of treatment and outcome and conditioning on a common effect of treatment and outcome. (Also conditioning on a mediator.)</p>
<p>Associations: 1) Cause &amp; effect, 2) Common causes, 3) Conditioning on a common effect</p>
<p>Bias introduced by association by chance can be removed by increasing the sample size.</p>
<p>A (parent) -&gt; B (descendant)</p>
<p>D(irectionally)-separation rules: Determine whether paths (which don‚Äôt need to follow arrows) are blocked or open.
D-separation rules:</p>
<ol style="list-style-type: decimal">
<li><p>If there are no variables being conditioned on, a path is blocked iff two arrows on the path collide at some
variable on the path.</p></li>
<li><p>Any path with a non-collider that has been conditioned on is blocked.</p></li>
<li><p>A collider that has been conditioned on doesn‚Äôt block the path.</p></li>
<li><p>A collider with a descendant that has been conditioned on doesn‚Äôt block the path.</p></li>
</ol>
<p>The summary of these D-separation rules is that a path is blocked if, and only if, it
contains a non-collider that has been conditioned on, or a collider that has not been conditioned on
and has no descendant that had been conditioned on.</p>
<p>Two variables are d-separated if all paths between then are blocked</p>
<p>And two variables are marginally or unconditionally independent if they are D-separated without conditioning
on all the variables. On the other hand, we say that two variables are conditionally independent, even a set variables L, if they are D-separated after conditioning on the variables L.</p>
<p>Faithfulness: Causal effects in opposite directions that cancel out, i.e a certain medication causes diseases in some
people and prevents it in others. Rare.</p>
<p>All paths are blocked variables are not associated.</p>
<p>back door path: a path connecting A and Y without using arrows that leave from A. eg. Given L ‚Üí A ‚Üí Y ‚Üê L, the path A - L - Y is a back door path.</p>
<p>back door path criterion: We can identify the causal effect of A on Y if we have enough data to block all back door paths between them</p>
<p>confounder: A variable that, possibly with other variables, can block back door paths between treatment and outcome.</p>
<p>Change in estimate definition of confounder: A var is a confounder of the effect of a on y if adjusting for it alters the association between a and y</p>
<p>Conventional definition of confounder: L is a confounder of the effect of a on y if L meets 3 conditions: L is associated with a, L is associated with y conditional on a, L is not on a causal pathway from a to y.</p>
<p>surrogate confounder: a descendant of a cofounder.</p>
<p>turn on recruiter linkedin</p>
<p>cofounding if absolute but cofounder is relative to other variables.</p>
<p>If you can‚Äôt randomize then you have to rely on observational data. Methods: 1) Measure enough variables to block all backdoor paths between A &amp; Y, 2) Alternatives to blocking back door paths (instrumental variable estimation) Here‚Äôs are a few options for 1):</p>
<p>Stratification (adjust for confounding): A way to adjust for a variable by only looking at a subset of it, for example only looking at people who smoke instead of all people. An alternative is to use regression to look at the effects in each subset and then pool the estimates.</p>
<p>Matching (adjust for confounding): Randomly match someone with a &amp; b to ~a &amp; b and a &amp; ~b to ~a &amp; ~b. Then see if there is an associating in the match subset.</p>
<p>G-methods: The following three methods should be used when time-varying confounders are affected by prior treatment.</p>
<p>Inverse Probability Weighting (adjust for confounding): Compute 1/p(a = 1 | b) and 1/p(a = 0 | b), and use these as weights in the association computation. Eliminates backdoor.</p>
<p>Standardization (G-formula)</p>
<p>G-estimation</p>
<p>All these confounding adjustment methods require knowledge of the true causal DAG since you need to know which paths to block.</p>
<p>Randomization, when possible, is the preferred approach to eliminating confounding.</p>
<p>Selection bias under the null is called collider stratification bias. One way it occurs is by conditioning on a collider or the child of a collider.</p>
<p>Case-control study: Outcome based selection design</p>
<p>Selection bias can occur due to eligibility criteria and loss to follow-up. The latter can occur from causes that have significance in the causal graph.</p>
<p>Internal validity: We say that a study has internal validity when the estimated association has a causal interpretation in the study population. That is, there is no selection bias, there is no confounding, et cetera.</p>
<p>External validity: We say that a study has external validity when the estimated association has a causal interpretation in another population. That is, we can transport or generalize the estimated effect to other populations.</p>
<p>In randomized trials, self-selection bias does not affect internal validity, but may affect external validity.</p>
<p>Bias under the null due to self-selection at baseline can happen in a follow up study.</p>
<p>Smart selection, adjustment for selection bias</p>
<p>Use inverse probability weighting to control for the four selection bias dags discussed: {insert dags here}</p>
<p>Competing events</p>
<p>Measurement error: True value isn‚Äôt equal to measured value.</p>
<p>Measurement bias: When the measure of association between a and y (measured treatment &amp; outcome values) differs from that of a and y.</p>
<p>Types of measurement error:</p>
<p>Type 1, independent non-differential.
Type 2, dependent non-differential.
Type 3, independent differential.
Type 4, dependent differential.</p>
<p>Independent non-differential error is the only type of measurement error that does not create bias under the null.</p>
<p>information bias</p>
<p>Statistical models and validation samples can be used to correct measurement error.</p>
<p>We can‚Äôt correctly choose methods of measurement error correction without considering the structure of the measurement error.</p>
<p>A causal DAG can‚Äôt eliminate bias due to confounding. A causal DAG can improve precision. A causal DAG can‚Äôt improve external validity. A causal DAG can represent sources of bias.</p>
<p>Time-varying treatments</p>
<p>treatment-confounder feedback: In its presence most methods are biased. Use G Methods.</p>
<p>The difference method: Run regression with E[M|A,C] which has the term beta_1 times a and E[Y|A,M,C] with the terms theta_1 times a &amp; theta_2 times m. The direct effect is theta_1 and the indirect effect is beta_1 times theta_1.</p>
</div>
<div id="causality-coursera" class="section level3">
<h3><span class="header-section-number">8.5.3</span> Causality Coursera</h3>
<p>The fundamental problem of causal inference: We can only observe one potential outcome for each person.</p>
<p>Causal effect: E(y^1) - E(y^0)</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="deep-learning.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/07-probabilistic_programming.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
