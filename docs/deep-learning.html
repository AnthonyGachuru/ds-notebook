<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Deep Learning | Data Science Cribsheet</title>
  <meta name="description" content="A collection of quick notes on the field." />
  <meta name="generator" content="bookdown 0.12 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Deep Learning | Data Science Cribsheet" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A collection of quick notes on the field." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Deep Learning | Data Science Cribsheet" />
  
  <meta name="twitter:description" content="A collection of quick notes on the field." />
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="sequences-1.html">
<link rel="next" href="probabilistic-programming.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">DS Cribsheet</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Workflow</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#preamble"><i class="fa fa-check"></i><b>1.1</b> Preamble</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#checklist"><i class="fa fa-check"></i><b>1.2</b> Checklist</a><ul>
<li class="chapter" data-level="1.2.1" data-path="index.html"><a href="index.html#preparation"><i class="fa fa-check"></i><b>1.2.1</b> Preparation</a></li>
<li class="chapter" data-level="1.2.2" data-path="index.html"><a href="index.html#pre-modeling"><i class="fa fa-check"></i><b>1.2.2</b> Pre-Modeling</a></li>
<li class="chapter" data-level="1.2.3" data-path="index.html"><a href="index.html#model"><i class="fa fa-check"></i><b>1.2.3</b> Model</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#notes"><i class="fa fa-check"></i><b>1.3</b> Notes</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#resources"><i class="fa fa-check"></i><b>1.4</b> Resources</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#pipelines"><i class="fa fa-check"></i><b>1.5</b> Pipelines</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="pre-modeling-1.html"><a href="pre-modeling-1.html"><i class="fa fa-check"></i><b>2</b> Pre-Modeling</a><ul>
<li class="chapter" data-level="2.1" data-path="pre-modeling-1.html"><a href="pre-modeling-1.html#import"><i class="fa fa-check"></i><b>2.1</b> Import</a><ul>
<li class="chapter" data-level="2.1.1" data-path="pre-modeling-1.html"><a href="pre-modeling-1.html#general"><i class="fa fa-check"></i><b>2.1.1</b> General</a></li>
<li class="chapter" data-level="2.1.2" data-path="pre-modeling-1.html"><a href="pre-modeling-1.html#sql"><i class="fa fa-check"></i><b>2.1.2</b> SQL</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="pre-modeling-1.html"><a href="pre-modeling-1.html#tidy-transform"><i class="fa fa-check"></i><b>2.2</b> Tidy &amp; Transform</a><ul>
<li class="chapter" data-level="2.2.1" data-path="pre-modeling-1.html"><a href="pre-modeling-1.html#general-1"><i class="fa fa-check"></i><b>2.2.1</b> General</a></li>
<li class="chapter" data-level="2.2.2" data-path="pre-modeling-1.html"><a href="pre-modeling-1.html#missingness-imputation"><i class="fa fa-check"></i><b>2.2.2</b> Missingness &amp; Imputation</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="pre-modeling-1.html"><a href="pre-modeling-1.html#visualize"><i class="fa fa-check"></i><b>2.3</b> Visualize</a></li>
<li class="chapter" data-level="2.4" data-path="pre-modeling-1.html"><a href="pre-modeling-1.html#pre-processing"><i class="fa fa-check"></i><b>2.4</b> Pre-processing</a><ul>
<li class="chapter" data-level="2.4.1" data-path="pre-modeling-1.html"><a href="pre-modeling-1.html#feature-engineering"><i class="fa fa-check"></i><b>2.4.1</b> Feature Engineering</a></li>
<li class="chapter" data-level="2.4.2" data-path="pre-modeling-1.html"><a href="pre-modeling-1.html#feature-selection"><i class="fa fa-check"></i><b>2.4.2</b> Feature Selection</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="model-1.html"><a href="model-1.html"><i class="fa fa-check"></i><b>3</b> Model</a><ul>
<li class="chapter" data-level="3.1" data-path="model-1.html"><a href="model-1.html#general-2"><i class="fa fa-check"></i><b>3.1</b> General</a></li>
<li class="chapter" data-level="3.2" data-path="model-1.html"><a href="model-1.html#model-selectionevaluation"><i class="fa fa-check"></i><b>3.2</b> Model Selection/Evaluation</a></li>
<li class="chapter" data-level="3.3" data-path="model-1.html"><a href="model-1.html#supervised"><i class="fa fa-check"></i><b>3.3</b> Supervised</a><ul>
<li class="chapter" data-level="3.3.1" data-path="model-1.html"><a href="model-1.html#glm"><i class="fa fa-check"></i><b>3.3.1</b> GLM</a></li>
<li class="chapter" data-level="3.3.2" data-path="model-1.html"><a href="model-1.html#generalized-additive-models"><i class="fa fa-check"></i><b>3.3.2</b> Generalized Additive Models</a></li>
<li class="chapter" data-level="3.3.3" data-path="model-1.html"><a href="model-1.html#knn"><i class="fa fa-check"></i><b>3.3.3</b> KNN</a></li>
<li class="chapter" data-level="3.3.4" data-path="model-1.html"><a href="model-1.html#svm"><i class="fa fa-check"></i><b>3.3.4</b> SVM</a></li>
<li class="chapter" data-level="3.3.5" data-path="model-1.html"><a href="model-1.html#decision-tree"><i class="fa fa-check"></i><b>3.3.5</b> Decision Tree</a></li>
<li class="chapter" data-level="3.3.6" data-path="model-1.html"><a href="model-1.html#bagging"><i class="fa fa-check"></i><b>3.3.6</b> Bagging</a></li>
<li class="chapter" data-level="3.3.7" data-path="model-1.html"><a href="model-1.html#random-forest"><i class="fa fa-check"></i><b>3.3.7</b> Random Forest</a></li>
<li class="chapter" data-level="3.3.8" data-path="model-1.html"><a href="model-1.html#boosting"><i class="fa fa-check"></i><b>3.3.8</b> Boosting</a></li>
<li class="chapter" data-level="3.3.9" data-path="model-1.html"><a href="model-1.html#naive-bayes"><i class="fa fa-check"></i><b>3.3.9</b> Naïve Bayes</a></li>
<li class="chapter" data-level="3.3.10" data-path="model-1.html"><a href="model-1.html#lda-qda"><i class="fa fa-check"></i><b>3.3.10</b> LDA &amp; QDA</a></li>
<li class="chapter" data-level="3.3.11" data-path="model-1.html"><a href="model-1.html#gams"><i class="fa fa-check"></i><b>3.3.11</b> GAMs</a></li>
<li class="chapter" data-level="3.3.12" data-path="model-1.html"><a href="model-1.html#hierarchical-models"><i class="fa fa-check"></i><b>3.3.12</b> Hierarchical Models</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="model-1.html"><a href="model-1.html#unsupervised"><i class="fa fa-check"></i><b>3.4</b> Unsupervised</a><ul>
<li class="chapter" data-level="3.4.1" data-path="model-1.html"><a href="model-1.html#k-means-clustering"><i class="fa fa-check"></i><b>3.4.1</b> K-Means Clustering</a></li>
<li class="chapter" data-level="3.4.2" data-path="model-1.html"><a href="model-1.html#hierarchical-clustering"><i class="fa fa-check"></i><b>3.4.2</b> Hierarchical Clustering</a></li>
<li class="chapter" data-level="3.4.3" data-path="model-1.html"><a href="model-1.html#dimensionality-reduction"><i class="fa fa-check"></i><b>3.4.3</b> Dimensionality Reduction</a></li>
<li class="chapter" data-level="3.4.4" data-path="model-1.html"><a href="model-1.html#multiple-correspondence-analysis"><i class="fa fa-check"></i><b>3.4.4</b> Multiple Correspondence Analysis</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="model-1.html"><a href="model-1.html#semi-supervised"><i class="fa fa-check"></i><b>3.5</b> Semi Supervised</a></li>
<li class="chapter" data-level="3.6" data-path="model-1.html"><a href="model-1.html#reinforcement-learning"><i class="fa fa-check"></i><b>3.6</b> Reinforcement Learning</a></li>
<li class="chapter" data-level="3.7" data-path="model-1.html"><a href="model-1.html#other-ml"><i class="fa fa-check"></i><b>3.7</b> Other ML</a><ul>
<li class="chapter" data-level="3.7.1" data-path="model-1.html"><a href="model-1.html#association-rule-mining"><i class="fa fa-check"></i><b>3.7.1</b> Association Rule Mining</a></li>
<li class="chapter" data-level="3.7.2" data-path="model-1.html"><a href="model-1.html#nlp"><i class="fa fa-check"></i><b>3.7.2</b> NLP</a></li>
<li class="chapter" data-level="3.7.3" data-path="model-1.html"><a href="model-1.html#geospatial"><i class="fa fa-check"></i><b>3.7.3</b> Geospatial</a></li>
<li class="chapter" data-level="3.7.4" data-path="model-1.html"><a href="model-1.html#ai"><i class="fa fa-check"></i><b>3.7.4</b> AI</a></li>
<li class="chapter" data-level="3.7.5" data-path="model-1.html"><a href="model-1.html#cv"><i class="fa fa-check"></i><b>3.7.5</b> CV</a></li>
<li class="chapter" data-level="3.7.6" data-path="model-1.html"><a href="model-1.html#online-learning"><i class="fa fa-check"></i><b>3.7.6</b> Online Learning</a></li>
<li class="chapter" data-level="3.7.7" data-path="model-1.html"><a href="model-1.html#sequences"><i class="fa fa-check"></i><b>3.7.7</b> Sequences</a></li>
<li class="chapter" data-level="3.7.8" data-path="model-1.html"><a href="model-1.html#federated-ml"><i class="fa fa-check"></i><b>3.7.8</b> Federated ML</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="communicate-deploy-maintain.html"><a href="communicate-deploy-maintain.html"><i class="fa fa-check"></i><b>4</b> Communicate, Deploy, Maintain</a><ul>
<li class="chapter" data-level="4.1" data-path="communicate-deploy-maintain.html"><a href="communicate-deploy-maintain.html#communicate"><i class="fa fa-check"></i><b>4.1</b> Communicate</a><ul>
<li class="chapter" data-level="4.1.1" data-path="communicate-deploy-maintain.html"><a href="communicate-deploy-maintain.html#outline"><i class="fa fa-check"></i><b>4.1.1</b> Outline</a></li>
<li class="chapter" data-level="4.1.2" data-path="communicate-deploy-maintain.html"><a href="communicate-deploy-maintain.html#other"><i class="fa fa-check"></i><b>4.1.2</b> Other</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="communicate-deploy-maintain.html"><a href="communicate-deploy-maintain.html#maintain"><i class="fa fa-check"></i><b>4.2</b> Maintain</a></li>
<li class="chapter" data-level="4.3" data-path="communicate-deploy-maintain.html"><a href="communicate-deploy-maintain.html#deploy"><i class="fa fa-check"></i><b>4.3</b> Deploy</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="stats.html"><a href="stats.html"><i class="fa fa-check"></i><b>5</b> Stats</a><ul>
<li class="chapter" data-level="5.1" data-path="stats.html"><a href="stats.html#general-3"><i class="fa fa-check"></i><b>5.1</b> General</a></li>
<li class="chapter" data-level="5.2" data-path="stats.html"><a href="stats.html#sample-size"><i class="fa fa-check"></i><b>5.2</b> Sample Size</a></li>
<li class="chapter" data-level="5.3" data-path="stats.html"><a href="stats.html#hypothesis-testing"><i class="fa fa-check"></i><b>5.3</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="5.4" data-path="stats.html"><a href="stats.html#ab-testing"><i class="fa fa-check"></i><b>5.4</b> AB Testing</a></li>
<li class="chapter" data-level="5.5" data-path="stats.html"><a href="stats.html#experimental-design"><i class="fa fa-check"></i><b>5.5</b> Experimental Design</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="sequences-1.html"><a href="sequences-1.html"><i class="fa fa-check"></i><b>6</b> Sequences</a><ul>
<li class="chapter" data-level="6.1" data-path="sequences-1.html"><a href="sequences-1.html#time-series"><i class="fa fa-check"></i><b>6.1</b> Time Series</a></li>
<li class="chapter" data-level="6.2" data-path="sequences-1.html"><a href="sequences-1.html#toolkit"><i class="fa fa-check"></i><b>6.2</b> Toolkit</a><ul>
<li class="chapter" data-level="6.2.1" data-path="sequences-1.html"><a href="sequences-1.html#notes-1"><i class="fa fa-check"></i><b>6.2.1</b> Notes</a></li>
<li class="chapter" data-level="6.2.2" data-path="sequences-1.html"><a href="sequences-1.html#workflow-1"><i class="fa fa-check"></i><b>6.2.2</b> Workflow</a></li>
<li class="chapter" data-level="6.2.3" data-path="sequences-1.html"><a href="sequences-1.html#preprocessing"><i class="fa fa-check"></i><b>6.2.3</b> Preprocessing</a></li>
<li class="chapter" data-level="6.2.4" data-path="sequences-1.html"><a href="sequences-1.html#feature-engineering-1"><i class="fa fa-check"></i><b>6.2.4</b> Feature Engineering</a></li>
<li class="chapter" data-level="6.2.5" data-path="sequences-1.html"><a href="sequences-1.html#packages-functions"><i class="fa fa-check"></i><b>6.2.5</b> Packages / Functions</a></li>
<li class="chapter" data-level="6.2.6" data-path="sequences-1.html"><a href="sequences-1.html#models"><i class="fa fa-check"></i><b>6.2.6</b> Models</a></li>
<li class="chapter" data-level="6.2.7" data-path="sequences-1.html"><a href="sequences-1.html#model-selection"><i class="fa fa-check"></i><b>6.2.7</b> Model Selection</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="sequences-1.html"><a href="sequences-1.html#dsp"><i class="fa fa-check"></i><b>6.3</b> DSP</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="deep-learning.html"><a href="deep-learning.html"><i class="fa fa-check"></i><b>7</b> Deep Learning</a><ul>
<li class="chapter" data-level="7.1" data-path="deep-learning.html"><a href="deep-learning.html#general-6"><i class="fa fa-check"></i><b>7.1</b> General</a></li>
<li class="chapter" data-level="7.2" data-path="deep-learning.html"><a href="deep-learning.html#pre-processing-1"><i class="fa fa-check"></i><b>7.2</b> Pre-processing</a></li>
<li class="chapter" data-level="7.3" data-path="deep-learning.html"><a href="deep-learning.html#defaults"><i class="fa fa-check"></i><b>7.3</b> Defaults</a></li>
<li class="chapter" data-level="7.4" data-path="deep-learning.html"><a href="deep-learning.html#rnnlstm"><i class="fa fa-check"></i><b>7.4</b> RNN/LSTM</a></li>
<li class="chapter" data-level="7.5" data-path="deep-learning.html"><a href="deep-learning.html#tuning"><i class="fa fa-check"></i><b>7.5</b> Tuning</a></li>
<li class="chapter" data-level="7.6" data-path="deep-learning.html"><a href="deep-learning.html#debugging"><i class="fa fa-check"></i><b>7.6</b> Debugging</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="probabilistic-programming.html"><a href="probabilistic-programming.html"><i class="fa fa-check"></i><b>8</b> Probabilistic Programming</a><ul>
<li class="chapter" data-level="8.1" data-path="probabilistic-programming.html"><a href="probabilistic-programming.html#bayes-rule"><i class="fa fa-check"></i><b>8.1</b> Bayes Rule</a></li>
<li class="chapter" data-level="8.2" data-path="probabilistic-programming.html"><a href="probabilistic-programming.html#other-2"><i class="fa fa-check"></i><b>8.2</b> Other</a></li>
<li class="chapter" data-level="8.3" data-path="probabilistic-programming.html"><a href="probabilistic-programming.html#doing-bayesian-da"><i class="fa fa-check"></i><b>8.3</b> Doing Bayesian DA</a></li>
<li class="chapter" data-level="8.4" data-path="probabilistic-programming.html"><a href="probabilistic-programming.html#statistical-rethinking"><i class="fa fa-check"></i><b>8.4</b> Statistical Rethinking</a></li>
<li class="chapter" data-level="8.5" data-path="probabilistic-programming.html"><a href="probabilistic-programming.html#causality"><i class="fa fa-check"></i><b>8.5</b> Causality</a><ul>
<li class="chapter" data-level="8.5.1" data-path="probabilistic-programming.html"><a href="probabilistic-programming.html#general-7"><i class="fa fa-check"></i><b>8.5.1</b> General</a></li>
<li class="chapter" data-level="8.5.2" data-path="probabilistic-programming.html"><a href="probabilistic-programming.html#causality-edx"><i class="fa fa-check"></i><b>8.5.2</b> Causality edX</a></li>
<li class="chapter" data-level="8.5.3" data-path="probabilistic-programming.html"><a href="probabilistic-programming.html#causality-coursera"><i class="fa fa-check"></i><b>8.5.3</b> Causality Coursera</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Science Cribsheet</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="deep-learning" class="section level1">
<h1><span class="header-section-number">Chapter 7</span> Deep Learning</h1>
<div id="general-6" class="section level2">
<h2><span class="header-section-number">7.1</span> General</h2>
<p><a href="https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-deep-learning">Cheatsheet</a></p>
<p><a href="http://www.wildml.com/deep-learning-glossary/">Glossary</a></p>
<p>Batch: Subset of the data used to calculate slopes during back propagation. Different batches are used to calculate different updates.</p>
<p>Epoch: One full pass through all the batches in the training data.</p>
<p>Activation Functions: Sigmoid: Takes a real-valued input and squashes it to range between 0 and 1. Softmax: Same end result as sigmoid, but different function. tanh: Takes a real-valued input and squashes it to the range [-1, 1]. ReLU:Has been shown to lead to very high-performance networks. This function takes a single number as an input, returning 0 if the input is negative, and the input if the input is positive.</p>
<p>Embeddings vs One-Hot Encoding: Embeddings are better than One-Hot Encodings because it allows for relationships to be shown between days.</p>
<p><a href="https://www.youtube.com/watch?v=nqEYVzJLR_c&amp;feature=youtu.be&amp;t=31">Rule of 30</a>: A change that affects at least 30 data points in your validation set is usually significant and not just noise.</p>
<p>In a single-label, multiclass classification problem, your network should end with a softmax activation so that it will output a probability distribution over the N output classes.</p>
<p>If you’re predicting N categories, don’t have layers with less than N nodes.</p>
<p>Softmax regression (or multinomial logistic regression) is a generalization of logistic regression to the case where we want to handle multiple classes.</p>
<p>Few people use kfold cv in deep learning because of the large datasets in play.</p>
<p>n inputs and k outputs gives you <span class="math inline">\((n+1)*k\)</span> parameters.</p>
<p>Do random initialization of weights but set bias to zero. Don’t intialize to values that are too large. He initialization works well for networks with ReLU activations.</p>
<p>end to end deep learning: an end-to-end approach as it maps directly the input (x) to the output (y). end-to-end learning works better in practice than multi-task learning, but requires a large amount of data.</p>
<p>restnet/attention &gt; RNN / LSTM</p>
<p>With a reduced the training set a high batch size and high learning rate cause the model to jump around chaotically.</p>
<p>In backpropagation all derivatives applied to same parameters, so they’re very correlated. This is bad for stochastic gradient descent. Gradients are either zero (don’t remember the past well) or infinity. The latter is fixed by gradient clipping and the former by using LSTMs.</p>
</div>
<div id="pre-processing-1" class="section level2">
<h2><span class="header-section-number">7.2</span> Pre-processing</h2>
<p>Normalization makes the cost function faster to optimize</p>
<p>One common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel).</p>
<p>Scale images: train_set_x = train_set_x_flatten/255.</p>
<p>Common steps for pre-processing a new dataset are: 1) Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, …), 2) Reshape the datasets such that each example is now a vector of size (num_px * num_px * 3, 1), 3) Standardize the data</p>
</div>
<div id="defaults" class="section level2">
<h2><span class="header-section-number">7.3</span> Defaults</h2>
<p>Regression: loss - mean_squared_error, metric - rmse, activation_function - relu</p>
<p>Classification: loss - categorical_crossentropy, metric - accuracy, activation_function - softmax</p>
</div>
<div id="rnnlstm" class="section level2">
<h2><span class="header-section-number">7.4</span> RNN/LSTM</h2>
<p>stateful vs stateless LSTMs</p>
<p>One major issue with layer_simple_rnn is that although it should theoretically be able to retain at time t information about inputs seen many time steps before, in practice, such long-term dependencies are impossible to learn. This is due to the vanishing gradient problem, an effect that is similar to what is observed with non-recurrent networks (feed forward networks) that are many layers deep: as you keep adding layers to a network, the network eventually becomes un-trainable.</p>
<p>The default activation function for LSTMs is the hyperbolic tangent (tanh), which outputs values between -1 and 1. This is the preferred range for the time series data: MinMaxScaler(feature_range = (-1, 1)).</p>
<p>Keras’ LSTM expects the input as well as the target data to be in a specific shape. The input has to be a 3-d array of size num_samples, num_timesteps, num_features.</p>
<p>Wiring the LSTM hidden states to sets of consecutive outputs of the same length. For example, if we want to produce predictions for 12 months, our LSTM should have a hidden state length of 12.</p>
<p>A loss function is used to optimize a machine learning algorithm. An accuracy metric is used to measure the algorithm’s performance (accuracy) in an interpretable way. Loss function applies to a single training example while the cost function applies to many. The loss function computes the error for a single training example; the cost function is the average of the loss functions of the entire training set.</p>
<p>A trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b ∗ c ∗ d, a) is to use <code>X_flatten = X.reshape(X.shape[0], -1).T</code></p>
<p>The tanh activation usually works better than sigmoid activation function for hidden units because the mean of its output is closer to zero, and so it centers the data better for the next layer.</p>
<p>Sigmoid outputs a value between 0 and 1 which makes it a very good choice for binary classification. You can classify as 0 if the output is less than 0.5 and classify as 1 if the output is more than 0.5. It can be done with tanh as well but it is less convenient as the output is between -1 and 1.</p>
<p>The deeper layers of a neural network are typically computing more complex features of the input than the earlier layers.</p>
<p>data augmentation for images: rotate and crop</p>
<p>If you have 10,000,000 examples, how would you split the train/dev/test set? 98% train . 1% dev . 1% test. The dev and test set should come from the same distribution</p>
<p>What is weight decay? A regularization technique (such as L2 regularization) that results in gradient descent shrinking the weights on every iteration. As a rule of thumb, the more training examples you have, the weaker this term should be. The more parameters you have the higher this term should be.</p>
<p>Techniques for reducing variance (reducing overfitting)? Dropout, L2 regularization, Data augmentation</p>
<p>Do a loss vs learning rate plot to pick a good one. It’s the key thing to set. Learning rate annealing: decrease learning rate as you reach the minimum.</p>
<p>Training for more epochs will lead to overfitting.</p>
<p>Number of params in logistic regression: k multiplied by d where k is the number of classes and d is the size of the dimensional space.</p>
<p>Large model weights can indicate that model is overfitted</p>
<p>Gradient descent extensions: adagrad, rms prop, adam, mini-batch gradient descent</p>
<p>A dense layer applies a linear transformation to its input</p>
<p>Always use ReLU activation because it doesn’t saturate and it converges faster. Use He et al initialization which is square root of two divided by square root of number of inputs.</p>
<p>batch normalization</p>
</div>
<div id="tuning" class="section level2">
<h2><span class="header-section-number">7.5</span> Tuning</h2>
<p><strong>Optimizers</strong>: If batch gradient descent in a deep network is taking excessively long to find a value of the parameters that achieves a small value for the cost function: Try mini-batch gradient descent, Try better random initialization for the weights, Try Adam, Try tuning the learning rate. A variant of this is Stochastic Gradient Descent (SGD), which is equivalent to mini-batch gradient descent where each mini-batch has just 1 example. When the training set is large, SGD can be faster. But the parameters will “oscillate” toward the minimum rather than converge smoothly. In practice, you’ll often get faster results if you do not use neither the whole training set, nor only one training example, to perform each update. Mini-batch gradient descent uses an intermediate number of examples for each step. With mini-batch gradient descent, you loop over the mini-batches instead of looping over individual training examples. The difference between gradient descent, mini-batch gradient descent and stochastic gradient descent is the number of examples you use to perform one update step. You have to tune a learning rate hyperparameter alpha. With a well-turned mini-batch size, usually it outperforms either gradient descent or stochastic gradient descent (particularly when the training set is large). Shuffling and Partitioning are the two steps required to build mini-batches. Powers of two are often chosen to be the mini-batch size, e.g., 16, 32, 64, 128. Mini-batch Gradient descent, Mini-batch Gradient descent with momentum, mini-batch with Adam. Adam usually performs best. Some advantages of Adam include: Relatively low memory requirements (though higher than gradient descent and gradient descent with momentum), usually works well even with little tuning of hyperparameters (except alpha).</p>
<p><strong>early stopping/termination</strong>: (stop as soon as performance on validation set drops after a certain number of epochs) and regularization which applies artifical constraints to reduce the number of free parameters while making it difficult to optimize. Use L2 regularization or dropout.</p>
<p><strong>Dropout</strong>. Slow down learning with regularization methods like dropout on the recurrent LSTM connections. Dropout works by randomly setting activations from one layer to the next to 0 repeatly. Forces the network to learn redundant representations, and takes average consensus for final prediction. Makes things more robust and prevents overfitting. You should use dropout (randomly eliminate nodes) only in training. Apply dropout both during forward and backward propagation. During training time, divide each dropout layer by keep_prob to keep the same expected value for the activations. For example, if keep_prob is 0.5, then we will on average shut down half the nodes, so the output will be scaled by 0.5 since only the remaining half are contributing to the solution. Dividing by 0.5 is equivalent to multiplying by 2. Hence, the output now has the same expected value. You can check that this works even when keep_prob is other values than 0.5. Also consider inverted dropout. Andrew Ng doesn’t like dropout bc it couples optimization with regularization</p>
<p><strong>Inverted dropout</strong>: At test time you do not apply dropout (do not randomly eliminate units) and do not keep the 1/keep_prob factor in the calculations used in training.</p>
<p><strong>Layers</strong>. Explore additional hierarchical learning capacity by adding more layers and varied numbers of neurons in each layer.</p>
<p><strong>Regularization</strong>. Explore how weight regularization, such as L1 and L2, can be used to slow down learning and overfitting of the network on some configurations.</p>
<p><strong>Optimization Algorithm</strong>. Explore the use of <a href="https://keras.io/optimizers/">alternate optimization algorithms</a>, such as classical gradient descent, to see if specific configurations to speed up or slow down learning can lead to benefits.</p>
<p><strong>Loss Function</strong>. Explore the use of <a href="https://keras.io/objectives/">alternative loss functions</a> to see if these can be used to lift performance.</p>
<p><strong>Features and Timesteps</strong>. Explore the use of lag observations as input features and input time steps of the feature to see if their presence as input can improve learning and/or predictive capability of the model.</p>
<p><strong>Larger Batch Size</strong>. Explore larger batch sizes than 4, perhaps requiring further manipulation of the size of the training and test datasets. Don’t use mini-batches larger than 32. Training with large mini-batches is bad for your test error.</p>
<p><strong>Change Learning Rate</strong>: Reducing the learning rate can improve results. Use different learning rates for each layer</p>
<p><strong>Model Capacity (Bias-Variance)</strong>: If your Neural Network model seems to have high variance try to add regularization &amp; get more training data. For high bias, make a bigger and deeper network. In general, the less training data you have, the worse overfitting will be, and using a small network is one way to mitigate overfitting. Less nodes and hidden layers corresponds to simpler model and vice versa.</p>
<p>Try to add batch normalization or dropout, maybe it will converge better. Try to augment your training data.</p>
<p>ADAGRAD is another optimization option that takes care of some of the hyperparameters for you.</p>
</div>
<div id="debugging" class="section level2">
<h2><span class="header-section-number">7.6</span> Debugging</h2>
<p><a href="https://karpathy.github.io/2019/04/25/recipe/" class="uri">https://karpathy.github.io/2019/04/25/recipe/</a></p>
<p><a href="http://theorangeduck.com/page/neural-network-not-working">1</a></p>
<p><a href="https://towardsdatascience.com/checklist-for-debugging-neural-networks-d8b2a9434f21">2</a></p>
<p>gradient checking</p>
<p>Sigmoid neurons can lead to vanshing gradients at its extremes. ReLU is better and Leaky ReLU is the best to avoid non-activation.</p>
<p>dying relu</p>
<p>Gradient checking verifies closeness between the gradients from backpropagation and the numerical approximation of the gradient (computed using forward propagation). Gradient checking is slow, so we don’t run it in every iteration of training. You would usually run it only to make sure your code is correct, then turn it off and use backprop for the actual learning process. In batch GD the cost should be monotone decreasing. In mini-batch it doesn’t have to be since it’s like training on a different training set every time. It should still be trending downwards. For small training set use ( &lt; 2000) use batch GD. Use batches of 64, 128, 256, 512 otherwise which should speed up training. Make sure it fits in meemory. MIni-batch size is another hyperparameter.</p>
<p>Vanishing gradient problem: the gradients of the network’s output with respect to the parameters in the early layers become extremely small. That’s a fancy way of saying that even a large change in the value of parameters for the early layers doesn’t have a big effect on the output. Change the activation function to solve.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sequences-1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="probabilistic-programming.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/06-deep_learning.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
