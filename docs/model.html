<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Data Science Cribsheeet</title>
  <meta name="description" content="A collection of quick notes on the field.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Data Science Cribsheeet" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A collection of quick notes on the field." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Data Science Cribsheeet" />
  
  <meta name="twitter:description" content="A collection of quick notes on the field." />
  




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="transform-visualize.html">
<link rel="next" href="udacity-ts-fundamentals.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">DS Cribsheet</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Workflow</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#preamble"><i class="fa fa-check"></i><b>1.1</b> Preamble</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#annotated-checklist"><i class="fa fa-check"></i><b>1.2</b> Annotated Checklist</a><ul>
<li class="chapter" data-level="1.2.1" data-path="index.html"><a href="index.html#preparation"><i class="fa fa-check"></i><b>1.2.1</b> Preparation</a></li>
<li class="chapter" data-level="1.2.2" data-path="index.html"><a href="index.html#import"><i class="fa fa-check"></i><b>1.2.2</b> Import</a></li>
<li class="chapter" data-level="1.2.3" data-path="index.html"><a href="index.html#tidy"><i class="fa fa-check"></i><b>1.2.3</b> Tidy</a></li>
<li class="chapter" data-level="1.2.4" data-path="index.html"><a href="index.html#utransform"><i class="fa fa-check"></i><b>1.2.4</b> uTransform</a></li>
<li class="chapter" data-level="1.2.5" data-path="index.html"><a href="index.html#uvisualize"><i class="fa fa-check"></i><b>1.2.5</b> uVisualize</a></li>
<li class="chapter" data-level="1.2.6" data-path="index.html"><a href="index.html#umodel"><i class="fa fa-check"></i><b>1.2.6</b> uModel</a></li>
<li class="chapter" data-level="1.2.7" data-path="index.html"><a href="index.html#communicate"><i class="fa fa-check"></i><b>1.2.7</b> Communicate</a></li>
<li class="chapter" data-level="1.2.8" data-path="index.html"><a href="index.html#deployment-maintenance"><i class="fa fa-check"></i><b>1.2.8</b> Deployment / Maintenance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="import-tidy.html"><a href="import-tidy.html"><i class="fa fa-check"></i><b>2</b> Import &amp; Tidy</a><ul>
<li class="chapter" data-level="2.1" data-path="import-tidy.html"><a href="import-tidy.html#general"><i class="fa fa-check"></i><b>2.1</b> General</a></li>
<li class="chapter" data-level="2.2" data-path="import-tidy.html"><a href="import-tidy.html#sql"><i class="fa fa-check"></i><b>2.2</b> SQL</a></li>
<li class="chapter" data-level="2.3" data-path="import-tidy.html"><a href="import-tidy.html#scraping"><i class="fa fa-check"></i><b>2.3</b> Scraping</a></li>
<li class="chapter" data-level="2.4" data-path="import-tidy.html"><a href="import-tidy.html#exploration"><i class="fa fa-check"></i><b>2.4</b> Exploration</a></li>
<li class="chapter" data-level="2.5" data-path="import-tidy.html"><a href="import-tidy.html#general-1"><i class="fa fa-check"></i><b>2.5</b> General</a></li>
<li class="chapter" data-level="2.6" data-path="import-tidy.html"><a href="import-tidy.html#missingness-imputation"><i class="fa fa-check"></i><b>2.6</b> Missingness &amp; Imputation</a></li>
<li class="chapter" data-level="2.7" data-path="import-tidy.html"><a href="import-tidy.html#data-table"><i class="fa fa-check"></i><b>2.7</b> Data Table</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="transform-visualize.html"><a href="transform-visualize.html"><i class="fa fa-check"></i><b>3</b> Transform &amp; Visualize</a><ul>
<li class="chapter" data-level="3.1" data-path="transform-visualize.html"><a href="transform-visualize.html#transform"><i class="fa fa-check"></i><b>3.1</b> Transform</a></li>
<li class="chapter" data-level="3.2" data-path="transform-visualize.html"><a href="transform-visualize.html#visualize"><i class="fa fa-check"></i><b>3.2</b> Visualize</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="model.html"><a href="model.html"><i class="fa fa-check"></i><b>4</b> Model</a><ul>
<li class="chapter" data-level="4.1" data-path="model.html"><a href="model.html#pre-processing"><i class="fa fa-check"></i><b>4.1</b> Pre-processing</a></li>
<li class="chapter" data-level="4.2" data-path="model.html"><a href="model.html#feature-engineering"><i class="fa fa-check"></i><b>4.2</b> Feature Engineering</a></li>
<li class="chapter" data-level="4.3" data-path="model.html"><a href="model.html#feature-selection"><i class="fa fa-check"></i><b>4.3</b> Feature Selection</a></li>
<li class="chapter" data-level="4.4" data-path="model.html"><a href="model.html#linear-regression"><i class="fa fa-check"></i><b>4.4</b> Linear Regression</a><ul>
<li class="chapter" data-level="4.4.1" data-path="model.html"><a href="model.html#intro"><i class="fa fa-check"></i><b>4.4.1</b> Intro</a></li>
<li class="chapter" data-level="4.4.2" data-path="model.html"><a href="model.html#assumptions"><i class="fa fa-check"></i><b>4.4.2</b> Assumptions</a></li>
<li class="chapter" data-level="4.4.3" data-path="model.html"><a href="model.html#evaluation"><i class="fa fa-check"></i><b>4.4.3</b> Evaluation</a></li>
<li class="chapter" data-level="4.4.4" data-path="model.html"><a href="model.html#proscons"><i class="fa fa-check"></i><b>4.4.4</b> Pros/Cons</a></li>
<li class="chapter" data-level="4.4.5" data-path="model.html"><a href="model.html#other"><i class="fa fa-check"></i><b>4.4.5</b> Other</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="model.html"><a href="model.html#ridgelasso"><i class="fa fa-check"></i><b>4.5</b> Ridge/Lasso</a><ul>
<li class="chapter" data-level="4.5.1" data-path="model.html"><a href="model.html#intro-1"><i class="fa fa-check"></i><b>4.5.1</b> Intro</a></li>
<li class="chapter" data-level="4.5.2" data-path="model.html"><a href="model.html#assumptions-1"><i class="fa fa-check"></i><b>4.5.2</b> Assumptions</a></li>
<li class="chapter" data-level="4.5.3" data-path="model.html"><a href="model.html#evaluation-1"><i class="fa fa-check"></i><b>4.5.3</b> Evaluation</a></li>
<li class="chapter" data-level="4.5.4" data-path="model.html"><a href="model.html#proscons-1"><i class="fa fa-check"></i><b>4.5.4</b> Pros/Cons</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="model.html"><a href="model.html#logistic-regression"><i class="fa fa-check"></i><b>4.6</b> Logistic Regression</a><ul>
<li class="chapter" data-level="4.6.1" data-path="model.html"><a href="model.html#intro-2"><i class="fa fa-check"></i><b>4.6.1</b> Intro</a></li>
<li class="chapter" data-level="4.6.2" data-path="model.html"><a href="model.html#assumptions-2"><i class="fa fa-check"></i><b>4.6.2</b> Assumptions</a></li>
<li class="chapter" data-level="4.6.3" data-path="model.html"><a href="model.html#evaluation-2"><i class="fa fa-check"></i><b>4.6.3</b> Evaluation</a></li>
<li class="chapter" data-level="4.6.4" data-path="model.html"><a href="model.html#proscons-2"><i class="fa fa-check"></i><b>4.6.4</b> Pros/Cons</a></li>
<li class="chapter" data-level="4.6.5" data-path="model.html"><a href="model.html#code-example"><i class="fa fa-check"></i><b>4.6.5</b> Code Example</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="model.html"><a href="model.html#poisson-regression"><i class="fa fa-check"></i><b>4.7</b> Poisson Regression</a><ul>
<li class="chapter" data-level="4.7.1" data-path="model.html"><a href="model.html#intro-3"><i class="fa fa-check"></i><b>4.7.1</b> Intro</a></li>
<li class="chapter" data-level="4.7.2" data-path="model.html"><a href="model.html#assumptions-3"><i class="fa fa-check"></i><b>4.7.2</b> Assumptions</a></li>
<li class="chapter" data-level="4.7.3" data-path="model.html"><a href="model.html#evaluation-3"><i class="fa fa-check"></i><b>4.7.3</b> Evaluation</a></li>
<li class="chapter" data-level="4.7.4" data-path="model.html"><a href="model.html#proscons-3"><i class="fa fa-check"></i><b>4.7.4</b> Pros/Cons</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="model.html"><a href="model.html#generalized-additive-models"><i class="fa fa-check"></i><b>4.8</b> Generalized Additive Models</a><ul>
<li class="chapter" data-level="4.8.1" data-path="model.html"><a href="model.html#intro-4"><i class="fa fa-check"></i><b>4.8.1</b> Intro</a></li>
<li class="chapter" data-level="4.8.2" data-path="model.html"><a href="model.html#assumptions-4"><i class="fa fa-check"></i><b>4.8.2</b> Assumptions</a></li>
<li class="chapter" data-level="4.8.3" data-path="model.html"><a href="model.html#evaluation-4"><i class="fa fa-check"></i><b>4.8.3</b> Evaluation</a></li>
<li class="chapter" data-level="4.8.4" data-path="model.html"><a href="model.html#proscons-4"><i class="fa fa-check"></i><b>4.8.4</b> Pros/Cons</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="model.html"><a href="model.html#bayesian"><i class="fa fa-check"></i><b>4.9</b> Bayesian</a></li>
<li class="chapter" data-level="4.10" data-path="model.html"><a href="model.html#naive-bayes"><i class="fa fa-check"></i><b>4.10</b> Naïve Bayes</a><ul>
<li class="chapter" data-level="4.10.1" data-path="model.html"><a href="model.html#intro-5"><i class="fa fa-check"></i><b>4.10.1</b> Intro</a></li>
<li class="chapter" data-level="4.10.2" data-path="model.html"><a href="model.html#assumptions-5"><i class="fa fa-check"></i><b>4.10.2</b> Assumptions</a></li>
<li class="chapter" data-level="4.10.3" data-path="model.html"><a href="model.html#characteristics"><i class="fa fa-check"></i><b>4.10.3</b> Characteristics</a></li>
<li class="chapter" data-level="4.10.4" data-path="model.html"><a href="model.html#evaluation-5"><i class="fa fa-check"></i><b>4.10.4</b> Evaluation</a></li>
<li class="chapter" data-level="4.10.5" data-path="model.html"><a href="model.html#proscons-5"><i class="fa fa-check"></i><b>4.10.5</b> Pros/Cons</a></li>
</ul></li>
<li class="chapter" data-level="4.11" data-path="model.html"><a href="model.html#lda"><i class="fa fa-check"></i><b>4.11</b> LDA</a><ul>
<li class="chapter" data-level="4.11.1" data-path="model.html"><a href="model.html#intro-6"><i class="fa fa-check"></i><b>4.11.1</b> Intro</a></li>
<li class="chapter" data-level="4.11.2" data-path="model.html"><a href="model.html#assumptions-6"><i class="fa fa-check"></i><b>4.11.2</b> Assumptions</a></li>
<li class="chapter" data-level="4.11.3" data-path="model.html"><a href="model.html#characteristics-1"><i class="fa fa-check"></i><b>4.11.3</b> Characteristics</a></li>
<li class="chapter" data-level="4.11.4" data-path="model.html"><a href="model.html#evaluation-6"><i class="fa fa-check"></i><b>4.11.4</b> Evaluation</a></li>
<li class="chapter" data-level="4.11.5" data-path="model.html"><a href="model.html#proscons-6"><i class="fa fa-check"></i><b>4.11.5</b> Pros/Cons</a></li>
</ul></li>
<li class="chapter" data-level="4.12" data-path="model.html"><a href="model.html#qda"><i class="fa fa-check"></i><b>4.12</b> QDA</a><ul>
<li class="chapter" data-level="4.12.1" data-path="model.html"><a href="model.html#intro-7"><i class="fa fa-check"></i><b>4.12.1</b> Intro</a></li>
<li class="chapter" data-level="4.12.2" data-path="model.html"><a href="model.html#assumptions-7"><i class="fa fa-check"></i><b>4.12.2</b> Assumptions</a></li>
<li class="chapter" data-level="4.12.3" data-path="model.html"><a href="model.html#characteristics-2"><i class="fa fa-check"></i><b>4.12.3</b> Characteristics</a></li>
<li class="chapter" data-level="4.12.4" data-path="model.html"><a href="model.html#evaluation-7"><i class="fa fa-check"></i><b>4.12.4</b> Evaluation</a></li>
<li class="chapter" data-level="4.12.5" data-path="model.html"><a href="model.html#proscons-7"><i class="fa fa-check"></i><b>4.12.5</b> Pros/Cons</a></li>
</ul></li>
<li class="chapter" data-level="4.13" data-path="model.html"><a href="model.html#advanced-bayesian"><i class="fa fa-check"></i><b>4.13</b> Advanced Bayesian</a></li>
<li class="chapter" data-level="4.14" data-path="model.html"><a href="model.html#clustering"><i class="fa fa-check"></i><b>4.14</b> Clustering</a></li>
<li class="chapter" data-level="4.15" data-path="model.html"><a href="model.html#k-means"><i class="fa fa-check"></i><b>4.15</b> K-Means</a><ul>
<li class="chapter" data-level="4.15.1" data-path="model.html"><a href="model.html#intro-8"><i class="fa fa-check"></i><b>4.15.1</b> Intro</a></li>
<li class="chapter" data-level="4.15.2" data-path="model.html"><a href="model.html#assumptions-8"><i class="fa fa-check"></i><b>4.15.2</b> Assumptions</a></li>
<li class="chapter" data-level="4.15.3" data-path="model.html"><a href="model.html#characteristics-3"><i class="fa fa-check"></i><b>4.15.3</b> Characteristics</a></li>
<li class="chapter" data-level="4.15.4" data-path="model.html"><a href="model.html#evaluation-8"><i class="fa fa-check"></i><b>4.15.4</b> Evaluation</a></li>
<li class="chapter" data-level="4.15.5" data-path="model.html"><a href="model.html#proscons-8"><i class="fa fa-check"></i><b>4.15.5</b> Pros/Cons</a></li>
</ul></li>
<li class="chapter" data-level="4.16" data-path="model.html"><a href="model.html#hierarchical-clustering"><i class="fa fa-check"></i><b>4.16</b> Hierarchical Clustering</a><ul>
<li class="chapter" data-level="4.16.1" data-path="model.html"><a href="model.html#intro-9"><i class="fa fa-check"></i><b>4.16.1</b> Intro</a></li>
<li class="chapter" data-level="4.16.2" data-path="model.html"><a href="model.html#assumptions-9"><i class="fa fa-check"></i><b>4.16.2</b> Assumptions</a></li>
<li class="chapter" data-level="4.16.3" data-path="model.html"><a href="model.html#characteristics-4"><i class="fa fa-check"></i><b>4.16.3</b> Characteristics</a></li>
<li class="chapter" data-level="4.16.4" data-path="model.html"><a href="model.html#evaluation-9"><i class="fa fa-check"></i><b>4.16.4</b> Evaluation</a></li>
<li class="chapter" data-level="4.16.5" data-path="model.html"><a href="model.html#proscons-9"><i class="fa fa-check"></i><b>4.16.5</b> Pros/Cons</a></li>
</ul></li>
<li class="chapter" data-level="4.17" data-path="model.html"><a href="model.html#pca"><i class="fa fa-check"></i><b>4.17</b> PCA</a><ul>
<li class="chapter" data-level="4.17.1" data-path="model.html"><a href="model.html#intro-10"><i class="fa fa-check"></i><b>4.17.1</b> Intro</a></li>
<li class="chapter" data-level="4.17.2" data-path="model.html"><a href="model.html#assumptions-10"><i class="fa fa-check"></i><b>4.17.2</b> Assumptions</a></li>
<li class="chapter" data-level="4.17.3" data-path="model.html"><a href="model.html#characteristics-5"><i class="fa fa-check"></i><b>4.17.3</b> Characteristics</a></li>
<li class="chapter" data-level="4.17.4" data-path="model.html"><a href="model.html#evaluation-10"><i class="fa fa-check"></i><b>4.17.4</b> Evaluation</a></li>
<li class="chapter" data-level="4.17.5" data-path="model.html"><a href="model.html#proscons-10"><i class="fa fa-check"></i><b>4.17.5</b> Pros/Cons</a></li>
<li class="chapter" data-level="4.17.6" data-path="model.html"><a href="model.html#other-1"><i class="fa fa-check"></i><b>4.17.6</b> Other</a></li>
</ul></li>
<li class="chapter" data-level="4.18" data-path="model.html"><a href="model.html#knn"><i class="fa fa-check"></i><b>4.18</b> KNN</a><ul>
<li class="chapter" data-level="4.18.1" data-path="model.html"><a href="model.html#intro-11"><i class="fa fa-check"></i><b>4.18.1</b> Intro</a></li>
<li class="chapter" data-level="4.18.2" data-path="model.html"><a href="model.html#assumptions-11"><i class="fa fa-check"></i><b>4.18.2</b> Assumptions</a></li>
<li class="chapter" data-level="4.18.3" data-path="model.html"><a href="model.html#characteristics-6"><i class="fa fa-check"></i><b>4.18.3</b> Characteristics</a></li>
<li class="chapter" data-level="4.18.4" data-path="model.html"><a href="model.html#evaluation-11"><i class="fa fa-check"></i><b>4.18.4</b> Evaluation</a></li>
<li class="chapter" data-level="4.18.5" data-path="model.html"><a href="model.html#proscons-11"><i class="fa fa-check"></i><b>4.18.5</b> Pros/Cons</a></li>
</ul></li>
<li class="chapter" data-level="4.19" data-path="model.html"><a href="model.html#svm"><i class="fa fa-check"></i><b>4.19</b> SVM</a><ul>
<li class="chapter" data-level="4.19.1" data-path="model.html"><a href="model.html#intro-12"><i class="fa fa-check"></i><b>4.19.1</b> Intro</a></li>
<li class="chapter" data-level="4.19.2" data-path="model.html"><a href="model.html#assumptions-12"><i class="fa fa-check"></i><b>4.19.2</b> Assumptions</a></li>
<li class="chapter" data-level="4.19.3" data-path="model.html"><a href="model.html#characteristics-7"><i class="fa fa-check"></i><b>4.19.3</b> Characteristics</a></li>
<li class="chapter" data-level="4.19.4" data-path="model.html"><a href="model.html#evaluation-12"><i class="fa fa-check"></i><b>4.19.4</b> Evaluation</a></li>
<li class="chapter" data-level="4.19.5" data-path="model.html"><a href="model.html#proscons-12"><i class="fa fa-check"></i><b>4.19.5</b> Pros/Cons</a></li>
</ul></li>
<li class="chapter" data-level="4.20" data-path="model.html"><a href="model.html#decision-tree"><i class="fa fa-check"></i><b>4.20</b> Decision Tree</a><ul>
<li class="chapter" data-level="4.20.1" data-path="model.html"><a href="model.html#intro-13"><i class="fa fa-check"></i><b>4.20.1</b> Intro</a></li>
<li class="chapter" data-level="4.20.2" data-path="model.html"><a href="model.html#assumptions-13"><i class="fa fa-check"></i><b>4.20.2</b> Assumptions</a></li>
<li class="chapter" data-level="4.20.3" data-path="model.html"><a href="model.html#characteristics-8"><i class="fa fa-check"></i><b>4.20.3</b> Characteristics</a></li>
<li class="chapter" data-level="4.20.4" data-path="model.html"><a href="model.html#evaluation-13"><i class="fa fa-check"></i><b>4.20.4</b> Evaluation</a></li>
<li class="chapter" data-level="4.20.5" data-path="model.html"><a href="model.html#proscons-13"><i class="fa fa-check"></i><b>4.20.5</b> Pros/Cons</a></li>
</ul></li>
<li class="chapter" data-level="4.21" data-path="model.html"><a href="model.html#bagging"><i class="fa fa-check"></i><b>4.21</b> Bagging</a><ul>
<li class="chapter" data-level="4.21.1" data-path="model.html"><a href="model.html#intro-14"><i class="fa fa-check"></i><b>4.21.1</b> Intro</a></li>
<li class="chapter" data-level="4.21.2" data-path="model.html"><a href="model.html#assumptions-14"><i class="fa fa-check"></i><b>4.21.2</b> Assumptions</a></li>
<li class="chapter" data-level="4.21.3" data-path="model.html"><a href="model.html#characteristics-9"><i class="fa fa-check"></i><b>4.21.3</b> Characteristics</a></li>
<li class="chapter" data-level="4.21.4" data-path="model.html"><a href="model.html#evaluation-14"><i class="fa fa-check"></i><b>4.21.4</b> Evaluation</a></li>
<li class="chapter" data-level="4.21.5" data-path="model.html"><a href="model.html#proscons-14"><i class="fa fa-check"></i><b>4.21.5</b> Pros/Cons</a></li>
</ul></li>
<li class="chapter" data-level="4.22" data-path="model.html"><a href="model.html#random-forest"><i class="fa fa-check"></i><b>4.22</b> Random Forest</a><ul>
<li class="chapter" data-level="4.22.1" data-path="model.html"><a href="model.html#intro-15"><i class="fa fa-check"></i><b>4.22.1</b> Intro</a></li>
<li class="chapter" data-level="4.22.2" data-path="model.html"><a href="model.html#assumptions-15"><i class="fa fa-check"></i><b>4.22.2</b> Assumptions</a></li>
<li class="chapter" data-level="4.22.3" data-path="model.html"><a href="model.html#characteristics-10"><i class="fa fa-check"></i><b>4.22.3</b> Characteristics</a></li>
<li class="chapter" data-level="4.22.4" data-path="model.html"><a href="model.html#evaluation-15"><i class="fa fa-check"></i><b>4.22.4</b> Evaluation</a></li>
<li class="chapter" data-level="4.22.5" data-path="model.html"><a href="model.html#proscons-15"><i class="fa fa-check"></i><b>4.22.5</b> Pros/Cons</a></li>
</ul></li>
<li class="chapter" data-level="4.23" data-path="model.html"><a href="model.html#boosting"><i class="fa fa-check"></i><b>4.23</b> Boosting</a><ul>
<li class="chapter" data-level="4.23.1" data-path="model.html"><a href="model.html#intro-16"><i class="fa fa-check"></i><b>4.23.1</b> Intro</a></li>
<li class="chapter" data-level="4.23.2" data-path="model.html"><a href="model.html#assumptions-16"><i class="fa fa-check"></i><b>4.23.2</b> Assumptions</a></li>
<li class="chapter" data-level="4.23.3" data-path="model.html"><a href="model.html#characteristics-11"><i class="fa fa-check"></i><b>4.23.3</b> Characteristics</a></li>
<li class="chapter" data-level="4.23.4" data-path="model.html"><a href="model.html#evaluation-16"><i class="fa fa-check"></i><b>4.23.4</b> Evaluation</a></li>
<li class="chapter" data-level="4.23.5" data-path="model.html"><a href="model.html#proscons-16"><i class="fa fa-check"></i><b>4.23.5</b> Pros/Cons</a></li>
</ul></li>
<li class="chapter" data-level="4.24" data-path="model.html"><a href="model.html#nlp"><i class="fa fa-check"></i><b>4.24</b> NLP</a></li>
<li class="chapter" data-level="4.25" data-path="model.html"><a href="model.html#topic-modeling"><i class="fa fa-check"></i><b>4.25</b> Topic Modeling</a></li>
<li class="chapter" data-level="4.26" data-path="model.html"><a href="model.html#recommendation-systems"><i class="fa fa-check"></i><b>4.26</b> Recommendation Systems</a></li>
<li class="chapter" data-level="4.27" data-path="model.html"><a href="model.html#time-series"><i class="fa fa-check"></i><b>4.27</b> Time Series</a><ul>
<li class="chapter" data-level="4.27.1" data-path="model.html"><a href="model.html#intro-17"><i class="fa fa-check"></i><b>4.27.1</b> Intro</a></li>
<li class="chapter" data-level="4.27.2" data-path="model.html"><a href="model.html#assumptions-17"><i class="fa fa-check"></i><b>4.27.2</b> Assumptions</a></li>
<li class="chapter" data-level="4.27.3" data-path="model.html"><a href="model.html#characteristics-12"><i class="fa fa-check"></i><b>4.27.3</b> Characteristics</a></li>
<li class="chapter" data-level="4.27.4" data-path="model.html"><a href="model.html#evaluation-17"><i class="fa fa-check"></i><b>4.27.4</b> Evaluation</a></li>
<li class="chapter" data-level="4.27.5" data-path="model.html"><a href="model.html#proscons-17"><i class="fa fa-check"></i><b>4.27.5</b> Pros/Cons</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="udacity-ts-fundamentals.html"><a href="udacity-ts-fundamentals.html"><i class="fa fa-check"></i><b>5</b> Udacity: TS Fundamentals</a><ul>
<li class="chapter" data-level="5.1" data-path="udacity-ts-fundamentals.html"><a href="udacity-ts-fundamentals.html#dl"><i class="fa fa-check"></i><b>5.1</b> DL</a></li>
<li class="chapter" data-level="5.2" data-path="udacity-ts-fundamentals.html"><a href="udacity-ts-fundamentals.html#tensorflow"><i class="fa fa-check"></i><b>5.2</b> TensorFlow</a></li>
<li class="chapter" data-level="5.3" data-path="udacity-ts-fundamentals.html"><a href="udacity-ts-fundamentals.html#keras"><i class="fa fa-check"></i><b>5.3</b> Keras</a><ul>
<li class="chapter" data-level="5.3.1" data-path="udacity-ts-fundamentals.html"><a href="udacity-ts-fundamentals.html#regression"><i class="fa fa-check"></i><b>5.3.1</b> Regression</a></li>
<li class="chapter" data-level="5.3.2" data-path="udacity-ts-fundamentals.html"><a href="udacity-ts-fundamentals.html#classification"><i class="fa fa-check"></i><b>5.3.2</b> Classification</a></li>
<li class="chapter" data-level="5.3.3" data-path="udacity-ts-fundamentals.html"><a href="udacity-ts-fundamentals.html#another-example"><i class="fa fa-check"></i><b>5.3.3</b> Another Example</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="udacity-ts-fundamentals.html"><a href="udacity-ts-fundamentals.html#association-rule-mining"><i class="fa fa-check"></i><b>5.4</b> Association Rule Mining</a><ul>
<li class="chapter" data-level="5.4.1" data-path="udacity-ts-fundamentals.html"><a href="udacity-ts-fundamentals.html#intro-18"><i class="fa fa-check"></i><b>5.4.1</b> Intro</a></li>
<li class="chapter" data-level="5.4.2" data-path="udacity-ts-fundamentals.html"><a href="udacity-ts-fundamentals.html#assumptions-18"><i class="fa fa-check"></i><b>5.4.2</b> Assumptions</a></li>
<li class="chapter" data-level="5.4.3" data-path="udacity-ts-fundamentals.html"><a href="udacity-ts-fundamentals.html#characteristics-13"><i class="fa fa-check"></i><b>5.4.3</b> Characteristics</a></li>
<li class="chapter" data-level="5.4.4" data-path="udacity-ts-fundamentals.html"><a href="udacity-ts-fundamentals.html#evaluation-18"><i class="fa fa-check"></i><b>5.4.4</b> Evaluation</a></li>
<li class="chapter" data-level="5.4.5" data-path="udacity-ts-fundamentals.html"><a href="udacity-ts-fundamentals.html#proscons-18"><i class="fa fa-check"></i><b>5.4.5</b> Pros/Cons</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="udacity-ts-fundamentals.html"><a href="udacity-ts-fundamentals.html#model-selection"><i class="fa fa-check"></i><b>5.5</b> Model Selection</a></li>
<li class="chapter" data-level="5.6" data-path="udacity-ts-fundamentals.html"><a href="udacity-ts-fundamentals.html#other-2"><i class="fa fa-check"></i><b>5.6</b> Other</a><ul>
<li class="chapter" data-level="5.6.1" data-path="udacity-ts-fundamentals.html"><a href="udacity-ts-fundamentals.html#general-2"><i class="fa fa-check"></i><b>5.6.1</b> General</a></li>
<li class="chapter" data-level="5.6.2" data-path="udacity-ts-fundamentals.html"><a href="udacity-ts-fundamentals.html#unbalanced-classes"><i class="fa fa-check"></i><b>5.6.2</b> Unbalanced Classes</a></li>
<li class="chapter" data-level="5.6.3" data-path="udacity-ts-fundamentals.html"><a href="udacity-ts-fundamentals.html#longitudinal-data"><i class="fa fa-check"></i><b>5.6.3</b> Longitudinal Data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="cdm.html"><a href="cdm.html"><i class="fa fa-check"></i><b>6</b> CDM</a><ul>
<li class="chapter" data-level="6.1" data-path="cdm.html"><a href="cdm.html#shiny"><i class="fa fa-check"></i><b>6.1</b> Shiny</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="frameworks.html"><a href="frameworks.html"><i class="fa fa-check"></i>Frameworks</a><ul>
<li class="chapter" data-level="6.2" data-path="frameworks.html"><a href="frameworks.html#pyspark"><i class="fa fa-check"></i><b>6.2</b> PySpark</a></li>
<li class="chapter" data-level="6.3" data-path="frameworks.html"><a href="frameworks.html#sparklyr"><i class="fa fa-check"></i><b>6.3</b> Sparklyr</a></li>
<li class="chapter" data-level="6.4" data-path="frameworks.html"><a href="frameworks.html#caret"><i class="fa fa-check"></i><b>6.4</b> Caret</a></li>
<li class="chapter" data-level="6.5" data-path="frameworks.html"><a href="frameworks.html#preprocessing"><i class="fa fa-check"></i><b>6.5</b> Preprocessing</a></li>
<li class="chapter" data-level="6.6" data-path="frameworks.html"><a href="frameworks.html#lm"><i class="fa fa-check"></i><b>6.6</b> LM</a></li>
<li class="chapter" data-level="6.7" data-path="frameworks.html"><a href="frameworks.html#lassoridge"><i class="fa fa-check"></i><b>6.7</b> Lasso/Ridge</a></li>
<li class="chapter" data-level="6.8" data-path="frameworks.html"><a href="frameworks.html#logreg"><i class="fa fa-check"></i><b>6.8</b> LogReg</a></li>
<li class="chapter" data-level="6.9" data-path="frameworks.html"><a href="frameworks.html#train-test-split-folds-cv"><i class="fa fa-check"></i><b>6.9</b> Train-Test Split, Folds, &amp; CV</a></li>
<li class="chapter" data-level="6.10" data-path="frameworks.html"><a href="frameworks.html#random-forest-1"><i class="fa fa-check"></i><b>6.10</b> Random Forest</a></li>
<li class="chapter" data-level="6.11" data-path="frameworks.html"><a href="frameworks.html#svm-1"><i class="fa fa-check"></i><b>6.11</b> SVM</a></li>
<li class="chapter" data-level="6.12" data-path="frameworks.html"><a href="frameworks.html#model-selection-1"><i class="fa fa-check"></i><b>6.12</b> Model Selection</a></li>
<li class="chapter" data-level="6.13" data-path="frameworks.html"><a href="frameworks.html#stacking"><i class="fa fa-check"></i><b>6.13</b> Stacking</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="code-snippets.html"><a href="code-snippets.html"><i class="fa fa-check"></i>Code Snippets</a><ul>
<li class="chapter" data-level="6.14" data-path="code-snippets.html"><a href="code-snippets.html#python"><i class="fa fa-check"></i><b>6.14</b> Python</a><ul>
<li class="chapter" data-level="6.14.1" data-path="code-snippets.html"><a href="code-snippets.html#check-csv-encoding"><i class="fa fa-check"></i><b>6.14.1</b> Check CSV Encoding</a></li>
<li class="chapter" data-level="6.14.2" data-path="code-snippets.html"><a href="code-snippets.html#mnist-example"><i class="fa fa-check"></i><b>6.14.2</b> MNIST Example</a></li>
</ul></li>
<li class="chapter" data-level="6.15" data-path="code-snippets.html"><a href="code-snippets.html#r"><i class="fa fa-check"></i><b>6.15</b> R</a></li>
<li class="chapter" data-level="6.16" data-path="code-snippets.html"><a href="code-snippets.html#shiny-reactive"><i class="fa fa-check"></i><b>6.16</b> Shiny Reactive</a></li>
<li class="chapter" data-level="6.17" data-path="code-snippets.html"><a href="code-snippets.html#custom-theme"><i class="fa fa-check"></i><b>6.17</b> Custom Theme</a></li>
<li class="chapter" data-level="6.18" data-path="code-snippets.html"><a href="code-snippets.html#create-factor"><i class="fa fa-check"></i><b>6.18</b> Create Factor</a><ul>
<li class="chapter" data-level="6.18.1" data-path="code-snippets.html"><a href="code-snippets.html#extract-date"><i class="fa fa-check"></i><b>6.18.1</b> Extract Date</a></li>
<li class="chapter" data-level="6.18.2" data-path="code-snippets.html"><a href="code-snippets.html#geom_path"><i class="fa fa-check"></i><b>6.18.2</b> Geom_Path</a></li>
<li class="chapter" data-level="6.18.3" data-path="code-snippets.html"><a href="code-snippets.html#theme-template"><i class="fa fa-check"></i><b>6.18.3</b> Theme Template</a></li>
<li class="chapter" data-level="6.18.4" data-path="code-snippets.html"><a href="code-snippets.html#heatmap"><i class="fa fa-check"></i><b>6.18.4</b> Heatmap</a></li>
<li class="chapter" data-level="6.18.5" data-path="code-snippets.html"><a href="code-snippets.html#calculate-accuracy"><i class="fa fa-check"></i><b>6.18.5</b> Calculate Accuracy</a></li>
<li class="chapter" data-level="6.18.6" data-path="code-snippets.html"><a href="code-snippets.html#replace-na"><i class="fa fa-check"></i><b>6.18.6</b> Replace NA</a></li>
<li class="chapter" data-level="6.18.7" data-path="code-snippets.html"><a href="code-snippets.html#filter-by-row"><i class="fa fa-check"></i><b>6.18.7</b> Filter By Row</a></li>
<li class="chapter" data-level="6.18.8" data-path="code-snippets.html"><a href="code-snippets.html#db-functions"><i class="fa fa-check"></i><b>6.18.8</b> DB Functions</a></li>
</ul></li>
<li class="chapter" data-level="6.19" data-path="code-snippets.html"><a href="code-snippets.html#knitr"><i class="fa fa-check"></i><b>6.19</b> Knitr</a></li>
<li class="chapter" data-level="6.20" data-path="code-snippets.html"><a href="code-snippets.html#bash"><i class="fa fa-check"></i><b>6.20</b> Bash</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="general-3.html"><a href="general-3.html"><i class="fa fa-check"></i>General</a><ul>
<li class="chapter" data-level="6.21" data-path="general-3.html"><a href="general-3.html#python-1"><i class="fa fa-check"></i><b>6.21</b> Python</a></li>
<li class="chapter" data-level="6.22" data-path="general-3.html"><a href="general-3.html#r-1"><i class="fa fa-check"></i><b>6.22</b> R</a></li>
<li class="chapter" data-level="6.23" data-path="general-3.html"><a href="general-3.html#git"><i class="fa fa-check"></i><b>6.23</b> Git</a></li>
<li class="chapter" data-level="6.24" data-path="general-3.html"><a href="general-3.html#other-3"><i class="fa fa-check"></i><b>6.24</b> Other</a></li>
<li class="chapter" data-level="6.25" data-path="general-3.html"><a href="general-3.html#yaml"><i class="fa fa-check"></i><b>6.25</b> YAML</a></li>
<li class="chapter" data-level="6.26" data-path="general-3.html"><a href="general-3.html#ab-testing-overview"><i class="fa fa-check"></i><b>6.26</b> AB Testing Overview</a></li>
<li class="chapter" data-level="6.27" data-path="general-3.html"><a href="general-3.html#designing-an-experiment"><i class="fa fa-check"></i><b>6.27</b> Designing An Experiment</a></li>
<li class="chapter" data-level="6.28" data-path="general-3.html"><a href="general-3.html#networks"><i class="fa fa-check"></i><b>6.28</b> Networks</a></li>
<li class="chapter" data-level="6.29" data-path="general-3.html"><a href="general-3.html#networkx"><i class="fa fa-check"></i><b>6.29</b> Networkx</a></li>
<li class="chapter" data-level="6.30" data-path="general-3.html"><a href="general-3.html#stats"><i class="fa fa-check"></i><b>6.30</b> Stats</a><ul>
<li class="chapter" data-level="6.30.1" data-path="general-3.html"><a href="general-3.html#infer-package"><i class="fa fa-check"></i><b>6.30.1</b> Infer Package</a></li>
<li class="chapter" data-level="6.30.2" data-path="general-3.html"><a href="general-3.html#statistical-tests"><i class="fa fa-check"></i><b>6.30.2</b> Statistical Tests</a></li>
<li class="chapter" data-level="6.30.3" data-path="general-3.html"><a href="general-3.html#stats-for-hackers"><i class="fa fa-check"></i><b>6.30.3</b> Stats For Hackers</a></li>
<li class="chapter" data-level="6.30.4" data-path="general-3.html"><a href="general-3.html#think-stats"><i class="fa fa-check"></i><b>6.30.4</b> Think Stats</a></li>
</ul></li>
<li class="chapter" data-level="6.31" data-path="general-3.html"><a href="general-3.html#other-4"><i class="fa fa-check"></i><b>6.31</b> Other</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Science Cribsheeet</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="model" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Model</h1>
<div id="pre-processing" class="section level2">
<h2><span class="header-section-number">4.1</span> Pre-processing</h2>
</div>
<div id="feature-engineering" class="section level2">
<h2><span class="header-section-number">4.2</span> Feature Engineering</h2>
<ul>
<li>Dealing with cyclical features: Hours of the day, days of the week, months in a year, and wind direction are all examples of features that are cyclical. Many new machine learning engineers don’t think to convert these features into a representation that can preserve information such as hour 23 and hour 0 being close to each other and not far. Keeping with the hour example, the best way to handle this is to calculate the sin and cos component so that you represent your cyclical feature as (x,y) coordinates of a circle. In this representation hour, 23 and hour 0 are right next to each other numerically, just as they should be.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> numpy <span class="im">as</span> np
df[<span class="st">&#39;hr_sin&#39;</span>] <span class="op">=</span> np.sin(df.hr<span class="op">*</span>(<span class="dv">2</span>.<span class="op">*</span>np.pi<span class="op">/</span><span class="dv">24</span>))
df[<span class="st">&#39;hr_cos&#39;</span>] <span class="op">=</span> np.cos(df.hr<span class="op">*</span>(<span class="dv">2</span>.<span class="op">*</span>np.pi<span class="op">/</span><span class="dv">24</span>))
df[<span class="st">&#39;mnth_sin&#39;</span>] <span class="op">=</span> np.sin((df.mnth<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>(<span class="dv">2</span>.<span class="op">*</span>np.pi<span class="op">/</span><span class="dv">12</span>))
df[<span class="st">&#39;mnth_cos&#39;</span>] <span class="op">=</span> np.cos((df.mnth<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>(<span class="dv">2</span>.<span class="op">*</span>np.pi<span class="op">/</span><span class="dv">12</span>))</code></pre></div>
</div>
<div id="feature-selection" class="section level2">
<h2><span class="header-section-number">4.3</span> Feature Selection</h2>
<ul>
<li><p>Variance Threshold: Use this to exclude features that don’t meet a variance threshold.</p></li>
<li><p>Univariate Feature Selection: For classification can use chi-squared and F-Test while F-Test for regression. Functions: chi2, f_regression, f_classification from sklearn.feature_selection.</p></li>
<li><p>SelectKBest/SelectPercentile: Keep the k highest scoring features and keep a user-specified highest scoring percentage of features. Can use the univariate feature selection in these functions.</p></li>
<li><p>sklearn.feature_selection.RFE(CV)</p></li>
<li><p>To work around magic values, convert the feature into two features: One feature holds only quality ratings, never magic values. One feature holds a boolean value indicating whether or not a quality_rating was supplied. Give this boolean feature a name like is_quality_rating_defined.</p></li>
<li><p>Make sure that your test set meets the following two conditions:</p>
<ul>
<li><p>Is large enough to yield statistically meaningful results.</p></li>
<li><p>Is representative of the data set as a whole. In other words, don’t pick a test set with different characteristics than the training set.</p></li>
</ul></li>
<li><p>Handling extreme outliers: Given data with a very long tail, log scaling does a slightly better job, but there’s still a significant tail of outlier values. Let’s pick yet another approach. What if we simply “cap” or “clip” the maximum value at an arbitrary value, say 4.0? Clipping the feature value at 4.0 doesn’t mean that we ignore all values greater than 4.0. Rather, it means that all values that were greater than 4.0 now become 4.0. This explains the funny hill at 4.0. Despite that hill, the scaled feature set is now more useful than the original data. Binning.</p></li>
<li><p>Know your data. Follow these rules: 1) Keep in mind what you think your data should look like. 2) Verify that the data meets these expectations (or that you can explain why it doesn’t). 3) Double-check that the training data agrees with other sources (for example, dashboards).</p></li>
</ul>
</div>
<div id="linear-regression" class="section level2">
<h2><span class="header-section-number">4.4</span> Linear Regression</h2>
<div id="intro" class="section level3">
<h3><span class="header-section-number">4.4.1</span> Intro</h3>
<p>The standard least squares coefficient estimates are scale equivariant: if we multiply a predictor variable by a constant, the corresponding least squares coefficient estimate will be scaled down by the same constant.</p>
<p>Forward Stepwise Selection is a Greedy Algorithm because at each step it selects the variable that improves the current model the most. There is no guarantee that the final result will be optimal.</p>
<p>Linear: When you’re predicting a continuous value.</p>
<p>Increasing x1 by 1 means you are increasing the underlying X1 by 1 standard deviation x1+1=(X1+sig-mu)/sig. Therefore you can say that b1 is the effect on y of increasing X1 by 1 sigma.</p>
<p>In regression, it is often recommended to center the variables so that the predictors have mean 0. This makes it so the intercept term is interpreted as the expected value of Y_i when the predictor values are set to their means. Otherwise, the intercept is interpreted as the expected value of Y_i when the predictors are set to 0, which may not be a realistic or interpretable situation.</p>
<p>When p &gt; n, we can no longer calculate a unique least square coefficient estimate, the variances become infinite, so OLS cannot be used at all.</p>
<p>In presence of few variables with medium / large sized effect, use lasso regression. In presence of many variables with small / medium sized effect, use ridge regression.</p>
<p>The fitted regression line using a sample of data gives imperfect predictions for future observations due to sampling variability and randomness in Y that is not related to X.</p>
<p>Use Box-Cox transformations when predictions are skewed</p>
<p>Got it: Linear Correlation =&gt; Causation if covariates respect: linearity normality indep homoscedasticity + all confounders in model</p>
<p>In summary, “pooling” your data and fitting one combined regression function allows you to easily and efficiently answer research questions concerning the binary predictor variable.</p>
<p>If two independent variables are measured in exactly the same units, we can asses their relative importance in their effect on y quite simply: the larger the coefficient, the stronger the effect.</p>
<p>Okay, so many of the features are strongly correlated. We can make use of this in our model by including polynomial features (e.g. PolynomialFeatures(degree=2) from scikit-learn). Adding these brings our validation loss down to 0.69256 (-0.05% from baseline).</p>
<p>We prefer natural logs (that is, logarithms base e) because, as described above, coefficients on the natural-log scale are directly interpretable as approximate proportional differences: with a coefficient of 0.06, a difference of 1 in x corresponds to an approximate 6% difference in y, and so forth.</p>
<p>RSS: The sum of the squared error terms for each observation in our dataset.</p>
<p>R.S.E: The standard deviation of the residuals about the regression surface. Estimate of sigma.</p>
<p>AIC, BIC, forward &amp; backwards selection</p>
<p>Transformations: Can remedy assumption violations and strengthen linear relationships between variables, but can lead to overfitting and less interpretability.</p>
<p>square root correction: Take square root of x variable(s).</p>
<p>log transformation: Take log of y.</p>
<p>General thumb: powers &gt; 1 for skewed left data, powers &lt; 1 for data skewed right.</p>
<p>Box-Cox: Iterates along values of lambda by maximum likelihood so as to maximize the fit of the transformed variable to a normal distribution. Does not guarantee normality since it minimizes standard deviation. Can only be used on positive values. Use on negative values by shifting by a constant value.</p>
<p>TSS: A measure of the total squared deviation of our response variable from its mean value.</p>
<p>R-squared = 1 – RSS/TSS.</p>
</div>
<div id="assumptions" class="section level3">
<h3><span class="header-section-number">4.4.2</span> Assumptions</h3>
<p>The regression assumption that is generally least important is that the errors are normally distributed. In fact, for the purpose of estimating the regression line (as compared to predicting individual data points), the assumption of normality is barely important at all. Thus, in contrast to many regression textbooks, we do not recommend diagnostics of the normality of regression residuals. [Gelman]</p>
<p>Linear regression errors are not autocorrelated. The Durbin-Watson statistic detects this; if it is close to 2, there is no autocorrelation.</p>
<p>A linear regression model is only as good as the validity of its assumptions, which can be summarized as follows:</p>
<p>Linearity: This is a linear relationship between the predictor and the response variables. If this relationship is not clearly present, transformations (log, polynomial, exponent and so on) of the X or Y may solve the problem.</p>
<p>Non-correlation of errors: A common problem in the time series and panel data where en = betan-1; if the errors are correlated, you run the risk of creating a poorly specified model.</p>
<p>Homoscedasticity: Normally the distributed and constant variance of errors, which means that the variance of the errors is constant across the different values of inputs. Violations of this assumption can create biased coefficient estimates, leading to statistical tests for significance that can be either too high or too low. This, in turn, leads to the wrong conclusion. This violation is referred to as heteroscedasticity.</p>
<p>No collinearity: No linear relationship between two predictor variables, which is to say that there should be no correlation between the features. This, again, can lead to biased estimates.</p>
<p>Presence of outliers: Outliers can severely skew the estimation and, ideally, must be removed prior to fitting a model using linear regression; this again can lead to a biased estimate.</p>
<p>Assumes one can fit a hyperplane to the data. Link function is the identity.</p>
<p>Assumptions: Succinctly: y = b0+b1*x+e, e is i.i.d N(0, variance).</p>
<p>Multiple Linear Reg: Additional assumption of little to no multicollinearity between predictor variables.</p>
<p>Variance Inflation Factor: MEasures how the model facotrs influence hte uncertainity of coefficient estimates. &gt;=5 is bad.</p>
<p>F-test &amp; partial f-testr for simple and multiple linear regression.</p>
<p>Linearity: The underlying function connecting the independent variable to the dependent variable is indeed linear. Check with a scatterplot.</p>
<p>Constant Variance / homoscedasticity: The error terms have the same variance. Check the residual plot which is a scatterplot of the residuals vs the fitted values.</p>
<p>Normality: The error terms are drawn from an identical Gaussian distribution, i.e a normal distribution of the dependent variable for each value of the independent variable. Inspect the quantile-quantile plot of the residuals. Also Shapiro-Wilk Test for Normality.</p>
<p>Independent Errors: The residual value for an arbitrary observation is not predictable from knowledge of another observation’s residual value; they are uncorrelated. Inspect the residual plot after fitting the regression. Ideally, there would be no clear pattern in the fluctuation of the error terms.</p>
<p>No Multicollinearity: For multiple linear regression. If not coefficient estimates could be unstable, introduce redundancies within predictors, and make it more difficult to make inferences. Could inflate standard errors, decrease power and reliability of regression coefficients, and create need for a larger sample size. Check the variance inflation factors. Tells the factor by which the estimated variance of a variable is larger in comparison ti it it were completely uncorrelated with other variables in the model. If the VIF is more than five then the variable is a candidate to remove from the model. Its information is contained in the other variables.</p>
<p><a href="http://www.statmethods.net/stats/rdiagnostics.html">Regression Diagnostics</a></p>
<p>Breusch-Pagan (BP) test lmtest package: is used to test for heteroskedasticity in a linear regression model</p>
</div>
<div id="evaluation" class="section level3">
<h3><span class="header-section-number">4.4.3</span> Evaluation</h3>
<p>Model Plot Notes:</p>
<p>Outliers are observations that have high residual values. Leverage points are observations that have unusually small or large independent variable values. Cook’s distance helps to measure the effect of deleting an observation from the #dataset and rerunning the regression. Observations that have large residual values and also high leverage tend to pose threats to the accuracy of the regression line and thus need to be further investigated. Look at influence plot. Confidence and prediction intervals. For avplot distinct patterns are indications of good contributions. Influence plot to look at hat values (lower is better). Use F-test to compare models.</p>
<p>For simple linear regression, the principal hypothesis test is as follows:</p>
<p>Null Hypothesis (H_0 ): ? 1 = 0</p>
<p>Alternative Hypothesis (H A ): ? 1 ≠ 0</p>
<p>What would it mean if the null hypothesis were true? We would expect that the population mean of Y would be ? 0 no matter what the value of X. In other words, this would mean that X has no effect on Y! What would it mean if the null hypothesis were false? We would expect that Y would vary with different values of X. In other words, this would mean that X does have an effect on Y.</p>
<p>F-Test: h0: all/some coefficients are zero vs ha: at least one non-zero coefficient. If h0 fail to reject than F value close to 1, else F value greater than 1.</p>
<p>Type 1 Error: Accept ha when h0 is true.</p>
<p>A.I.C and B.I.C: Smaller value is better. Rewards goodness of fit and penalizes complexity. Favor A.I.C for prediction (n increases results in increase in accuracy) and B.I.C for descriptive power (penalty term more stringest and favors a parsimonious model)</p>
<p>NB: R-squared increases whenever a predictor is added, and it doesn’t take model complexity into consideration which makes it prone to overfitting. Use adjusted R-squared instead. Additional predictors only make this increase is they are statistically significant. It’s easy to show that given no other data for a set of numbers, the predictor that minimises the MAE is the median, while the predictor that minimises the MSE is the mean.</p>
<p>compare rmse to sd of data for sense of how good it is (rsme &lt; sd good and vice versa)</p>
<p><strong>Shapiro-Wilk Test for Normality</strong></p>
<p>H0: The data is normally distributed. HA: The data is not normally distributed. To run this test in R, the syntax is quite simple:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">0</span>)

data =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>) <span class="co"># Generating data from a standard normal distribution.</span>
<span class="kw">shapiro.test</span>(data) <span class="co"># P-value insignificant; retain H0, conclude data is normally distributed.</span>

<span class="co"># Inspect this same data using the QQ-plots (For normal data, the QQ-plot produces a straight-line relationship. For uniform data, the QQ-plot does NOT produce a straight-line relationship): </span>
<span class="kw">qqnorm</span>(normal.data) <span class="op">+</span><span class="st"> </span><span class="kw">qqline</span>(normal.data)</code></pre></div>
</div>
<div id="proscons" class="section level3">
<h3><span class="header-section-number">4.4.4</span> Pros/Cons</h3>
<p><strong>Pros</strong></p>
<ul>
<li><p>Very fast (runs in constant time)</p></li>
<li><p>Easy to understand the model</p></li>
<li><p>Less prone to overfitting</p></li>
</ul>
<p><strong>Cons</strong></p>
<ul>
<li><p>Unable to model complex relationships</p></li>
<li><p>Unable to capture nonlinear relationships without first transforming the inputs</p></li>
</ul>
</div>
<div id="other" class="section level3">
<h3><span class="header-section-number">4.4.5</span> Other</h3>
<ul>
<li><p><a href="https://en.wikipedia.org/wiki/Quantile_regression" class="uri">https://en.wikipedia.org/wiki/Quantile_regression</a></p></li>
<li><p>If there is an ascending or descending order to your dependent variable, ordinal regression is more appropriate as it has more power than multinomial logistic regression.</p></li>
<li><p>leaps lm package r is a greedy way to find the best predictors by cycling through all the combinations.</p></li>
</ul>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> statsmodels.api <span class="im">as</span> sm
X <span class="op">=</span> sm.add_constant(X)
rgr_lr <span class="op">=</span> sm.OLS(Y, X).fit(disp<span class="op">=</span><span class="va">False</span>)
<span class="bu">print</span>(results.summary())</code></pre></div>
</div>
</div>
<div id="ridgelasso" class="section level2">
<h2><span class="header-section-number">4.5</span> Ridge/Lasso</h2>
<div id="intro-1" class="section level3">
<h3><span class="header-section-number">4.5.1</span> Intro</h3>
<p>Minimizes RSS but also has shrinkage penalty (L2 penalty). A small lambda penalizes the RSS more than the shrinkage penalty. A large lambda penalizes the shrinkage penalty more than the RSS. By shrinking the coefficient estimates towards 0 by increasing lambda, the bias increases slightly but remains relatively small, the variance reduces substantially, and the mean squared error of the predictions drops. Not scale invariant due to the shrinkage penalty. To avoid the issue of overvaluing or undervaluing certain predictor variables simply based on their magnitudes, we must standardize the variables prior to performing ridge regression. The main disadvantage of ridge regression is that, while parameter estimates are shrunken, they only asymptotically approach 0 as we increase the value of lambda.</p>
<p>Standardize for ridge/lasso alpha makes them not scale invariant</p>
<p>Minimizes RSS but also has L1 penalty (norm). This necessarily forcessome coefficient estimates to be exactly 0 (when lambda is sufficiently large). It has the added advantage of essentially performing variable selection, yielding models that are both accurate and parsimonious. Restricting ourselves to simpler models by including a Lasso penalty will generally decrease the variance of the fits at the cost of higher bias.</p>
<p>Note: In both ridge and lasso regression, is important to select an appropriate value of lambda by means of cross-validation.</p>
</div>
<div id="assumptions-1" class="section level3">
<h3><span class="header-section-number">4.5.2</span> Assumptions</h3>
</div>
<div id="evaluation-1" class="section level3">
<h3><span class="header-section-number">4.5.3</span> Evaluation</h3>
</div>
<div id="proscons-1" class="section level3">
<h3><span class="header-section-number">4.5.4</span> Pros/Cons</h3>
</div>
</div>
<div id="logistic-regression" class="section level2">
<h2><span class="header-section-number">4.6</span> Logistic Regression</h2>
<p>The right-hand predictor side of the equation must be linear with the left-hand outcome side of the equation. You must test for linearity in the logit (in logistic regression the logit is the outcome side). This is commonly done with the Box-Tidwell Transformation (Test): Add to the logistic model interaction terms which are the crossproduct of each independent times its natural logarithm [(X)ln(X)]. If these terms are significant, then there is nonlinearity in the logit. This method is not sensitive to small nonlinearities.</p>
<div id="intro-2" class="section level3">
<h3><span class="header-section-number">4.6.1</span> Intro</h3>
<p>Logistic regression assumes that logit(y) is linear in the values of x. Like linear regression, logistic regression will find the best coefficients to predict y, including finding advantageous combinations and cancellations when the inputs are correlated. In other words, you can think of logistic regression as a linear regression that finds the log-odds of the probability that you’re interested in. Logistic Regression can also be used with kernel methods.</p>
<p>Logistic: When you’re predicting which category your observation is in. (Is this is a cat or a dog?)</p>
<p>The link function is the logit function. Logit is the inverse of the sigmoid and gives the log odds, i.e p/(1-p).</p>
<p>Easy to incorpotate prior knowledge, number fo features are small, training is fast, precision not critical.</p>
<p>No closed form expression to maximize coefficients with maximum likelihood estimation, so one uses gradient descent or stochastic gradient descent. The second one is faster.</p>
<p>For logistic regression value of c penalizes features. Low c penalizes a lot and vice versa. A large c decreases the effect of the regularization term (the l2 penalization).</p>
<p>Makes a linear boundary between classes.</p>
<p>For multiclass classification it will build multiple models. Given 3 classes 0,1, and 2, there will be models 0 and not 0, 1 and not 1, and 2 and not 2.</p>
<p>Standardization isn’t required for logistic regression. The main goal of standardizing features is to help convergence of the technique used for optimization. For example, if you use Newton-Raphson to maximize the likelihood, standardizing the features makes the convergence faster. Otherwise, you can run your logistic regression without any standardization treatment on the features.</p>
</div>
<div id="assumptions-2" class="section level3">
<h3><span class="header-section-number">4.6.2</span> Assumptions</h3>
<p>Secondly, the linear regression analysis requires all variables to be multivariate normal. This assumption can best be checked with a histogram or a Q-Q-Plot.</p>
<ul>
<li><p>Requires the dependent variable to be binary and ordinal logistic regression requires the dependent variable to be ordinal.</p></li>
<li><p>The observations to be independent of each other. In other words, the observations should not come from repeated measurements or matched data.</p></li>
<li><p>There to be little or no multicollinearity among the independent variables.</p></li>
<li><p>Assumes linearity of independent variables and logit of the probabilities (logit(p) = log(p/(1-p))). Check <a href="https://stats.stackexchange.com/questions/169348/how-should-i-check-the-assumption-of-linearity-to-the-logit-for-the-continuous-i">in R</a>.</p></li>
<li><p>There is no influential values (extreme values or outliers) in the continuous predictors</p></li>
</ul>
</div>
<div id="evaluation-2" class="section level3">
<h3><span class="header-section-number">4.6.3</span> Evaluation</h3>
<ul>
<li><p>Wald Test: h0: b = 0 and ha: b =/= 0. h0 means the log odds are unaffected by x and so x has no bearing on the prediction of success.</p></li>
<li><p>Deviance G^2 and Goodness of Fit: The deviance associated with a given logistic regression model M is based on comparing the maximum log-likelihood of model M against the saturated model S. The smaller the deviance the better. For goodness of fit h0 says M is appropriate. Can be expanded to compare models by deviance.</p></li>
<li><p>McFadden’s Pseudo R^2.</p></li>
<li><p>A.I.C / B.I.C</p></li>
</ul>
</div>
<div id="proscons-2" class="section level3">
<h3><span class="header-section-number">4.6.4</span> Pros/Cons</h3>
<p><strong>Pros</strong></p>
<ul>
<li><p>Highly interpretably</p></li>
<li><p>Model training and prediction are fast</p></li>
<li><p>No tuning outside of regularization required</p></li>
<li><p>Features don’t need scaling</p></li>
<li><p>Can perform with a small number of observations</p></li>
<li><p>Outputs well-calibrated predicted probabilities</p></li>
<li><p>low variance</p></li>
</ul>
<p><strong>Cons</strong></p>
<ul>
<li><p>Presumes a linear relationship between the features and the log-odds of the response.</p></li>
<li><p>Performance is generally not competitive with the best supervised learning methods</p></li>
<li><p>Can’t automatically learn feature interactions</p></li>
<li><p>Breaks down when classes are perfectly separable.</p></li>
<li><p>High bias</p></li>
</ul>
</div>
<div id="code-example" class="section level3">
<h3><span class="header-section-number">4.6.5</span> Code Example</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> statsmodels.api <span class="im">as</span> sm
X <span class="op">=</span> sm.add_constant(X)
clf_lr <span class="op">=</span> sm.Logit(Y, X).fit(disp<span class="op">=</span><span class="va">False</span>)
<span class="bu">print</span>(results.summary())</code></pre></div>
</div>
</div>
<div id="poisson-regression" class="section level2">
<h2><span class="header-section-number">4.7</span> Poisson Regression</h2>
<div id="intro-3" class="section level3">
<h3><span class="header-section-number">4.7.1</span> Intro</h3>
<p>Poisson: When you’re predicting a count value. (How many dogs will I see in the park?)</p>
<p>Poisson Regression: The Poisson distribution is used to model variation in count data (that is, data that can equal 0,1,2,…).</p>
<p>Remember that you must specify family = poisson or family = quasipoisson when using glm() to fit a count model.</p>
</div>
<div id="assumptions-3" class="section level3">
<h3><span class="header-section-number">4.7.2</span> Assumptions</h3>
<p>One of the assumptions of Poisson regression to predict counts is that the event you are counting is Poisson distributed: the average count per unit time is the same as the variance of the count. In practice, “the same” means that the mean and the variance should be of a similar order of magnitude.</p>
<p>When the variance is much larger than the mean, the Poisson assumption doesn’t apply, and one solution is to use quasipoisson regression, which does not assume that variance=meanvariance=mean.</p>
</div>
<div id="evaluation-3" class="section level3">
<h3><span class="header-section-number">4.7.3</span> Evaluation</h3>
</div>
<div id="proscons-3" class="section level3">
<h3><span class="header-section-number">4.7.4</span> Pros/Cons</h3>
<p><strong>Pros</strong></p>
<p><strong>Cons</strong></p>
</div>
</div>
<div id="generalized-additive-models" class="section level2">
<h2><span class="header-section-number">4.8</span> Generalized Additive Models</h2>
<div id="intro-4" class="section level3">
<h3><span class="header-section-number">4.8.1</span> Intro</h3>
<p>mgcv::gam: More complex than lm, more likely to overfit, best used on larger data sets. use s(var) to denote non-linear relationship. Don’t use s() with categorical variable. Use type = terms for y values of plots to get predictions type = response.</p>
<p>Also remember that gam() from the package mgcv has the calling interface</p>
<p>gam(formula, family, data) For standard regression, use family = gaussian (the default).</p>
<p>For GAM models, the predict() method returns a matrix, so use as.numeric() to convert the matrix to a vector.</p>
</div>
<div id="assumptions-4" class="section level3">
<h3><span class="header-section-number">4.8.2</span> Assumptions</h3>
</div>
<div id="evaluation-4" class="section level3">
<h3><span class="header-section-number">4.8.3</span> Evaluation</h3>
</div>
<div id="proscons-4" class="section level3">
<h3><span class="header-section-number">4.8.4</span> Pros/Cons</h3>
<p><strong>Pros</strong></p>
<p><strong>Cons</strong></p>
</div>
</div>
<div id="bayesian" class="section level2">
<h2><span class="header-section-number">4.9</span> Bayesian</h2>
<p>Bayes Rule:</p>
<p>old_prob [P(h)/p(~h)] * strength_of_evidence [p(e|h)/p(e|~h)] = new_prob [p(h|e)/p(~h|e)]</p>
<blockquote>
<p>Models have a lot of parameters classical methods that use simple point estimates of the parameters don’t adequately capture uncertainty so we turn to Bayesian methods because Bayesian inference is one way of finding a large model with metadata.</p>
</blockquote>
<blockquote>
<p>The second reason we use Bayesian inference is for combining information you might have experimental data on the drug under one condition and aggregate data under another condition we can combine polls and election forecast of public opinion data from multiple poles and environmental statistics we have measurements of different quality Bayesian methods are particularly adapted tocombining information.</p>
</blockquote>
<blockquote>
<p>The third appeal of Bayesian methods and the sort of applications that I work on is that the inferences can map directly to decisions based on inferences express not its estimates and standard errors but rather as probability distributions we can take these probability distributions and pipe them directly into decision analysis for these reasons.</p>
</blockquote>
<ul>
<li>An advantage of recognizing that the prior distribution is a testable part of a Bayesian model is that it clarifies the role of the prior inference, and where it comes from.</li>
</ul>
</div>
<div id="naive-bayes" class="section level2">
<h2><span class="header-section-number">4.10</span> Naïve Bayes</h2>
<div id="intro-5" class="section level3">
<h3><span class="header-section-number">4.10.1</span> Intro</h3>
<p>Practitioners do use Naive Bayes regularly for ranking, where the actual values of the probabilities are not relevant—only the relative values for examples in the different classes. Another advantage of Naive Bayes is that it is naturally an “incremental learner.” An incremental learner is an induction technique that can update its model one training example at a time. It does not need to reprocess all past training examples when new training data become available.</p>
<p>Document/spam classification is one use. Assumptions are that all the features are equally likely and are all important. Would be computational expensive without these assumptions with having to track all the joint probabilities. Gaussian for continuous variables, Bernoulli for binary input, and Multinomial for binary and more input. Use MLE to estimate parameters for Gaussian. Bernoulli/Multinomial used for spam classification.The Laplace Estimator (usually chosen to be 1) is a corrective measure that adds a small amount of error to each of the counts in the frequency table of words. The addition of error ensures that each resulting probability of each event will necessarily be nonzero, even if the event did not appear in the training data.</p>
<p>Note: In addition, we can use function “partial_fit” to fit on a batch of samples incrementally while we are using. MultinomialNB and Bernoulli NB also support sample weighting.</p>
</div>
<div id="assumptions-5" class="section level3">
<h3><span class="header-section-number">4.10.2</span> Assumptions</h3>
</div>
<div id="characteristics" class="section level3">
<h3><span class="header-section-number">4.10.3</span> Characteristics</h3>
</div>
<div id="evaluation-5" class="section level3">
<h3><span class="header-section-number">4.10.4</span> Evaluation</h3>
</div>
<div id="proscons-5" class="section level3">
<h3><span class="header-section-number">4.10.5</span> Pros/Cons</h3>
<p><strong>Pros</strong></p>
<ul>
<li><p>It is relatively simple to understand. Training the classifier does not require many observations, and the method also works well with large amounts of data. It is easy to obtain the estimated probabilityfor a classification prediction.</p></li>
<li><p>Computationally fast</p></li>
<li><p>Simple to implement</p></li>
<li><p>Works well with high dimensions</p></li>
</ul>
<p><strong>Cons</strong></p>
<ul>
<li><p>Relies on independence assumption and will perform badly if this assumption is not met</p></li>
<li><p>While easily attainable, the estimated probabilities are often less reliable than the predicted class labels themselves.</p></li>
</ul>
</div>
</div>
<div id="lda" class="section level2">
<h2><span class="header-section-number">4.11</span> LDA</h2>
<div id="intro-6" class="section level3">
<h3><span class="header-section-number">4.11.1</span> Intro</h3>
<p>Before we take a look into the inner workings of LDA in the following subsections, let’s summarize the key steps of the LDA approach:</p>
<p>Standardize the -dimensional dataset ( is the number of features).</p>
<p>For each class, compute the -dimensional mean vector.</p>
<p>Construct the between-class scatter matrix and the within-class scatter matrix .</p>
<p>Compute the eigenvectors and corresponding eigenvalues of the matrix .</p>
<p>Choose the eigenvectors that correspond to the largest eigenvalues to construct a -dimensional transformation matrix ; the eigenvectors are the columns of this matrix.</p>
<p>Project the samples onto the new feature subspace using the transformation matrix.</p>
<p>LDA similar to Naive Bayes but doesn’t assume variables are independent, assumes that p(x|y) is a multivariate normal distribution, and assumes gaussian distributions for each class share the same covariance matrix. If they don’t share the same covariance matrix, then we use Quadratic Discriminant Analysis which assumes each class has its own.</p>
</div>
<div id="assumptions-6" class="section level3">
<h3><span class="header-section-number">4.11.2</span> Assumptions</h3>
</div>
<div id="characteristics-1" class="section level3">
<h3><span class="header-section-number">4.11.3</span> Characteristics</h3>
</div>
<div id="evaluation-6" class="section level3">
<h3><span class="header-section-number">4.11.4</span> Evaluation</h3>
</div>
<div id="proscons-6" class="section level3">
<h3><span class="header-section-number">4.11.5</span> Pros/Cons</h3>
</div>
</div>
<div id="qda" class="section level2">
<h2><span class="header-section-number">4.12</span> QDA</h2>
<div id="intro-7" class="section level3">
<h3><span class="header-section-number">4.12.1</span> Intro</h3>
<p>TBD</p>
</div>
<div id="assumptions-7" class="section level3">
<h3><span class="header-section-number">4.12.2</span> Assumptions</h3>
</div>
<div id="characteristics-2" class="section level3">
<h3><span class="header-section-number">4.12.3</span> Characteristics</h3>
</div>
<div id="evaluation-7" class="section level3">
<h3><span class="header-section-number">4.12.4</span> Evaluation</h3>
</div>
<div id="proscons-7" class="section level3">
<h3><span class="header-section-number">4.12.5</span> Pros/Cons</h3>
</div>
</div>
<div id="advanced-bayesian" class="section level2">
<h2><span class="header-section-number">4.13</span> Advanced Bayesian</h2>
<ul>
<li><p>Approximate Bayesian Computation</p></li>
<li><p>Laplace approximation: Approximate the posterior of a non-conjugate model</p></li>
<li><p>When one uses likelihood to get point estimates of model parameters, it’s called maximum-likelihood estimation, or MLE. If one also takes the prior into account, then it’s maximum a posteriori estimation (MAP). MLE and MAP are the same if the prior is uniform.</p></li>
<li><p>Beta conjugacy: Beta for prior &amp; binomial for likelihoood implies beta for posterior. Update: beta(alpha + successes, beta + failures). Mean: alpha/(alpha + beta)</p></li>
<li><p>Small alpha and beta means the prior is less informative</p></li>
<li><p>Empirical Bayes is an approximation to more exact Bayesian methods. With a lot of data it is a very good approximation.</p></li>
</ul>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> numpy.random <span class="im">import</span> beta <span class="im">as</span> beta_dist
<span class="im">import</span> numpy <span class="im">as</span> np
N_samp <span class="op">=</span> <span class="dv">10000</span> <span class="co">#number of samples to draw</span>
clicks_A <span class="op">=</span> <span class="dv">450</span>
views_A <span class="op">=</span> <span class="dv">56000</span>
clicks_B <span class="op">=</span> <span class="dv">345</span>
views_B <span class="op">=</span> <span class="dv">49000</span>
alpha <span class="op">=</span> <span class="fl">1.1</span>
beta <span class="op">=</span> <span class="fl">14.2</span>
A_samples <span class="op">=</span> beta_dist(clicks_A <span class="op">+</span> alpha, views_A <span class="op">-</span> clicks_A <span class="op">+</span> beta, N_samp)
B_samples <span class="op">=</span> beta_dist(clicks_B <span class="op">+</span> alpha, views_B <span class="op">-</span> clicks_B <span class="op">+</span> beta, N_samp)</code></pre></div>
<p>Posterior prob that CTR_A &gt; CTR_B given data is np.mean(A_samples &gt; B_samples). Prob that lift of A relative to B is &gt;=3%: np.mean(100*(A_samples-B_samples)/B_samples &gt; 3)</p>
</div>
<div id="clustering" class="section level2">
<h2><span class="header-section-number">4.14</span> Clustering</h2>
<ul>
<li><p><a href="http://scikit-learn.org/stable/modules/clustering.html#clustering">Comparison</a></p></li>
<li><p><a href="https://twitter.com/thomaswdinsmore/status/965223193043718145">Rules</a></p></li>
<li><p><a href="https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68" class="uri">https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68</a></p></li>
</ul>
</div>
<div id="k-means" class="section level2">
<h2><span class="header-section-number">4.15</span> K-Means</h2>
<div id="intro-8" class="section level3">
<h3><span class="header-section-number">4.15.1</span> Intro</h3>
<p>K-Means clustering method considers two assumptions regarding the clusters – first that the clusters are spherical and second that the clusters are of similar size.</p>
<p>k-means assumes the variance of the distribution of each attribute (variable) is spherical; all variables have the same variance;</p>
<p>the prior probability for all k clusters is the same, i.e., each cluster has roughly equal number of observations;</p>
<p>Must choose k for clusters. Initialize cluster centes randomly, assign each point to its closest cluster by a distance metric. Recalculate centroids. Halt when cluster assignments no longer change. Clusters will be distinct and non-overlapping. Goal to minimize within-cluster variation. The within-cluster variation for the kth cluster is the sum of all of the pairwise squared Euclidean distances between the observations in the kth cluster divided by the total number of observations in the kth cluster. Limitation is that it depends on initialization of centroids. Guaranteed convergence but only to a local minima. Run the algorithm several times and pick the solution that yields the smallest within-cluster variance. Increasing k will decrease variance and increase bias and vice versa. K-means: Jaccard coefficient.</p>
<p>Note: Not enough to use within-cluster variance as this decreases as one increases k. Use elbow method or scree plot to choose optimal k where the variance no longer decreases dramatically. To choose how many groups to use, I ran a k-means analysis for each possible number of groups from 5 to 35 and found that 20 was right about the spot that the amount of variance stopped decreasing consistently. This is called the elbow test and while the results weren’t totally definitive, they fit in well with the general rule of thumb for determining # of groups which is the square root of the # of observations/2.</p>
<p>One way to assess whether a cluster represents true structure is to see if the cluster holds up under plausible variations in the dataset. The fpc package has a function called clusterboot() that uses bootstrap resampling to evaluate how stable a given cluster is.[3] clusterboot() is an integrated function that both performs the clustering and evaluates the final produced clusters. It has interfaces to a number of R clustering algorithms, including both hclust and kmeans.</p>
<p>In K-means, we assume that each cluster fits a Gaussian distribution (normal distribution)</p>
<p>We can set K to optimally cluster the data by starting with a small number of clusters, and then iteratively splitting them until all clusters fit a normal distribution.</p>
</div>
<div id="assumptions-8" class="section level3">
<h3><span class="header-section-number">4.15.2</span> Assumptions</h3>
</div>
<div id="characteristics-3" class="section level3">
<h3><span class="header-section-number">4.15.3</span> Characteristics</h3>
</div>
<div id="evaluation-8" class="section level3">
<h3><span class="header-section-number">4.15.4</span> Evaluation</h3>
<p>Davies–Bouldin index: This is an internal evaluation scheme, where the validation of how well the clustering has been done is made using quantities and features inherent to the dataset. This has a drawback that a good value reported by this method does not imply the best information retrieval.</p>
</div>
<div id="proscons-8" class="section level3">
<h3><span class="header-section-number">4.15.5</span> Pros/Cons</h3>
<p><strong>Pros</strong></p>
<p>Uses simple principle which can be explained in non-statistical terms. It is efficient.</p>
<p><strong>Cons</strong></p>
<p>Less sophisticated, random choices of centers, and requires guess for centers.</p>
</div>
</div>
<div id="hierarchical-clustering" class="section level2">
<h2><span class="header-section-number">4.16</span> Hierarchical Clustering</h2>
<div id="intro-9" class="section level3">
<h3><span class="header-section-number">4.16.1</span> Intro</h3>
<p>Generates dendrograms. The lower down a fusion occurs the more similar the group of observations and vice versa. Begin with n observations and a distance measure of all pairwise dissimialirities. Evaluate pairwise intercluster dissimiliarities among the clusters and fuse pair of clusters that are least dissimilar. Repeat process for remaining clusters. Continue for i=n to i=2. Need to pick a dissimilarity measure and a linkage method. Complete linkage: maximal inter-cluster dissimilarity, single linkage: minimal inter-cluster dissimilarity, avergae linkage: mean inter-cluster dissimilarity. Complete is sensitive to outliers but tends to produce compact clusters. Distance between groups: complete, average, and ward. Single not as sensitive to outliers but susceptible to chaining effect where clusters can often not represent intuitive groups. Average strikes a balance between the two. Usually standardize data before clustering. Sklearn.metrics has pairwise distances.</p>
<p>K-Means vs Hierarchical Clustering: K-means clustering needs the number of clusters to be speciﬁed. Hierarchical clustering doesn’t need the number of clusters to be speciﬁed. K-means clustering is usually more effcient run-time wise.</p>
</div>
<div id="assumptions-9" class="section level3">
<h3><span class="header-section-number">4.16.2</span> Assumptions</h3>
</div>
<div id="characteristics-4" class="section level3">
<h3><span class="header-section-number">4.16.3</span> Characteristics</h3>
</div>
<div id="evaluation-9" class="section level3">
<h3><span class="header-section-number">4.16.4</span> Evaluation</h3>
</div>
<div id="proscons-9" class="section level3">
<h3><span class="header-section-number">4.16.5</span> Pros/Cons</h3>
</div>
</div>
<div id="pca" class="section level2">
<h2><span class="header-section-number">4.17</span> PCA</h2>
<div id="intro-10" class="section level3">
<h3><span class="header-section-number">4.17.1</span> Intro</h3>
</div>
<div id="assumptions-10" class="section level3">
<h3><span class="header-section-number">4.17.2</span> Assumptions</h3>
</div>
<div id="characteristics-5" class="section level3">
<h3><span class="header-section-number">4.17.3</span> Characteristics</h3>
</div>
<div id="evaluation-10" class="section level3">
<h3><span class="header-section-number">4.17.4</span> Evaluation</h3>
</div>
<div id="proscons-10" class="section level3">
<h3><span class="header-section-number">4.17.5</span> Pros/Cons</h3>
<ul>
<li><p>Center and scale data. The eigenvectors yield orthogonal directions of greatest variability(principal components). The eigenvalues correspond to the magnitude of variancealong the principal components. Interpretability is a negative. PCA is completely nonparametric: any data set can be plugged in and an answer comes out, requiring no parameters to tweak and no regard for how the data was recorded. From one perspective, the fact that PCA is non-parametric (or plug-and-play) can be considered a positive feature because the answer is unique and independent of the user. From another perspective the fact that PCA is agnostic to the source of the data is also a weakness. When using dimensional reduction we restrict ourselves to simpler models. Thus, we expect bias to increase and variance to decrease. Getting latent components. Visualize high dimensional data. Reduce noise. Preprocessing. scale and center data before doing. Needs regularization. Too many principal components F1 score starts to drop after increase. Best F1 is 1. set number of components. Use fit_transforms, use explained_variance_ratio_ to see how much of the variance is explained by each component. Choose features that explain most of the variances, then use a inverse_transform to reconstruct your x. Minimum number of principal components is min(n,p).</p></li>
<li><p>pca: It accounts for as much of the variability in the data as possible by considering highly correlated features. Each succeeding component in turn has the highest variance using the features that are less correlated with the first principal component and that are orthogonal to the preceding component.</p></li>
</ul>
</div>
<div id="other-1" class="section level3">
<h3><span class="header-section-number">4.17.6</span> Other</h3>
<ul>
<li><ol style="list-style-type: decimal">
<li>look at variance: apply(USArrests , 2, var) | 2) pca &amp; scale: pca = prcomp(USArrests , scale = TRUE) | 3) plot: biplot(pca, scale = 0)</li>
</ol></li>
<li><p>pca$rotation contains the principal component loadings matrix which explains proportion of each variable along each principal component.</p></li>
<li><p>Kaiser-Harris criterion suggests retaining PCs with eigenvalues &gt; 1; PCs with eigenvalues &lt; 1 explain less variance than contained in a single variable. Cattell Scree test visually inspects the elbow graph for diminishing return; retain PCs before a drastic drop-off.</p></li>
</ul>
<p>PC columns contain loadings; correlations of the observed variables with the Pcs. h2 column displays the component comunalities; amount of variance explained by the components.</p>
<p>Note: Look at factorplot. Shows pot of relationship between variables and principal components.</p>
<p>It is also possible to decompress the reduced dataset back to 784 dimensions by applying the inverse transformation of the PCA projection. Of course this won’t give you back the original data, since the projection lost a bit of information (within the 5% variance that was dropped), but it will likely be quite close to the original data. The mean squared distance between the original data and the reconstructed data (compressed and then decompressed) is called the reconstruction error.</p>
<p>It turns out that many things behave very differently in high-dimensional space. For example, if you pick a random point in a unit square (a 1 × 1 square), it will have only about a 0.4% chance of being located less than 0.001 from a border (in other words, it is very unlikely that a random point will be “extreme” along any dimension). But in a 10,000-dimensional unit hypercube (a 1 × 1 × ⋯ × 1 cube, with ten thousand 1s), this probability is greater than 99.999999%. Most points in a high-dimensional hypercube are very close to the border.</p>
<p>Before looking at the PCA algorithm for dimensionality reduction in more detail, let’s summarize the approach in a few simple steps: Standardize the -dimensional dataset.</p>
<p>Construct the covariance matrix.Decompose the covariance matrix into its eigenvectors and eigenvalues. Select eigenvectors that correspond to the largest eigenvalues, where is the dimensionality of the new feature subspace ().</p>
<p>Construct a projection matrix from the “”top“” eigenvectors.</p>
<p>Transform the -dimensional input dataset using the projection matrix to obtain the new -dimensional feature subspace.</p>
<p>Probabilistic PCA introduces the Gaussian distribution to the PCA modeling framework.</p>
<p>Kernel PCA must project data into a higher dimensional space than that of the original data.</p>
</div>
</div>
<div id="knn" class="section level2">
<h2><span class="header-section-number">4.18</span> KNN</h2>
<div id="intro-11" class="section level3">
<h3><span class="header-section-number">4.18.1</span> Intro</h3>
<p>Calculate distance between each observation and all the others. Determine the k nearest observations to the observation. Classify the observation as the most frequent class of the k nearest observations. Small k’s are not robust to outliers, highlight local variations and induce unstable decision boundaries. Large k’s are robust to outliers, highlight global variations and induce stable decision boundaries. Good value to choose is k = sqrt(n). Can use maximum prior probability to decide ties. Use Euclidean distance for numerical data and Hamming distance for categorical data. Latter treats dimensions equally and is symmetric. Low k is low bias, high variance and high k is high bias low variance. 1NN can’t adapt to outliers and has no notion of class frequencies. Can use weighted KNN. Easy in sklearn.</p>
<p>curse of dimensionality, overfitting, correlated features, cost to update, sensitivity of distance metrics</p>
<p>smaller k means smaller training error, ut larger k is more stable.</p>
</div>
<div id="assumptions-11" class="section level3">
<h3><span class="header-section-number">4.18.2</span> Assumptions</h3>
</div>
<div id="characteristics-6" class="section level3">
<h3><span class="header-section-number">4.18.3</span> Characteristics</h3>
</div>
<div id="evaluation-11" class="section level3">
<h3><span class="header-section-number">4.18.4</span> Evaluation</h3>
</div>
<div id="proscons-11" class="section level3">
<h3><span class="header-section-number">4.18.5</span> Pros/Cons</h3>
<p><strong>Pros</strong></p>
<p>Only assumption is proximity. Non-parametric.</p>
<p>The only assumption we are making about our data is related to proximity (i. e., observations that are close by in the feature space are similar to each other in respect to the target value). We do not have to fit a model to the data since this is a non-parametric approach.</p>
<ul>
<li>Powerful</li>
<li>No training involved (“lazy”)</li>
<li>Naturally handles multiclass classification and regression</li>
</ul>
<p>What is a common drawback of 3NN classifiers? Prediction on large data sets is slow.</p>
<p><strong>Cons</strong></p>
<p>Have to decide k and distance metric. Can be sensitive to outliers or irrelevant attributes. Computationally expensive.</p>
<p>knn is terrible with high dimensions. Bad with categorical data.</p>
<p>Expensive and slow to predict new instances - Must define a meaningful distance function - Performs poorly on high-dimensionality datasets</p>
<p>We have to decide on K and a distance metric.</p>
<p>Can be sensitive to outliers or irrelevant attributes because they add noise.</p>
<p>Computationally expensive; as the number of observations, dimensions, and K increases, the time it takes for the algorithm to run and the space it takes to store the computations increases dramatically.</p>
</div>
</div>
<div id="svm" class="section level2">
<h2><span class="header-section-number">4.19</span> SVM</h2>
<div id="intro-12" class="section level3">
<h3><span class="header-section-number">4.19.1</span> Intro</h3>
<p>Selection of margin in SVM is a convex optimization problem. What is the objective of the maximum margin approach? To ensure small prediction error when the underlying distribution is not known</p>
<p>The LinearSVC class regularizes the bias term, so you should center the training set first by subtracting its mean. This is automatic if you scale the data using the StandardScaler. Moreover, make sure you set the loss hyperparameter to “hinge”, as it is not the default value. Finally, for better performance you should set the dual hyperparameter to False, unless there are more features than training instances (we will discuss duality later in the chapter).</p>
<p>With so many kernels to choose from, how can you decide which one to use? As a rule of thumb, you should always try the linear kernel first (remember that LinearSVC is much faster than SVC(kernel=“linear”)), especially if the training set is very large or if it has plenty of features. If the training set is not too large, you should try the Gaussian RBF kernel as well; it works well in most cases. Then if you have spare time and computing power, you can also experiment with a few other kernels using cross-validation and grid search, especially if there are kernels specialized for your training set’s data structure.</p>
<p>Here mostly talking about a maximal margin classifier. A separating hyperline of data of two classes. Maximizes distance to the nearest point in either class, i.e, the margin. Points which delineate the boundary are called support vectors. Correct classification first and then maximizing the margin. SVM only really works well with linear hyperplanes. The c hyperparameter deals with the tradeoff between a smooth decision boundary and correct classification. The larger the c the more points classified correctly as it increases the penalty for wrong classification. A smaller c gives more room for intial error which helps improve accuracy for not easily separable data. Work well in complicated domains with clear line of separation. Not so well in for large datasets because of training and not when there is a lot of noise. Can solve nonlinear decision boundaries by using polynomial terms. Kernel trick that draws boundaries by looking into higher dimensions.</p>
<p>Extracting the coefficients from the hyperplane equation (not including the intercept) yields what is called a normal vector. This vector points in a direction orthogonal to the surface of the hyperplane and essentially defines its orientation.</p>
<p>Big data set and lot of features SVM might be slow and prone to overfitting. SVM slower than naïve bayes. A kernelis a function that quantifies the similarity of two observations. The beauty of the “kernel trick” is that, even if there is an infinite-dimensional basis, we need only look at the n^2 inner products between training data points. SVM not scale invariant. Check if library normalizes by default. RBF kernel is a good default. Try exponential sequences for parameters.</p>
<p>Extension of above info about relaxing margin. Doesn’t perfectly separate classes but is more robust to outliers. Helps better classify obervations. Take a penalty by potentially misclassifying soe observations. Have better predictive power. Called a soft margin. The C parameter is the budget for the slack variables which tell where observations are relative to the margin and hyperplane. Only observations that either fall on the margin or violate the margin affect the solution to the optimization problem. More robust than SVM.</p>
<p>When the support vector classifier is combined with a non-linear kernel, the resulting classifier is called a support vector machine.</p>
<p>Note: Important to make sure the normal vector is of unit length. For the radial kernel, suppose we have a test observation. If it is far from a training observation, the Euclidean distance will be large, but the value of the radial kernel will be small. using kernels is much more computationally efficientbecause we only need to compute the kernel for distinct pairs of observations in our dataset. Furthermore, we need not actually work in the enlarged feature space. Intuitively, the gamma parameter defines how far the influence of a single training example reaches, with low values meaning ‘far’ and high values meaning ‘close’. The gamma parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors.</p>
<p>Multiclass</p>
<p>1v1: Construct a support vector machine for each pair of categories. For each classifier, record the prediction for each observation. Have the classifiers vote on the prediction for each observation.</p>
<p>1vAll: Construct a support vector machine for each individual category against all other categories combined. Assign the observation to the classifier with the largest function value.</p>
</div>
<div id="assumptions-12" class="section level3">
<h3><span class="header-section-number">4.19.2</span> Assumptions</h3>
</div>
<div id="characteristics-7" class="section level3">
<h3><span class="header-section-number">4.19.3</span> Characteristics</h3>
</div>
<div id="evaluation-12" class="section level3">
<h3><span class="header-section-number">4.19.4</span> Evaluation</h3>
</div>
<div id="proscons-12" class="section level3">
<h3><span class="header-section-number">4.19.5</span> Pros/Cons</h3>
<p><strong>Pros</strong></p>
<ul>
<li><p>Performs similarly to logistic regression when linear separation</p></li>
<li><p>Performs well with non-linear boundary depending on the kernel used</p></li>
<li><p>Handle high dimensional data well</p></li>
<li><p>Can model complex, nonlinear relationships</p></li>
<li><p>Robust to noise (because they maximize margins)</p></li>
</ul>
<p><strong>Cons</strong></p>
<ul>
<li><p>Susceptible to overfitting/training issues depending on kernel</p></li>
<li><p>Need to select a good kernel function</p></li>
<li><p>Model parameters are difficult to interpret</p></li>
<li><p>Sometimes numerical stability problems</p></li>
<li><p>Requires significant memory and processing power</p></li>
</ul>
</div>
</div>
<div id="decision-tree" class="section level2">
<h2><span class="header-section-number">4.20</span> Decision Tree</h2>
<div id="intro-13" class="section level3">
<h3><span class="header-section-number">4.20.1</span> Intro</h3>
<p>So should you use Gini impurity or entropy? The truth is, most of the time it does not make a big difference: they lead to similar trees. Gini impurity is slightly faster to compute, so it is a good default. However, when they differ, Gini impurity tends to isolate the most frequent class in its own branch of the tree, while entropy tends to produce slightly more balanced trees.</p>
<p>Decision Trees make very few assumptions about the training data (as opposed to linear models, which obviously assume that the data is linear, for example). Such a model is often called a nonparametric model, not because it does not have any parameters (it often has a lot) but because the number of parameters is not determined prior to training, so the model structure is free to stick closely to the data.</p>
<p>More generally, the main issue with Decision Trees is that they are very sensitive to small variations in the training data. For example, if you just remove the widest Iris-Versicolor from the iris training set (the one with petals 4.8 cm long and 1.8 cm wide) and train a new Decision Tree, you may get the model represented in Figure 6-8. As you can see, it looks very different from the previous Decision Tree (Figure 6-2). Actually, since the training algorithm used by Scikit-Learn is stochastic6 you may get very different models even on the same training data (unless you set the random_state hyperparameter).</p>
<p>Construct solutions that stratify the feature space into relatively easy to describe regions. Partition predictor space into rectangular or box-like regions. For regression predict the meant of the reponse variables in each region for the training observations in that space. Top down and greedy approach called recursive binary spltting to find ebst solution. Greedy because splits based on best result. The recursive binary splitting process depends on the greatest reduction in the RSS. Entropy very important. Decides where tree splits. Measure of impurity in examples. Entropy between 0 and 1. 0 means no impurity. Maximial is 1 when there is an even split between options. Key term: information gain. Decisions tree maximize this. Easy to interpret, prone to overfitting. Where each data point has its own node, the variance is high and the training error is 0. Deciding on the number of splits prior to building a tree isn’t the best strategy. The best subtree will be the one that yields the lowest test error rate. Given a subtree, we can estimate the test error by implementing the crossvalidationprocess, but this is too cumbersome because the large number of possible subtrees. The tuning parameter alpha helps balance the tradeoff between the overall complexity of the tree and its fit to the training data. Small values of alpha yield trees that are quite extensive (have many terminal nodes). Large values of alpha yield trees that are quite limited (have few terminal nodes). Similar to ridge/lasso regression. Ultimately, the subtree that is used for prediction is built using all of the available data with the determined optimal value of alpha.</p>
<p>Regression: Use cost complexity pruning to build set of subtrees as a function of lambda. Classification: Use Gini index, measures total variance among classes.</p>
<p>Note: Duplicate splits happen when they increase node purity. While pruning reduces the variance of the overall tree model upon repeated builds with different datasets, we induce biasbecause the trees are much simpler.</p>
</div>
<div id="assumptions-13" class="section level3">
<h3><span class="header-section-number">4.20.2</span> Assumptions</h3>
</div>
<div id="characteristics-8" class="section level3">
<h3><span class="header-section-number">4.20.3</span> Characteristics</h3>
</div>
<div id="evaluation-13" class="section level3">
<h3><span class="header-section-number">4.20.4</span> Evaluation</h3>
</div>
<div id="proscons-13" class="section level3">
<h3><span class="header-section-number">4.20.5</span> Pros/Cons</h3>
<p><strong>Pros</strong></p>
<ul>
<li><p>Fast</p></li>
<li><p>Robust to noise and missing values</p></li>
<li><p>Accurate</p></li>
</ul>
<p><strong>Cons</strong></p>
<ul>
<li><p>Weak predictive accuracy and prone to overfitting.</p></li>
<li><p>Complex trees are hard to interpret</p></li>
<li><p>Duplication within the same sub-tree is possible</p></li>
</ul>
</div>
</div>
<div id="bagging" class="section level2">
<h2><span class="header-section-number">4.21</span> Bagging</h2>
<div id="intro-14" class="section level3">
<h3><span class="header-section-number">4.21.1</span> Intro</h3>
<p>used to decrease variance in decision trees</p>
<p>How it works: train multiple trees using bootstrapped datasets, regression trees: average the predictions from all the trees, classification: take a majority vote of the predictions, majority vote: the class which occurs most frequently, Out of bag (OOB) error, Only a portion of the data is used to train each tree, The remaining data which wasn’t selected can be used to assess the tree’s performance, (the response for each observation is predicted using only the trees that were not fit using that observation)</p>
<p>Bagging involves randomly generating Bootstrap samples from the dataset and trains the models individually. Predictions are then made by aggregating or averaging all the response variables:For example, consider a dataset (Xi, Yi), where i=1 …n, contains n data points.Now, randomly select B samples with replacements from the original dataset using Bootstrap technique.Next, train the B samples with regression/classification models independently. Then, predictions are made on the test set by averaging the responses from all the B models generated in the case of regression. Alternatively, the most often occurring class among B samples is generated in the case of classification.</p>
<p>Bagging stabilizes decision trees and improves accuracy by reducing variance.</p>
<p>Bagging reduces generalization error.</p>
<p>Take repeated samples of the same size from the single overall training dataset. Treat these different sets of data as pseudo-training sets. Fitting separate, independent decision treesto each of the bootstrapped training data sets. We can then average all predictions (or take the majority vote) to obtain the bagged estimate. Instead of pruning back our trees, create very large trees in the first place. These large trees will tend to have low bias, but high variance. Retain the low bias, but get rid of the high variance by averaging. Con is that the trees are correlation. train multiple trees using bootstrapped data to reduce variance and prevent overfitting</p>
</div>
<div id="assumptions-14" class="section level3">
<h3><span class="header-section-number">4.21.2</span> Assumptions</h3>
</div>
<div id="characteristics-9" class="section level3">
<h3><span class="header-section-number">4.21.3</span> Characteristics</h3>
</div>
<div id="evaluation-14" class="section level3">
<h3><span class="header-section-number">4.21.4</span> Evaluation</h3>
</div>
<div id="proscons-14" class="section level3">
<h3><span class="header-section-number">4.21.5</span> Pros/Cons</h3>
<p><strong>Pros</strong></p>
<ul>
<li><p>reduces variance in comparison to regular decision trees</p></li>
<li><p>Can provide variable importance measures: classification: Gini index + regression: RSS</p></li>
<li><p>Can easily handle qualitative (categorical) features</p></li>
<li><p>Out of bag (OOB) estimates can be used for model validation</p></li>
</ul>
<p><strong>Cons</strong></p>
<ul>
<li><p>Not as easy to visually interpret</p></li>
<li><p>Does not reduce variance if the features are correlated</p></li>
</ul>
</div>
</div>
<div id="random-forest" class="section level2">
<h2><span class="header-section-number">4.22</span> Random Forest</h2>
<div id="intro-15" class="section level3">
<h3><span class="header-section-number">4.22.1</span> Intro</h3>
<p>Build a number of decision trees using bootstrapped samples</p>
<p>At each split in the tree only a portion of the features are considered</p>
<p>a feature is selected from a subset of features not the whole feature space</p>
<p>the subset is usually sqrt(n.features)</p>
<p>This allows trees to be built without some of the strong features decorrelates the trees, unlike bagging</p>
<p>Random forests are improvised supervised algorithms than bootstrap aggregation or bagging methods, though they are built on a similar approach. Unlike selecting all the variables in all the B samples generated using the Bootstrap technique in bagging, we select only a few predictor variables randomly from the total variables for each of the B samples. Then, these samples are trained with the models. Predictions are made by averaging the result of each model. The number of predictors in each sample is decided using the formula m = √p, where p is the total variable count in the original dataset.Here are some key <a href="notes:This" class="uri">notes:This</a> approach removes the condition of dependency of strong predictors in the dataset as we intentionally select fewer variables.</p>
<p>Draw a random bootstrap sample of size n (randomly choose n samples from the training set with replacement).Grow a decision tree from the bootstrap sample. At each node:Randomly select d features without replacement.Split the node using the feature that provides the best split according to the objective function, for instance, by maximizing the information gain.Repeat the steps 1 to 2 k times.Aggregate the prediction by each tree to assign the class label by majority vote. Majority voting will be discussed in more detail in Chapter 7, Combining Different Models for Ensemble Learning.</p>
<p>Random forests further improve decision tree performance by de-correlating the individual trees in the bagging ensemble.</p>
<p>Random forests decorrelate tree it generates and thus results in a reduction in variance. Similar to bagging, we first build various decision trees on bootstrapped training samples, but we split internal nodes in a special way. Each time a split is considered within the construction of a decision tree, only a random subset of ᬊthe overall predictors are allowed to be candidates. At every split, a new subset of predictors is randomly selected. The random forest procedure forces the decision tree building process to use different predictors to split at different times. Should a good predictor be left out of consideration for some splits, it still has many chances to be consideredin the construction of other splits. We can’t overfit by adding more trees. The variance just ends up decreasing. Out of bag score is a good estimate of the test score and is the non-bootstrapped data. Need to turn on this scoing method. Error Rate: Depends on correlation between trees (higher is worse), strength of single trees (higher is better). Increasing number of features for each split increases correlation and strength of single trees.</p>
</div>
<div id="assumptions-15" class="section level3">
<h3><span class="header-section-number">4.22.2</span> Assumptions</h3>
</div>
<div id="characteristics-10" class="section level3">
<h3><span class="header-section-number">4.22.3</span> Characteristics</h3>
</div>
<div id="evaluation-15" class="section level3">
<h3><span class="header-section-number">4.22.4</span> Evaluation</h3>
</div>
<div id="proscons-15" class="section level3">
<h3><span class="header-section-number">4.22.5</span> Pros/Cons</h3>
<p><strong>Pros</strong></p>
<p>Decorrelates trees (relative to boosted trees)</p>
<p>important when dealing with mulitple features which may be correlated</p>
<p>reduced variance (relative to regular trees)</p>
<p>All purpose model that performs well on most problems. Automatically chooses the best features. High accuracy.</p>
<p><strong>Cons</strong></p>
<p>Hard to interpret and needs a lot of work to tune parameters.</p>
<p>Not as easy to visually interpret</p>
</div>
</div>
<div id="boosting" class="section level2">
<h2><span class="header-section-number">4.23</span> Boosting</h2>
<div id="intro-16" class="section level3">
<h3><span class="header-section-number">4.23.1</span> Intro</h3>
<p>Unlike boosting and random forests, can overfit if number of trees is too large</p>
<p>Similar to bagging, but trees are grown sequentially. Each tree is created using information from previously grown trees</p>
<p>Each tree is generated using information from previously grown trees; the addition of a new tree improves upon the performance of the previous trees. The trees are now dependentupon one another. Boosting approach tends to slowly learn our data. Given a current decision tree model, we fit a new decision tree to the residualsof the current decision tree. The new decision tree (based on the residuals) is then added to the current decision tree, and the residuals are updated. We limit the number of terminal nodes in order to sequentially fit small trees. By fitting small trees to the residuals, we slowly improve the overall model in areas where it does not perform well.</p>
<p>Note: Unlike in bagging and random forests, boosting can overfit if ᫦is large (although very slowly). Use cross-validation to select number of trees᫦. Alpha controls the rate at which boosting learns and is usually between 0.01 to 0.001. A small alpha usually goes with a large number of trees. Can also choose the number of splits. Typically using stumps (single splits d = 1) is sufficient and results in an additive model.</p>
<p>Note on Variable Importance: For regression trees, we can use the reduction in the RSS. For classification trees, we can use the reduction in the Gini index. A relatively large value indicates a notable drop in the RSS or Gini index, and thus a better fit to the data; corresponding variables are relatively important predictors.</p>
<p>Similar to bagging, but learns sequentially and builds off previous trees</p>
</div>
<div id="assumptions-16" class="section level3">
<h3><span class="header-section-number">4.23.2</span> Assumptions</h3>
</div>
<div id="characteristics-11" class="section level3">
<h3><span class="header-section-number">4.23.3</span> Characteristics</h3>
</div>
<div id="evaluation-16" class="section level3">
<h3><span class="header-section-number">4.23.4</span> Evaluation</h3>
</div>
<div id="proscons-16" class="section level3">
<h3><span class="header-section-number">4.23.5</span> Pros/Cons</h3>
<p><strong>Pros</strong></p>
<p>Somewhat more interpretable than boosted trees/random forest as the user can define the size of each tree resulting in a collection of stumps (1 level) which can be viewed as an additive model</p>
<p>Can easily handle qualitative (categorical) features</p>
<p><strong>Cons</strong></p>
</div>
</div>
<div id="nlp" class="section level2">
<h2><span class="header-section-number">4.24</span> NLP</h2>
<ul>
<li>TF-IDF: A feature vectorization method widely used in text mining to reflect the importance of a term to a document in the corpus. Denote a term by t, a document by d, and the corpus by D. TF(i,d) is the number of times the term t appears in document d while document frequency DF(t,D) is the number of documents which contain the term t. If a term appears very often across the corpus it doesn’t carry any special info about the document. Inverse document frequency is a numerical measure of how much info a term provides. If a term appears in all documents its IDF value is 0. TF-IDF is the product of TF and IDF. HashingTF uses a hash function to transform input into text.</li>
</ul>
<p><strong>Basic Py NLP Pipeline</strong></p>
<ul>
<li><p>Converting text to numerical data is vectorization.</p></li>
<li><p>Tokenize: A token is an individual word (or group of words) extracted from a document, using whitespace or punctuation as separators.</p></li>
<li><p>Create bag of words: Count tokens within a document and normalised to de-emphasise tokens that appear frequently within a document.</p></li>
<li><p>Vectorization: Corpus represented as a large matrix, each row of which represents one of the documents and each column represents token occurance within that document. CountVectorizer or TF-IDF.</p></li>
<li><p>tf-idf &amp; cosine similarity</p></li>
</ul>
</div>
<div id="topic-modeling" class="section level2">
<h2><span class="header-section-number">4.25</span> Topic Modeling</h2>
<ul>
<li><p>Pointwise mutual information is any way to evaluate topic models. Uses joint probabilities to evaluate how much a word tells you a label.</p></li>
<li><p>Mutual information generalized pointwise mutual information averaged over all events.</p></li>
<li><p>If you’re just using the probability vectors that come out of a topic model you are selecting a probability metric. If you want to do post-processing you something like mutual information or point was mutual information you’re selecting another metric. Metrics always have a point of view.</p></li>
<li><p>They’re just like human writers. Probability says I like really common words, pointwise mutual information as I like really really really discerning words and I don’t care how common they are. Mutual information is sort of a balance between the two.</p></li>
<li><p>In topic modeling using latent Dirichlet distribution, how are “topics” represented? As distributions over words.</p></li>
</ul>
</div>
<div id="recommendation-systems" class="section level2">
<h2><span class="header-section-number">4.26</span> Recommendation Systems</h2>
<p>Algorithms for Personalization: Linear/Logistic/Elastic Net Regression, Restricted Boltzmann Machines, Singular Value Decomposition, Makov Chains, Latent Dirichlet Allocation, Association Rules, Gradient Boosted Decision Trees, Random Forests, Affinity Propagation, K-Means, Matix Factorization, Alternating Least Squares.</p>
</div>
<div id="time-series" class="section level2">
<h2><span class="header-section-number">4.27</span> Time Series</h2>
<div id="intro-17" class="section level3">
<h3><span class="header-section-number">4.27.1</span> Intro</h3>
</div>
<div id="assumptions-17" class="section level3">
<h3><span class="header-section-number">4.27.2</span> Assumptions</h3>
</div>
<div id="characteristics-12" class="section level3">
<h3><span class="header-section-number">4.27.3</span> Characteristics</h3>
</div>
<div id="evaluation-17" class="section level3">
<h3><span class="header-section-number">4.27.4</span> Evaluation</h3>
</div>
<div id="proscons-17" class="section level3">
<h3><span class="header-section-number">4.27.5</span> Pros/Cons</h3>
<p><strong>Pros</strong></p>
<p><strong>Cons</strong></p>
<ul>
<li><p>Seasonal Decomposition; Use additive model when the magnitude of the seasonal fluctuations surrounding the general trend doesn’t vary over time. Use the multiplicative model when the magnitude of the seasonal fluctuations surrounding the general trend appear to change in a proportional manner over time. Go from additive to multiplicative with a log transformation.</p></li>
<li><p>White noise: Describes the assumption that each element in the time series is a random draw from N(0, constant variance). Also called a stationary series. Time series with trends or seasonality are not stationary.</p></li>
</ul>
<p><strong>ARIMA(p,d,q) (Auto-Regressive Integrated Moving Average) Models</strong></p>
<ul>
<li><p>AR(p): auto-regressive component for lags on the stationary series. The AR models sayd that the value of a variable at a specific time is related to the value of the variable at previous times.</p></li>
<li><p>I(d): Integrated component for a series that needs to be differenced.</p></li>
<li><p>MA(q): Moving average component for lag of the forecast errors. In a moving average model of order q, each value in a time series is predicted from the linear combination of the previous q errors. The value of a variable at a specific time is related to the residuals of prediction at previous times.</p></li>
<li><p>In an auto-regressive model of order p, each valeu in a time series is predicted from a linear combination of the previous p values.</p></li>
<li><p>Procedure: Make stationary if necessary by differencing. Determine possible values of p and q. Assess model fit and try other values of p and q to overfit. Make forecasts with the final model.</p></li>
<li><p>Augmented Dicky-Fuller Test: This tests whether or not a time series is stationary. h0: series is not stationary, ha: the series is stationary.</p></li>
<li><p>Determine p and q: Look at autocorrelation (AC) and partial correlation (PAC) functions. AC measures the way observations relate to each other. PAC measure the way observations relate to each other after accounting for all other intervening observations. Plot of the autocorrelation function (ACF) displays correlation of the series with itself at different lags. A plot of the PAC displays the amount of autocorrelation not explained by lower order correlations. Spikes in the PACF will choose for AR(p). Spikes in the ACF will choose MA(q).</p></li>
</ul>
<p><strong>Assess Model Fit</strong></p>
<ul>
<li><p>Appropriate model should resemble white noise. Check scatterplot of residuals vs fit to check constant variance and qqplot to check normality. Autocorrelations should be zero to check for violation of independent errors.</p></li>
<li><p>Box-Ljung Test: Check if all autocorrelations are zero, i.e the series is of white noise. H0: autocorrelations are all 0. ha: At least one is nonzero.</p></li>
<li><p>Overfit model with extra AR/MA terms and compare using AIC/BIC.</p></li>
<li><p>Interpretation: AR coefficient closer to 1 means series returns to mean slowly, vice versa for closer to 0.</p></li>
<li><p>MA(1) coefficient indicates how much the shock of the previous time period is retained in the current time period. MA(2) refers to the previous two time periods.</p></li>
</ul>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="transform-visualize.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="udacity-ts-fundamentals.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/03-model.Rmd",
"text": "Edit"
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
