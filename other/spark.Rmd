---
title: "Bayesian"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## PySpark

https://www.qubole.com/resources/pyspark-cheatsheet/

Quick Test:

```{python}
import findspark
findspark.init()
import pyspark
import random

sc = pyspark.SparkContext(appName="Pi")
num_samples = 100000000
def inside(p):     
    x, y = random.random(), random.random()
    return x*x + y*y < 1
count = sc.parallelize(range(0, num_samples)).filter(inside).count()
pi = 4 * count / num_samples
print(pi)

sc.stop()
```

On Windows: setx SPARK_HOME <path>

To start working with Spark DataFrames, you first have to create a SparkSession object from your SparkContext. You can think of the SparkContext as your connection to the cluster and the SparkSession as your interface with that connection.

import pyspark
from pyspark.sql import SparkSession
sc = pyspark.SparkContext()
spark = SparkSession.builder.getOrCreate()

Your SparkSession has an attribute called catalog which lists all the data inside the cluster. This attribute has a few methods for extracting different pieces of information.

spark.catalog.listTables()

query = "FROM flights SELECT * LIMIT 10"
flights10 = spark.sql(query)
flights10.show()

df['g'] = df['g'].astype(str)

spark = SparkSession.builder.appName('chosenName').getOrCreate()

df=spark.read.csv('fileNameWithPath', mode="DROPMALFORMED",inferSchema=True, header = True)

df = spark.read.format("csv").option("header","true").option("inferSchema","true").load("FileStore/tables/pzufk5ib1500654887654/campaign.csv")

airports = spark.read.csv(file_path, header = True)

toPandas() / table

Create spark_temp from pd_temp
spark_temp = spark.createDataFrame(pd_temp)

Add spark_temp to the catalog
spark_temp.createOrReplaceTempView("temp")

Examine the tables in the catalog again
print(spark.catalog.listTables())

df = df.withColumn("newCol", df.oldCol + 1)

Create the DataFrame flights
flights = spark.table('flights')

col names
spark_df.schema.names
spark_df.printSchema()

Let's take a look at the .filter() method. As you might suspect, this is the Spark counterpart of SQL's WHERE clause. The .filter() method takes either a Spark Column of boolean (True/False) values or the WHERE clause of a SQL expression as a string.

long_flights1 = flights.filter('distance > 1000')

Filter flights with a boolean column
long_flights2 = flights.filter(flights.distance > 1000)

You can also use the .alias() method to rename a column you're selecting.

The equivalent Spark DataFrame method .selectExpr() takes SQL expressions as a string

Find the shortest flight from PDX in terms of distance
flights.filter(flights.origin == "PDX").groupBy().min("distance").show()

Average duration of Delta flights
flights.filter(flights.carrier=='DL').filter(flights.origin=='SEA').groupBy().avg('air_time').show()

Total hours in the air
flights.withColumn("duration_hrs", flights.air_time/60).groupBy().sum('duration_hrs').show()

.show() vs collect()

import pyspark.sql.functions as F

Standard deviation
by_month_dest.agg(F.stddev('dep_delay')).show()


Spark only handles numeric data.

final_test_data.drop('State')

At the core of the pyspark.ml module are the Transformer and Estimator classes. Transformer classes are used for pre-processing and have a .transform() method. eg. PCA

Estimator classes are for modeling and all implement a .fit() method. eg. StringIndexerModel for including categorical data saved as strings in your models

cast() method: nominative determinism on columns

withColumn(): create new column

model_data = model_data.withColumn("arr_delay", model_data.arr_delay.cast('integer'))

model_data = model_data.withColumn("plane_age", model_data.year - model_data.plane_year)

Remove missing values: SQL

model_data = model_data.filter("arr_delay is not NULL and dep_delay is not NULL and air_time is not NULL and plane_year is not NULL")

pyspark.ml.feature

You can create what are called 'one-hot vectors' to represent the carrier and the destination of each flight. A one-hot vector is a way of representing a categorical feature where every observation has a vector in which all elements are zero except for at most one element, which has a value of one (1).

Dummying

The first step to encoding your categorical feature is to create a StringIndexer. Members of this class are Estimators that take a DataFrame with a column of strings and map each unique string to a number. Then, the Estimator returns a Transformer that takes a DataFrame, attaches the mapping to it as metadata, and returns a new DataFrame with a numeric column corresponding to the string column.

The second step is to encode this numeric column as a one-hot vector using a OneHotEncoder. This works exactly the same way as the StringIndexer by creating an Estimator and then a Transformer

Create a StringIndexer

carr_indexer = StringIndexer(inputCol="carrier", outputCol="carrier_index")

Create a OneHotEncoder

carr_encoder = OneHotEncoder(inputCol="carrier_index", outputCol="carrier_fact")

Make a VectorAssembler

vec_assembler = VectorAssembler(inputCols=["month", "air_time", "carrier_fact", "dest_fact", "plane_age"], outputCol="features")

 Import Pipeline
from pyspark.ml import Pipeline

 Make the pipeline
flights_pipe = Pipeline(stages=[dest_indexer, dest_encoder, carr_indexer, carr_encoder, vec_assembler])

In Spark it's important to make sure you split the data after all the transformations. This is because operations like StringIndexer don't always produce the same index even when given the same list of strings.

 Fit and transform the data
piped_data = flights_pipe.fit(model_data).transform(model_data)

 Split the data into training and test sets
training, test = piped_data.randomSplit([.6, .4])

**Tuning & Selection**

 Import LogisticRegression
from pyspark.ml.classification import LogisticRegression

 Create a LogisticRegression Estimator
lr = LogisticRegression()

 Import the evaluation submodule
import pyspark.ml.evaluation as evals

 Create a BinaryClassificationEvaluator
evaluator = evals.BinaryClassificationEvaluator(metricName="areaUnderROC")

 Import the tuning submodule
import pyspark.ml.tuning as tune

 Create the parameter grid
grid = tune.ParamGridBuilder()

 Add the hyperparameter
grid = grid.addGrid(lr.regParam, np.arange(0, .1, .01))
grid = grid.addGrid(lr.elasticNetParam, [0, 1])

 Build the grid
grid = grid.build()

 Create the CrossValidator
cv = tune.CrossValidator(estimator=lr,
               estimatorParamMaps=grid,
               evaluator=evaluator
               )

 Fit cross validation models
models = cv.fit(training)

 Extract the best model
best_lr = models.bestModel

 Use the model to predict the test set
test_results = best_lr.transform(test)

 Evaluate the predictions
print(evaluator.evaluate(test_results))

**Find out how Spark finds out which columns are the features.**

label (this is the default name for the response variable in Spark's machine learning routines

## Sparklyr



