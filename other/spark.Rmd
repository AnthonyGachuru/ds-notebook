---
title: "Bayesian"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## PySpark

https://www.qubole.com/resources/pyspark-cheatsheet/

Quick Test:

```{python}
import findspark
findspark.init()
import pyspark
import random

sc = pyspark.SparkContext(appName="Pi")
num_samples = 100000000
def inside(p):     
    x, y = random.random(), random.random()
    return x*x + y*y < 1
count = sc.parallelize(range(0, num_samples)).filter(inside).count()
pi = 4 * count / num_samples
print(pi)

sc.stop()
```

On Windows: setx SPARK_HOME <path>

To start working with Spark DataFrames, you first have to create a SparkSession object from your SparkContext. You can think of the SparkContext as your connection to the cluster and the SparkSession as your interface with that connection.

import pyspark
from pyspark.sql import SparkSession
sc = pyspark.SparkContext()
spark = SparkSession.builder.getOrCreate()

Your SparkSession has an attribute called catalog which lists all the data inside the cluster. This attribute has a few methods for extracting different pieces of information.

spark.catalog.listTables()

query = "FROM flights SELECT * LIMIT 10"
flights10 = spark.sql(query)
flights10.show()

df['g'] = df['g'].astype(str)

spark = SparkSession.builder.appName('chosenName').getOrCreate()

df=spark.read.csv('fileNameWithPath', mode="DROPMALFORMED",inferSchema=True, header = True)

df = spark.read.format("csv").option("header","true").option("inferSchema","true").load("FileStore/tables/pzufk5ib1500654887654/campaign.csv")

airports = spark.read.csv(file_path, header = True)

toPandas() / table

Create spark_temp from pd_temp
spark_temp = spark.createDataFrame(pd_temp)

Add spark_temp to the catalog
spark_temp.createOrReplaceTempView("temp")

Examine the tables in the catalog again
print(spark.catalog.listTables())

df = df.withColumn("newCol", df.oldCol + 1)

Create the DataFrame flights
flights = spark.table('flights')

col names
spark_df.schema.names
spark_df.printSchema()

Let's take a look at the .filter() method. As you might suspect, this is the Spark counterpart of SQL's WHERE clause. The .filter() method takes either a Spark Column of boolean (True/False) values or the WHERE clause of a SQL expression as a string.

long_flights1 = flights.filter('distance > 1000')

Filter flights with a boolean column
long_flights2 = flights.filter(flights.distance > 1000)

You can also use the .alias() method to rename a column you're selecting.

The equivalent Spark DataFrame method .selectExpr() takes SQL expressions as a string

Find the shortest flight from PDX in terms of distance
flights.filter(flights.origin == "PDX").groupBy().min("distance").show()

Average duration of Delta flights
flights.filter(flights.carrier=='DL').filter(flights.origin=='SEA').groupBy().avg('air_time').show()

Total hours in the air
flights.withColumn("duration_hrs", flights.air_time/60).groupBy().sum('duration_hrs').show()

.show() vs collect()

import pyspark.sql.functions as F

Standard deviation
by_month_dest.agg(F.stddev('dep_delay')).show()


Spark only handles numeric data.

final_test_data.drop('State')

At the core of the pyspark.ml module are the Transformer and Estimator classes. Transformer classes are used for pre-processing and have a .transform() method. eg. PCA

Estimator classes are for modeling and all implement a .fit() method. eg. StringIndexerModel for including categorical data saved as strings in your models

cast() method: nominative determinism on columns

withColumn(): create new column

model_data = model_data.withColumn("arr_delay", model_data.arr_delay.cast('integer'))

model_data = model_data.withColumn("plane_age", model_data.year - model_data.plane_year)

Remove missing values: SQL

model_data = model_data.filter("arr_delay is not NULL and dep_delay is not NULL and air_time is not NULL and plane_year is not NULL")

pyspark.ml.feature

You can create what are called 'one-hot vectors' to represent the carrier and the destination of each flight. A one-hot vector is a way of representing a categorical feature where every observation has a vector in which all elements are zero except for at most one element, which has a value of one (1).

Dummying

The first step to encoding your categorical feature is to create a StringIndexer. Members of this class are Estimators that take a DataFrame with a column of strings and map each unique string to a number. Then, the Estimator returns a Transformer that takes a DataFrame, attaches the mapping to it as metadata, and returns a new DataFrame with a numeric column corresponding to the string column.

The second step is to encode this numeric column as a one-hot vector using a OneHotEncoder. This works exactly the same way as the StringIndexer by creating an Estimator and then a Transformer

Create a StringIndexer

carr_indexer = StringIndexer(inputCol="carrier", outputCol="carrier_index")

Create a OneHotEncoder

carr_encoder = OneHotEncoder(inputCol="carrier_index", outputCol="carrier_fact")

Make a VectorAssembler

vec_assembler = VectorAssembler(inputCols=["month", "air_time", "carrier_fact", "dest_fact", "plane_age"], outputCol="features")

 Import Pipeline
from pyspark.ml import Pipeline

 Make the pipeline
flights_pipe = Pipeline(stages=[dest_indexer, dest_encoder, carr_indexer, carr_encoder, vec_assembler])

In Spark it's important to make sure you split the data after all the transformations. This is because operations like StringIndexer don't always produce the same index even when given the same list of strings.

 Fit and transform the data
piped_data = flights_pipe.fit(model_data).transform(model_data)

 Split the data into training and test sets
training, test = piped_data.randomSplit([.6, .4])

**Tuning & Selection**

 Import LogisticRegression
from pyspark.ml.classification import LogisticRegression

 Create a LogisticRegression Estimator
lr = LogisticRegression()

 Import the evaluation submodule
import pyspark.ml.evaluation as evals

 Create a BinaryClassificationEvaluator
evaluator = evals.BinaryClassificationEvaluator(metricName="areaUnderROC")

 Import the tuning submodule
import pyspark.ml.tuning as tune

 Create the parameter grid
grid = tune.ParamGridBuilder()

 Add the hyperparameter
grid = grid.addGrid(lr.regParam, np.arange(0, .1, .01))
grid = grid.addGrid(lr.elasticNetParam, [0, 1])

 Build the grid
grid = grid.build()

 Create the CrossValidator
cv = tune.CrossValidator(estimator=lr,
               estimatorParamMaps=grid,
               evaluator=evaluator
               )

 Fit cross validation models
models = cv.fit(training)

 Extract the best model
best_lr = models.bestModel

 Use the model to predict the test set
test_results = best_lr.transform(test)

 Evaluate the predictions
print(evaluator.evaluate(test_results))

**Find out how Spark finds out which columns are the features.**

label (this is the default name for the response variable in Spark's machine learning routines

## Sparklyr

library(sparklyr)

Connect to your Spark cluster
spark_conn <- spark_connect(master = "local")

Print the version of Spark
spark_version(sc = spark_conn)

Disconnect from Spark
spark_disconnect(sc = spark_conn)

src_tbls(sc)

Copy track_metadata to Spark
track_metadata_tbl <- copy_to(spark_conn, track_metadata)

Print 5 rows, all columns
print(track_metadata_tbl, n = 5, width = Inf)

To collect your data: that is, to move it from Spark to R, you call collect()

copy_to() moves your data from R to Spark

The solution is to use compute() to compute the calculation, but store the results in a temporary data frame on Spark.

If you want to delay returning the data, you can use dbSendQuery() to execute the query, then dbFetch() to return the results.

Write SQL query
query <- "SELECT * FROM track_metadata WHERE year < 1935 AND duration > 300"

Run the query
(results <- dbGetQuery(spark_conn, query))

A semi join returns the rows of the first table where it can find a match in the second table. The principle is shown in this diagram.

feature transforms: ft_
ml functions: ml_
spark df functions: sdf_

general structure

a_tibble %>%
  ft_some_transformation("x", "y", some_other_args)

hotttnesss <- track_metadata_tbl %>%
  # Select artist_hotttnesss
  select(artist_hotttnesss) %>%
  # Binarize to is_hottt_or_nottt
  ft_binarizer('artist_hotttnesss', 'is_hottt_or_nottt', threshold = .5) %>%
  # Collect the result
  collect() %>%
  # Convert is_hottt_or_nottt to logical
  mutate(is_hottt_or_nottt = as.logical(is_hottt_or_nottt))

ft_tokenizer(): to lower and split into individual words
ft_regex_tokenizer
sdf_sort

track_metadata_tbl has been pre-defined

track_metadata_tbl

Get the schema

(schema <- sdf_schema(track_metadata_tbl))

Transform the schema

schema %>%
  lapply(function(x) do.call(data_frame, x)) %>%
  bind_rows()

train-test split
partitioned <- track_metadata_tbl %>%
  sdf_partition(training = 0.7, testing = 0.3)

### ml

ls("package:sparklyr", pattern = "^ml")

Parquet files: quicker to read and write.  parquet files can be used with other tools in the Hadoop ecosystem, like Shark, Impala, Hive, and Pig. spark_read_parquet

parquet_dir has been pre-defined

parquet_dir

List the files in the parquet dir

filenames <- dir(parquet_dir, full.names = TRUE)

Show the filenames and their sizes

data_frame(
  filename = basename(filenames),
  size_bytes = file.size(filenames)
)

Import the data into Spark

timbre_tbl <- spark_read_parquet(spark_conn, 'timbre', parquet_dir)

gradient_boosted_trees_model <- track_data_to_model_tbl %>%
  # Run the gradient boosted trees model
   ml_gradient_boosted_trees('year', feature_colnames)

responses <- track_data_to_predict_tbl %>%
  # Select the year column
  select(year) %>%
  # Collect the results
  collect() %>%
  # Add in the predictions
  mutate(
    predicted_year = predict(
      gradient_boosted_trees_model,
      track_data_to_predict_tbl
    )
  )


