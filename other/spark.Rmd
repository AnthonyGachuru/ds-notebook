---
title: "Bayesian"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## PySpark

https://www.qubole.com/resources/pyspark-cheatsheet/

Quick Test:

```{python}
import findspark
findspark.init()
import pyspark
import random

sc = pyspark.SparkContext(appName="Pi")
num_samples = 100000000
def inside(p):     
    x, y = random.random(), random.random()
    return x*x + y*y < 1
count = sc.parallelize(range(0, num_samples)).filter(inside).count()
pi = 4 * count / num_samples
print(pi)

sc.stop()
```

On Windows: setx SPARK_HOME <path>

To start working with Spark DataFrames, you first have to create a SparkSession object from your SparkContext. You can think of the SparkContext as your connection to the cluster and the SparkSession as your interface with that connection.

import pyspark
from pyspark.sql import SparkSession
sc = pyspark.SparkContext()
spark = SparkSession.builder.getOrCreate()

Your SparkSession has an attribute called catalog which lists all the data inside the cluster. This attribute has a few methods for extracting different pieces of information.

spark.catalog.listTables()

query = "FROM flights SELECT * LIMIT 10"
flights10 = spark.sql(query)
flights10.show()

df['g'] = df['g'].astype(str)

spark = SparkSession.builder.appName('chosenName').getOrCreate()

df=spark.read.csv('fileNameWithPath', mode="DROPMALFORMED",inferSchema=True, header = True)

df = spark.read.format("csv").option("header","true").option("inferSchema","true").load("FileStore/tables/pzufk5ib1500654887654/campaign.csv")

airports = spark.read.csv(file_path, header = True)

toPandas() / table

Create spark_temp from pd_temp
spark_temp = spark.createDataFrame(pd_temp)

Add spark_temp to the catalog
spark_temp.createOrReplaceTempView("temp")

Examine the tables in the catalog again
print(spark.catalog.listTables())

df = df.withColumn("newCol", df.oldCol + 1)

Create the DataFrame flights
flights = spark.table('flights')

col names
spark_df.schema.names
spark_df.printSchema()

Let's take a look at the .filter() method. As you might suspect, this is the Spark counterpart of SQL's WHERE clause. The .filter() method takes either a Spark Column of boolean (True/False) values or the WHERE clause of a SQL expression as a string.

long_flights1 = flights.filter('distance > 1000')

Filter flights with a boolean column
long_flights2 = flights.filter(flights.distance > 1000)

You can also use the .alias() method to rename a column you're selecting.

The equivalent Spark DataFrame method .selectExpr() takes SQL expressions as a string

Find the shortest flight from PDX in terms of distance
flights.filter(flights.origin == "PDX").groupBy().min("distance").show()

Average duration of Delta flights
flights.filter(flights.carrier=='DL').filter(flights.origin=='SEA').groupBy().avg('air_time').show()

Total hours in the air
flights.withColumn("duration_hrs", flights.air_time/60).groupBy().sum('duration_hrs').show()

.show() vs collect()

import pyspark.sql.functions as F

Standard deviation
by_month_dest.agg(F.stddev('dep_delay')).show()


