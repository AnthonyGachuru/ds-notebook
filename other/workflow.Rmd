---
title: "Workflow"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Preamble

* Workflow: import -> tidy -> Explore [ transform <-> visualize <-> model ] -> communicate

* Workflow Languages: (R) import -> tidy -> (Python) transform <-> visualize <-> model -> (rmarkdown) communicate

* Use branches to delineate iterations

* Generally need 3 rows of data per variable (to prevent model from seeing signal where there is only noise) [Nina Zumel]

* Domain knowledge for feature selection and random forest feature importance (5-10) best variables. [Nina Zumel]

A typical data science workflow can be framed as following these 7 steps:

Business: start with a business question, define the goal and measure of success

Data: find, access and explore the data

Features: extract, assess and evaluate, select and sort

Models: find the right model for the problem at hand, compare, optimize, and fine tune

Communication: Interpret and communicate the results

Production: transform the code into production ready code, integrate into current ecosystem and deploy

Maintain: adapt the models and features to the evolution of the environment

**VanderplasJupyter**

* create helper functions and units tests as R/py for Rmd/ipynb

* pytest/hypothesis for testing

* use makefile to run cmd commands

* Make package for workflow with data (__init__.py) and use this formation for function docs:

def fun(a): 
```{python}
def fun(a):

  Role
  -----
  Does something
  
  Parameters
  ---------
  Accepts something
  
  Returns
  -------
  Returns something
  
  return(a)
```

* [Conda Setup](http://tdhopper.com/blog/2015/Nov/24/my-python-environment-workflow-with-conda/)

**YAML**

A really useful feature is sharing environments so others can install all the packages used in your code, with the correct versions. You can save the packages to a YAML file with:

`conda env export > environment.yaml`

The first part conda env export writes out all the packages in the environment, including the Python version. Above you can see the name of the environment and all the dependencies (along with versions) are listed. The second part of the export command, > environment.yaml writes the exported text to a YAML file environment.yaml. This file can now be shared and others will be able to create the same environment you used for the project. To create an environment from an environment file use:

`conda env create -f environment.yaml`

This will create a new environment with the same name listed in environment.yaml.

# General Workflow

## Data Ingestion/Wrangling

* [Sqlite Converter](https://github.com/thombashi/sqlitebiter)

* Easy Database Management: [dataset](https://dataset.readthedocs.io/en/latest/)

* [Data Validation](https://github.com/data-cleaning/validate)

* Encoding Precendence: utf-8, iso-8859-1, utf-16

* na.strings: Replace empty strings by na: `x <- read.csv("filepath/R_NA.csv",header=TRUE,na.strings=c(""))`

* Missingness: Missing completely at random/missing at random/missing not at random

* Dealing with missing data: df.fillna(value), df.isnull().sum(), df = df[np.isfinite(df['EPS'])]

* scipy.stats.describe

* group_by aggregate functions: size, count, sum, mean, median, sd, var, min, max, prod, first, last

* Replacing blank cells by nas: titanic3[titanic3==""] = NA

**Explore Data**

* Histograms

* [EdaR](https://github.com/ujjwalkarn/xda)

* [Binned Stats](https://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.stats.binned_statistic.html)

* Quick Exploration Guide: S.O.C.S (Shape, Outlier, Center, Spread). 

* Outlier Detection - Tukey IQR, Kernel density estimation, Bonferroni test

* PCA & non-linear dimensionality reduction, Isomap - manifold or cluster

**Imputation**

* Pros include helping to retain a larger data set, potentially avoiding bias, and the resulting standard errors tend to be too small.

* Mean: Simple but can distort distribution, underestimate standard deviation, and distort variable relationships by dragging correlation to zero.

* Random: Can amplify outlier observation and induce bias.

* Regression: Use observed variables and relationships between these variables. Must make assumptions and can badly extrapolate.

## Visualization

* Make trendlines, scatterplots, and add confidence intervals

## Modeling

### Preprocessing

* sklearn: preprocessing.binarize

* pandas.factorize: Encode input values as an enumerated type

### Feature Selection

* Variance Threshold: Use this to exclude features that don't meet a variance threshold.
 
* Univariate Feature Selection: For classification can use chi-squared and F-Test while F-Test for regression. Functions: chi2, f_regression, f_classification from sklearn.feature_selection.

* SelectKBest/SelectPercentile: Keep the k highest scoring features and keep a user-specified highest scoring percentage of features. Can use the univariate feature selection in these functions. 

* sklearn: feature_selection.RFE/CV

### Model Selection

* Gradient boosting for class imbalance

* cross_val_score & cross_val_predict

* Building custom ensembles: mlxtend

* [Sklearn Evaluation](https://edublancas.github.io/sklearn-evaluation/)

* Skplot::[1](https://github.com/DistrictDataLabs/yellowbrick):[2](https://github.com/reiinakano/scikit-plot)

# R Stack

import

tidy

Explore

[ transform ] 

[visualize]

[model ]

communicate

# Py Stack

import

tidy

Explore

[ transform ] 

[visualize]

[model ]

communicate

