---
title: "Workflow"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Preamble

* Workflow: import -> tidy -> Explore [ transform <-> visualize <-> model ] -> communicate

* Workflow Languages: (R) import -> tidy -> (Python) transform <-> visualize <-> model -> (rmarkdown) communicate

* Use branches to delineate iterations

* Generally need 3 rows of data per variable (to prevent model from seeing signal where there is only noise) [Nina Zumel]

* Domain knowledge for feature selection and random forest feature importance (5-10) best variables. [Nina Zumel]

A typical data science workflow can be framed as following these 7 steps:

Business: start with a business question, define the goal and measure of success

Data: find, access and explore the data

Features: extract, assess and evaluate, select and sort

Models: find the right model for the problem at hand, compare, optimize, and fine tune

Communication: Interpret and communicate the results

Production: transform the code into production ready code, integrate into current ecosystem and deploy

Maintain: adapt the models and features to the evolution of the environment

# My Stack

Import

https://github.com/ContinuumIO/TextAdapter

library("readxl")

Tidy 

widyr
tidyr
dplyr

U:Transform

https://blog.datascienceheroes.com/x-ray-vision-on-your-datasets/
https://github.com/ropenscilabs/skimr
[xda: Exploratory Analysis](https://github.com/ujjwalkarn/xda)
[Imputation in R](https://cran.r-project.org/web/packages/simputation/index.html)
https://cran.r-project.org/web/packages/janitor/vignettes/introduction.html
https://github.com/ResidentMario/missingno
https://github.com/shawnbrown/datatest
TDD::[1](http://engineering.pivotal.io/post/test-driven-development-for-data-science/)|[2](http://www.tdda.info/)|[3](http://stochasticsolutions.com/)|[**Data Testing**](https://github.com/ericmjl/data-testing-tutorial)
[Argument Checks](https://rdrr.io/cran/checkmate/)[PyValidation](https://github.com/shawnbrown/datatest)
missingness::[1](https://github.com/MimiOnuoha/missing-datasets):[2](https://github.com/njtierney/naniar)|[Py](https://github.com/ResidentMario/missingno)
https://jennybc.github.io/purrr-tutorial/index.html
https://github.com/drsimonj/corrr
https://cran.r-project.org/web/packages/simputation/index.html
[Categorical Dissimilarity](https://www.rdocumentation.org/packages/StatMatch/versions/1.2.5/topics/gower.dist)
[Regex For Humans](https://github.com/iogf/crocs)
https://github.com/yhat/pandasql
https://github.com/njtierney/naniar

U:Visualize

ggformula
ggplot2
plotnine

U:Model

Work with data subset

http://www.ats.ucla.edu/stat/mult_pkg/whatstat/
https://github.com/WinVector/vtreat
https://systemml.apache.org

Pipelines & Feature Unions::[1](https://github.com/zipfian/pipelines_and_featureunions)|[2](http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html)

https://github.com/scikit-learn-contrib/boruta_py
[Algorithms](https://github.com/EpistasisLab/scikit-rebate)

http://contrib.scikit-learn.org/imbalanced-learn/stable/

https://github.com/DistrictDataLabs/yellowbrick
https://github.com/marcotcr/lime
https://github.com/TeamHG-Memex/eli5

https://github.com/hyperopt/hyperopt-sklearn
https://github.com/rsteca/sklearn-deap

https://t.co/Zcnd7EJKsn
https://rasbt.github.io/mlxtend/

https://github.com/databricks/spark-sklearn
pysparkling/sparkling_pandas/sparklyr

https://github.com/ShopRunner/jupyter-notify

Communicate

Rmarkdown/Jupyter/Shiny/Dash

**Vanderplas Jupyter**

* create helper functions and units tests as R/py for Rmd/ipynb

* pytest/hypothesis for testing

* use makefile to run cmd commands

* Make package for workflow with data (__init__.py) and use this formation for function docs:

def fun(a): 
```{python}
def fun(a):

  Role
  -----
  Does something
  
  Parameters
  ---------
  Accepts something
  
  Returns
  -------
  Returns something
  
  return(a)
```

* [Conda Setup](http://tdhopper.com/blog/2015/Nov/24/my-python-environment-workflow-with-conda/)

**YAML**

A really useful feature is sharing environments so others can install all the packages used in your code, with the correct versions. You can save the packages to a YAML file with:

`conda env export > environment.yaml`

The first part conda env export writes out all the packages in the environment, including the Python version. Above you can see the name of the environment and all the dependencies (along with versions) are listed. The second part of the export command, > environment.yaml writes the exported text to a YAML file environment.yaml. This file can now be shared and others will be able to create the same environment you used for the project. To create an environment from an environment file use:

`conda env create -f environment.yaml`

This will create a new environment with the same name listed in environment.yaml.

# General Workflow

## Data Ingestion/Wrangling

* [Sqlite Converter](https://github.com/thombashi/sqlitebiter)

* Easy Database Management: [dataset](https://dataset.readthedocs.io/en/latest/)

* [Data Validation](https://github.com/data-cleaning/validate)

* Encoding Precendence: utf-8, iso-8859-1, utf-16

* na.strings: Replace empty strings by na: `x <- read.csv("filepath/R_NA.csv",header=TRUE,na.strings=c(""))`

* Missingness: Missing completely at random/missing at random/missing not at random

* Dealing with missing data: df.fillna(value), df.isnull().sum(), df = df[np.isfinite(df['EPS'])]

* scipy.stats.describe

* group_by aggregate functions: size, count, sum, mean, median, sd, var, min, max, prod, first, last

* Replacing blank cells by nas: titanic3[titanic3==""] = NA

**Explore Data**

* Histograms

* [EdaR](https://github.com/ujjwalkarn/xda)

* [Binned Stats](https://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.stats.binned_statistic.html)

* Quick Exploration Guide: S.O.C.S (Shape, Outlier, Center, Spread). 

* Outlier Detection - Tukey IQR, Kernel density estimation, Bonferroni test

* PCA & non-linear dimensionality reduction, Isomap - manifold or cluster

**Imputation**

* Pros include helping to retain a larger data set, potentially avoiding bias, and the resulting standard errors tend to be too small.

* Mean: Simple but can distort distribution, underestimate standard deviation, and distort variable relationships by dragging correlation to zero.

* Random: Can amplify outlier observation and induce bias.

* Regression: Use observed variables and relationships between these variables. Must make assumptions and can badly extrapolate.

## Visualization

* Make trendlines, scatterplots, and add confidence intervals

## Modeling

### Preprocessing

* sklearn: preprocessing.binarize

* pandas.factorize: Encode input values as an enumerated type

### Feature Selection

* Variance Threshold: Use this to exclude features that don't meet a variance threshold.
 
* Univariate Feature Selection: For classification can use chi-squared and F-Test while F-Test for regression. Functions: chi2, f_regression, f_classification from sklearn.feature_selection.

* SelectKBest/SelectPercentile: Keep the k highest scoring features and keep a user-specified highest scoring percentage of features. Can use the univariate feature selection in these functions. 

* sklearn: feature_selection.RFE/CV

### Model Selection

* Gradient boosting for class imbalance

* cross_val_score & cross_val_predict

* Building custom ensembles: mlxtend

* [Sklearn Evaluation](https://edublancas.github.io/sklearn-evaluation/)

* Skplot::[1](https://github.com/DistrictDataLabs/yellowbrick):[2](https://github.com/reiinakano/scikit-plot)

# R Stack

import

tidy

Explore

[ transform ] 

[visualize]

[model ]

communicate

# Py Stack

import

tidy

Explore

[ transform ] 

[visualize]

[model ]

communicate

## Import 

Step 0: Figure out what I’m trying to do with the data

Step 1: See what it looks out

So the first thing I do is fool around a bit to try to figure out what the data set “looks” like by doing things like what does looking at the types of variables I have, what the first few observations and last few observations look like.

If the data set is really big, I usually take a carefully chosen random subsample to make it possible to do my exploration interactively

missing values or outliers

summary() 

## Tidy

## Transform

## Visualize 

To compare the distributions of variables I usually use overlayed density plots 

I make tons of scatterplots to look at relationships between variables

Step 3: Plot. That. Stuff. After getting a handle with mostly text based tables and output (things that don’t require a graphics device)

Usually color/size the dots in the scatterplots by other variables to see if I can identify any confounding relationships that might screw up analyses downstream. Then, if the data are multivariate, I do some dimension reduction to get a feel for high dimensional structure. Nobody mentioned principal components or hierarchical clustering in the Twitter conversation, but I end up using these a lot to just figure out if there are any weird multivariate dependencies I might have missed.

## Model 

tuning | dummy | standardize | feature selection | dimensionality reduction

## Communicate

N/A

## [Kaggle Checklist](https://www.kaggle.com/general/19959)

I use every new contest to update any general python environments (all Anaconda) I have on the 5 computers I may actually do coding on. (Even though this is not strictly necessary since I set up project environments, it's a trigger to keep things up to date.)

Add the competition end date to my electronic and wall calendars.

Update the kaggle evaluation metric wiki to reference back to the contest page.

Identify any related contests

Subscribe to the Forum

Create a github repo, a standard folder structure, and a wiki so that I have a single place for model notes and to capture forum posts of interest. (I have a sub checklist for the steps, e.g., what I add to the .gitignore, etc.)

Download the data

Create virtual environments (conda) for my 3 linux machines with a standard set of libraries. Again, this is a separate sub checklist, and includes updating xgboost, theano, etc. Once I start a project, I do not update libraries unless absolutely necessary.

Explore the data, which includes basic outlier analysis.

Force myself to write error analysis code before I start building a model. (This is hard to stay true to, but worth the effort.)

Start feature transformation, and finally model building


