---
title: "Workflow"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Preamble

A typical data science workflow can be framed as following these 7 steps:

* Business: start with a business question, define the goal and measure of success

* Data: find, access and explore the data

* Features: extract, assess and evaluate, select and sort

* Models: find the right model for the problem at hand, compare, optimize, and fine tune

* Communication: Interpret and communicate the results

* Production: transform the code into production ready code, integrate into current ecosystem and deploy

* Maintain: adapt the models and features to the evolution of the environment

More succinctly, as per Hadley Wickham:

import -> tidy -> understand [ transform <-> visualize <-> model ] -> communicate

# Checklist

## Import 

* Figure out what Iâ€™m trying to do with the data, and ssee what it looks like. Types of variables, the first few observations and last few observations look like.

* If the data set is really big, I usually take a carefully chosen random subsample to make it possible to do my exploration interactively

* missing values or outliers

* summary() 

## Tidy

* Tidy data

## Transform

* Data set dependent.

* Imputation

    * Pros include helping to retain a larger data set, potentially avoiding bias, and the resulting standard errors tend to be too small.

    * Mean: Simple but can distort distribution, underestimate standard deviation, and distort variable relationships by dragging correlation to zero.

    * Random: Can amplify outlier observation and induce bias.

    * Regression: Use observed variables and relationships between these variables. Must make assumptions and can badly extrapolate.

## Visualize 

* Make trendlines, scatterplots, and add confidence intervals

* To compare the distributions of variables I usually use overlayed density plots 

* I make tons of scatterplots to look at relationships between variables

* Usually color/size the dots in the scatterplots by other variables to see if I can identify any confounding relationships that might screw up analyses downstream. Then, if the data are multivariate, I do some dimension reduction to get a feel for high dimensional structure. 

* Nobody mentioned principal components or hierarchical clustering in the Twitter conversation, but I end up using these a lot to just figure out if there are any weird multivariate dependencies I might have missed.

* Histograms

* Quick Exploration Guide: S.O.C.S (Shape, Outlier, Center, Spread). 

* Outlier Detection - Tukey IQR, Kernel density estimation, Bonferroni test

* PCA & non-linear dimensionality reduction, Isomap - manifold or cluster

## Model 

* standardize

* dimensionality reduction

* feature selection

    * Variance Threshold: Use this to exclude features that don't meet a variance threshold.
 
    * Univariate Feature Selection: For classification can use chi-squared and F-Test while F-Test for regression. Functions: chi2, f_regression, f_classification from sklearn.feature_selection.

    * SelectKBest/SelectPercentile: Keep the k highest scoring features and keep a user-specified highest scoring percentage of features. Can use the univariate feature selection in these functions. 

* sklearn: feature_selection.RFE/CV

* Dummying

    * sklearn: preprocessing.binarize

    * pandas.factorize: Encode input values as an enumerated type

* tuning

## Communicate

* N/A

# My Stack

General maintenance:

* TDD::[1](http://engineering.pivotal.io/post/test-driven-development-for-data-science/)|[2](http://www.tdda.info/)|[3](http://stochasticsolutions.com/)|[**Data Testing**](https://github.com/ericmjl/data-testing-tutorial)

* Helper functions, unit tests, packages, makefiles.

* pytest/hypothesis/testthat

## Import

pandas | read_csv | readxl

* [Sqlite Converter](https://github.com/thombashi/sqlitebiter)

* Easy Database Management: [dataset](https://dataset.readthedocs.io/en/latest/)

* [Data Validation](https://github.com/data-cleaning/validate)

## TIdy

widyr | tidyr | dplyr

## uTransform

https://blog.datascienceheroes.com/x-ray-vision-on-your-datasets/

https://github.com/ropenscilabs/skimr

[xda: Exploratory Analysis](https://github.com/ujjwalkarn/xda)

[Imputation in R](https://cran.r-project.org/web/packages/simputation/index.html)

https://cran.r-project.org/web/packages/janitor/vignettes/introduction.html

https://github.com/ResidentMario/missingno

https://github.com/shawnbrown/datatest

[Argument Checks](https://rdrr.io/cran/checkmate/)[PyValidation](https://github.com/shawnbrown/datatest)

missingness::[1](https://github.com/MimiOnuoha/missing-datasets):[2](https://github.com/njtierney/naniar)|[Py](https://github.com/ResidentMario/missingno)

https://jennybc.github.io/purrr-tutorial/index.html

https://github.com/drsimonj/corrr

https://cran.r-project.org/web/packages/simputation/index.html

[Categorical Dissimilarity](https://www.rdocumentation.org/packages/StatMatch/versions/1.2.5/topics/gower.dist)

[Regex For Humans](https://github.com/iogf/crocs)

https://github.com/yhat/pandasql

https://github.com/njtierney/naniar

* Encoding Precendence: utf-8, iso-8859-1, utf-16

* na.strings: Replace empty strings by na: `x <- read.csv("filepath/R_NA.csv",header=TRUE,na.strings=c(""))`

* Missingness: Missing completely at random/missing at random/missing not at random

* Dealing with missing data: df.fillna(value), df.isnull().sum(), df = df[np.isfinite(df['EPS'])]

* scipy.stats.describe

* group_by aggregate functions: size, count, sum, mean, median, sd, var, min, max, prod, first, last

* Replacing blank cells by nas: titanic3[titanic3==""] = NA

## uVisualize

ggformula | ggplot2

plotnine

* [EdaR](https://github.com/ujjwalkarn/xda)

* [Binned Stats](https://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.stats.binned_statistic.html)

## uModel

http://www.ats.ucla.edu/stat/mult_pkg/whatstat/

https://github.com/WinVector/vtreat

https://systemml.apache.org

Pipelines & Feature Unions::[1](https://github.com/zipfian/pipelines_and_featureunions)|[2](http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html)

https://github.com/scikit-learn-contrib/boruta_py

[Algorithms](https://github.com/EpistasisLab/scikit-rebate)

http://contrib.scikit-learn.org/imbalanced-learn/stable/

https://github.com/DistrictDataLabs/yellowbrick

https://github.com/marcotcr/lime

https://github.com/TeamHG-Memex/eli5

https://github.com/hyperopt/hyperopt-sklearn

https://github.com/rsteca/sklearn-deap

https://github.com/databricks/spark-sklearn

pysparkling/sparkling_pandas/sparklyr

* Gradient boosting for class imbalance

* Building custom ensembles: mlxtend

* [Sklearn Evaluation](https://edublancas.github.io/sklearn-evaluation/)

* Skplot::[1](https://github.com/DistrictDataLabs/yellowbrick):[2](https://github.com/reiinakano/scikit-plot)

## Communicate

Rmarkdown | Jupyter | Shiny | Dash

# R Stack

## Import

## Tidy

## uTransform

## uVisualize

ggplot2

## uModel

caret

sparklyr

## Communicate

Rmarkdown 

Shiny

# Py Stack

import

tidy

Explore

[ transform ] 

[visualize]

[model ]

communicate