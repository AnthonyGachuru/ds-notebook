# Udacity: AB Testing 

## Overview

Guide: Choose metric (CTR), review stats (Hypothesis Testing), design, analyse

Need to have clear control and thing to test. Also need time. Frequency of customer interaqction is important.

shopping and searching not independent events.

Point estimate of CTR (users who clicked)/users. Need margin of error for estimate. Approximate binomial by normal if N*p & N(1-p) > 5. 

marin of error = Z*(stndad error)

se = sqrt(p(1-p)/n)

Z for 95% s 1.96 and 2.58 for 99%.

for Two samples calculate a pooled CTR point estimate and standard error. 

Need to be substantive (a worthy difference) in addition to being statistically significant. Some changes might not be useful due to the resources one might want to allocate.

Result are repeatable (stat sig) bar should be lower than business interesting (practically sig)

Stat Power: How many page view necessary to get a stat sig result. 

Size v Power: The smaller the effect or increased confidence you want to detect/have (greater power of experiment), the larger the sample you need to use.

alpha = P(reject null when true) 
beta = P(accept null when false)

small sample: low alpha, high beta
larger sample: low alpha, low beta

1- alpha = confidence level
1 - beta = sensitivity (often 80%)

practical significance: 2%, alpha = 0.05, beta = 0.2

sample size dp_to ctr (increase in se), confidence level, sensitivity
sample size invp_to practical sig (larger changes easier to detect)

above is design of experiment. below is analysis:

estimate diff:  experimental prop - control prop
margin of error = z*se

conf int: estimated diff +/- margin of error

#### Policy & Ethics

In the study, what risk is the participant undertaking? The main threshold is whether the risk exceeds that of “minimal risk”.

Next, what benefits might result from the study? Even if the risk is minimal, how might the results help? 

Third, what other choices do participants have? For example, if you are testing out changes to a search engine, participants always have the choice to use another search engine.

Finally, what data is being collected, and what is the expectation of privacy and confidentiality? This last question is quite nuanced, encompassing numerous questions: Do participants understand what data is being collected about them? What harm would befall them should that data be made public? Would they expect that data to be considered private and confidential?

Our recommendation is that there should be internal reviews of all proposed studies by experts regarding the questions:

Are participants facing more than minimal risk?
Do participants understand what data is being gathered?
Is that data identifiable?
How is the data handled?

Internal process recommendations
Finally, regarding internal process of data handling, we recommend that:

Every employee who might be involved in A/B test be educated about the ethics and the protection of the participants. Clearly there are other areas of ethics beyond what we’ve covered that discuss integrity, competence, and responsibility, but those generally are broader than protecting participants of A/B tests (cite ACM code of ethics).

All data, identified or not, be stored securely, with access limited to those who need it to complete their job. Access should be time limited. There should be clear policies of what data usages are acceptable and not acceptable. Moreover, all usage of the data should be logged and audited regularly for violations. You create a clear escalation path for how to handle cases where there is even possibly more than minimal risk or data sensitivity issues.

rate better for usability test than prob

#### Choosing & Characterizing Metrics

**Define**

Make sure to granularize funnel

**Intuition**

NA

**Characterize**

Focus groups, UER (user experience study), survey, experiments, retrospective analysis, external data

temporal effects across hours, days, weeks are important

different browsers screw with the CTR depending on how they deal with JS

Can use cookies to track users

Metric definitions

Def #1 (Cookie probability): For each <time interval>, number of cookies that click divided by number of cookies

Def #2 (Pageview probability): Number of pageviews with a click within <time interval> divided by number of pageviews

Def #3 (Rate): Number of clicks divided by number of pageviews

CTR vs CTP: The first can be >1. 

Take into consideration mean and median. Median is robust but might not be useful so look at percentiles. 

Measuring sensitiviyt and robustness: 1) Running experiments to see if metrics move as predicted. A vs A experiments: Measure people who saw the same thing to see if there is a difference between them according to the metric or its too sensitive. Look at previous experiments. 2) Retrospective analysis of logs to see how the metrics responded in the past.

Metric For Loading Vid: Look at distribution of load times for vid. Want a robust metric that doesn't really change for comparable vids. But wnat a metric that is sensitive to a change that you care about like resolution.

How to compare experiment vs control: difference or relative change. Relative change allows you to keep same significance boundary. 

Check that praticial sig level is good for metric. Need to have handle on variability for confidence interval.

sign test: Good for looking to implement a change, but doesn't give effect size.

empirical vs analytic: underlying distribution could be weird so do empirical. Analyticcal for simple metrics. If empirical and anlytical don't agree then run AvA test.

A vs A test: Std proportional to square root of number of samples. Diminishing returns for many tests. Can use bootstrap if don't have enough traffic for a big A v A test.

## Designing An Experiment

**Choose Subject**

Unit of Diversion: How to indentify a person. Cookie (could clear so change), userid, deviceid, and address (changes all the time).

Intra-user vs inter-user experiment.

**Choose Population**

Size

Duration

Cohort: People who enter the experiment at the same time.

Cohort > Population: Learning effects, user retention

Can reduce sample if just targeting english speaking traffic

When going to run experiment: holiday or not? How many people to put through experiment and control?

**Analyzing Results**

Sanity Checks:

Pass all before move on

Good invariants: # of events, video load time (user has no control over it). Want these to  be about the same for experiment and control.

Single Metric:

sign test

Multiple Metrics:

More metrics increase false positive chance. Use Bonferroni assumption: makes no independence assumption and is conservative. 

margin of error smaller than observed diff so the confidence interval won't include 0. 

Different strategies: family-wise error rate, false discovery rate, 

Gotchas: 

Simpson's Paradox

Usually A/B testing works for testing changes in elements in the web page. A/B testing framework is following sequence:

Design a research question.
Choose test statistics method or metrics to evaluate experiment.
Designing the control group and experiment group.
Analyzing results, and draw valid conclusions.
