---
title: "Workflow"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Stats

* **[Power Analysis Calculator](http://www.evanmiller.org/ab-testing/sample-size.html)**

* Type I Error: Saying there is an effect when there isn't one. Type II error: concluding there is no effect when, in fact, there is one.

* Probably the most useful types of statistics for skewed probability distributions like this one, though, are quantiles. 

* get 95% ci around p-value

* Probability is calibrated if it is empirically correct, i.e. the empirical probability matches the theoretical one.

* [Effect size](https://artax.karlin.mff.cuni.cz/r-help/library/lsr/html/cohensD.html)

* jarque.bera.test: Tests the null of normaility for the series using the Jarque-Bera test statistics	

* box.test: Compute the Box-Pierce or Ljung-Box test statistics for examining the null of independence in a given series			

* shapiro.test: Test for normality		

* Sum of normally distributed random variables: sum of means & sum of variances

* Zero correlation doesn't mean independent variables. Covariance only determines linear relationships.

* test heteroscedaticity: breusch-pagan or ncv test

* Test Normality: Shapiro–Wilk test is a test of normality

* Bypass inferential stats if features at least 1/10 of data points. 

* The probability (p-value) of observing results at least as extreme as what is present in your data sample. P-value less than .05 retain h_0 else reject h_0.

* One Sample T-test: To examine the average difference between a sample and the known value 
of the population mean. Assumes the population from which the sample is drawn is normally distributed and the sample observations are randomly drawn and independent. 

* Two Sample T-test: To examine the average difference between two samples drawn from two 
different populations. Assumes the populations from which the samples are drawn are normally dist, the standard deviations of the two populations are equal, and sample observations are randomly drawn and independent.

* F-Test: To assess whether the variances of two different populations are equal. Assumes the population from which the sample is drawn is normally distributed and the sample observations are randomly drawn and independent.

* Barlett Test: F-Test for more than two populations. 

* One-Way ANOVA: To assess the equality of means of two or more groups. Assumes the populations from which the samples are drawn are normally dist, the standard deviations of the populations are equal, and sample observations are randomly drawn and independent. 

* Chi-Squared Test of Independence: To test whether two categorical variables are independent.  Assumes the sample observations are randomly drawn and independent.


* If we take a larger and larger sample from a population, its distribution will tend to become normal no matter what it is initially. It won't. The Central Limit Theorem, the misreading of which is the cause of this mistake, refers to the distribution of standardized sums of random variables as their number grow, not to the distribution of a collection of random variables.

* Frequentists: Probability is a measure of the the frequency of repeated events. The parameters are fixed (but unknown) and data are random.

* Bayesians: Probability is a measure of the degree of certainty about values, so the interpretation is that rameters are random and data are fixed.

* Independent rvs are uncorrelated. That is Cor (X,Y)=0

* Don’t sum sd’s, sum variances.

* One Sample T-Test? To examine the average difference between a sample and the known value of the population mean. Assumptions: The population from which the sample is drawn is normally distributed. Sample observations are randomly drawn and independent.

* When do we use the Two Sample T-Test? To examine the average difference between two samples drawn from two different populations. Assumptions: The populations from which the samples are drawn are normally dist. The standard deviations of the two populations are equal. Sample observations are randomly drawn and independent.

* When do we use the F-Test? To assess whether the variances of two different populations are equal. Assumptions: The populations from which the samples are drawn are normally dist. Sample observations are randomly drawn and independent.

* When do we use One-Way ANOVA?  To assess the equality of means of two or more groups. NB: When there are exactly two groups, this is equivalent to a Two Sample T-Test. Assumptions: The populations from which the samples are drawn are normally dist.  The standard deviations of the populations are equal. Sample observations are randomly drawn and independent.

* When do we use the chisq2 Test of Independence? To test whether two categorical variables are independent. Assumptions: Sample observations are randomly drawn and independent.

* To perform the one-way ANOVA, we can use the f_oneway() function of the SciPy package.

* The degrees of freedom is the number of data rows minus the number of coefficients fit.

* Hypothesis tests aim to describe the plausibility of a parameter taking on a specific value. Confidence intervals aim to describe a range of plausible values a parameter can take on. If the value of the parameter specified by H0 is contained within the 95% confidence interval, then H0 cannot be rejectedat the 0.05 p-value threshold. If the value of the parameter specified by H0 is not contained within the 95% confidence interval, then H0 can be rejectedat the 0.05 p-value threshold.

### Level and power of statistical test

The appropriate sample size depends on many things, chiefly the complexity of the analysis and the expected effect size. The "30" rule comes from one of the simplest cases: Whether to use a z-test or t-test for comparing two means.  
1) for large n (sample size), many distributions can be approximated to normal. [ courtesy : Central Limit Theorems ]
2) The approximation becomes better with larger n, given other things remaining the same.

However if you are wondering why 30 ? why not 25 or 40 or something else ? Note that actual answer will depend:

1) how much error are you going to tolerate
2) size of the effect
3) exact distribution family (like t vs chi-square vs gamma)


The power of any test of statistical significance is defined as the probability that it will reject a false null hypothesis. Statistical power is inversely related to beta or the probability of making a Type II error. In short, power = 1 – β. In plain English, statistical power is the likelihood that a study will detect an effect when there is an effect there to be detected. If statistical power is high, the probability of making a Type II error, or concluding there is no effect when, in fact, there is one, goes down. Statistical power is affected chiefly by the size of the effect and the size of the sample used to detect it. Bigger effects are easier to detect than smaller effects, while large samples offer greater test sensitivity than small samples.

The following four quantities have an intimate relationship:

sample size
effect size
significance level = P(Type I error) = probability of finding an effect that is not there
power = 1 - P(Type II error) = probability of finding an effect that is there

Given any three, we can determine the fourth.

The power of any test of statistical significance is defined as the probability that it will reject a false null hypothesis. Statistical power is inversely related to beta or the probability of making a Type II error. In short, power = 1 – β.

For quantitative explanatory variables, effect sizes always have the unit of the response variable divided by the unit of the explanatory variable.

### Stats For Hackers
 
* Methods: Simluation, Boostrapping (with replacement, not good with ranks), Shuffling (Works when assumption is that the data are the same, representative samples, non-longitudinal data), & Cross Validation.

* Assumes iid.

### Think Stats

* Is it possible that the apparent effect is due to selection bias or some other error in the experimental setup? If so, then we might conclude that the effect is an artifact; that is, something we created (by accident) rather than found.

* The sampling distribution is the distribution of the samples mean.

* The CDF is the function that maps values to their percentile rank in a distribution.

* I’ll start with the exponential distribution because it is easy to work with. In the real world, exponential distributions come up when we look at a series of events and measure the times between events, which are called interarrival times. If the events are equally likely to occur at any time, the distribution of inter-arrival times tends to look like an exponential distribution.

* CLT: More specifically, if the distribution of the values has mean and standard deviation μ and σ, the distribution of the sum is approximately N (nμ, nσ^2).

* Another option is to report just the Bayes Factor or likelihood ratio, P(E | H A ) / P(E|H 0 ), rather than the posterior probability.

* Statistical power is the probability that the test will be positive if the null
hypothesis is false. In general, the power of a test depends on the sample
size, the magnitude of the effect, and the threshold α.

* If there are no outliers, the sample mean minimizes the mean squared error

* An estimator is unbiased if the expected total (or mean) error, after many iterations of the estimation game, is 0.

* A challenge in measuring correlation is that the variables we want to compare might not be expressed in the same units. For example, height might be in centimeters and weight in kilograms. And even if they are in the same
units, they come from different distributions. There are two common solutions to these problems:

1. Transform all values to standard scores. This leads to the Pearson coefficient of correlation.

2. Transform all values to their percentile ranks. This leads to the Spearman coefficient.




