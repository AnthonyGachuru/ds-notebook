# General

## Python

* https://chrisalbon.com/

* pandas: isin | crosstab

* pd.group_by aggregate functions: size, count, sum, mean, median, sd, var, min, max, prod, first, last

* The only float that isn't equal to itself is nan.

* %matplotlib notebook: interactive plots

* Memory storage: use pd.copy to create a deep copy instead of a shallow copy

* IPy: %who or %whos shows defined variables & %reset resets them

* Append dict to df: use ignore_index = True. eg. films_df = films_df.append(film_dict, ignore_index=1)

* Debugging: import pdb | pdb.set_trace(), 

* dir() gives a list of in scope variables

* globals() gives a dictionary of global variables

* locals() gives a dictionary of local variables

**Vanderplas Jupyter**

* create helper functions and units tests as R/py for Rmd/ipynb

* pytest/hypothesis for testing

* use makefile to run cmd commands

* Write file with date: df.to_csv('{}_model.csv'.format(str(datetime.datetime.now()).split(' ')[0]))

* Make package for workflow with data (__init__.py) and use this formation for function docs:

```{python, eval = FALSE}
def fun(a):

  #Role
  #-----
  #Does something
  
  #Parameters
  #---------
  #Accepts something
  
  #Returns
  #-------
  #Returns something
  
  return(a)
```

## R

* rm(list = ls()): Remove all objects in the current workspace

* cut(): Transform a numeric variable into a categorical variable.

* car: recode (good for dummying)

* Examine the objects in the workspace: ls.str()

* This is how you dynamically update a value in Rmarkdown: x = *backtick* r x *backtick*. 

**R for Everything Talk**

* dir.exists | dir.create | download.file

* untar | unlink | file.info

* dir | file.rename | file.copy

* count.fields | system

* dplyr five main actions: select, filter, arrange, mutate, summarize.

## Git

* "Just learned something cool- when you screw up a git merge, you can use git reset --hard master@{"300 minutes ago"} with any time quantity you want in there to get back to where things were a period of time ago." [[source]](https://twitter.com/data_stephanie/status/968226587547258886)

* Add upstream to update a forked repo with the original: remote add upstream repo_url 

* To undo git add . use git reset (no dot)

* git config --global credential.helper 'cache --timeout=10000000'

* Update chnages from original repo, put in new branch: git fetch upstream

* Merge the upstream chnages to master on your local machine: git merge upstream/master

* Update local repo with master changes: git pull origin master

* git diff (--cache) | git rm --cached 't.py' //remove from staging area

* git show version_name + git branch (--merged, --no-merged) + git merge fix20

* never rebase commits to public repo + rebase: branch working with behind in commit

* use revert (change commit and log the change) + dont use reset: erases commit and doesnt log.

* get fork url use: git clone url + git remote add upstream url: ref original repo

* git fetch upstream: pull changes fro original repo + git push origin master: push stuff to clone

* git merge upstream/master: merge github and local + make pull request, send pull request

* Reset: Move master and working directory.

* Revert: Less harsh version of reset, maintains commit history but moves forward by one from most recent commit.

## Other

* Cheatsheets: [1](https://startupsventurecapital.com/essential-cheat-sheets-for-machine-learning-and-deep-learning-researchers-efb6a8ebd2e5) | [2](https://github.com/kailashahirwar/cheatsheets-ai) | [3](https://github.com/juliangaal/python-cheat-sheet):[4](http://www.datasciencefree.com/cheatsheets.html)

## YAML

A really useful feature is sharing environments so others can install all the packages used in your code, with the correct versions. You can save the packages to a YAML file with:

`conda env export > environment.yaml`

The first part conda env export writes out all the packages in the environment, including the Python version. Above you can see the name of the environment and all the dependencies (along with versions) are listed. The second part of the export command, > environment.yaml writes the exported text to a YAML file environment.yaml. This file can now be shared and others will be able to create the same environment you used for the project. To create an environment from an environment file use:

`conda env create -f environment.yaml`

This will create a new environment with the same name listed in environment.yaml.

## AB Testing Overview

Guide: Choose metric (CTR), review stats (Hypothesis Testing), design, analyse

Need to have clear control and thing to test. Also need time. Frequency of customer interaqction is important.

shopping and searching not independent events.

Point estimate of CTR (users who clicked)/users. Need margin of error for estimate. Approximate binomial by normal if N*p & N(1-p) > 5. 

marin of error = Z*(stndad error)

se = sqrt(p(1-p)/n)

Z for 95% s 1.96 and 2.58 for 99%.

for Two samples calculate a pooled CTR point estimate and standard error. 

Need to be substantive (a worthy difference) in addition to being statistically significant. Some changes might not be useful due to the resources one might want to allocate.

Result are repeatable (stat sig) bar should be lower than business interesting (practically sig)

Stat Power: How many page view necessary to get a stat sig result. 

Size v Power: The smaller the effect or increased confidence you want to detect/have (greater power of experiment), the larger the sample you need to use.

alpha = P(reject null when true) 
beta = P(accept null when false)

small sample: low alpha, high beta
larger sample: low alpha, low beta

1- alpha = confidence level
1 - beta = sensitivity (often 80%)

practical significance: 2%, alpha = 0.05, beta = 0.2

sample size dp_to ctr (increase in se), confidence level, sensitivity
sample size invp_to practical sig (larger changes easier to detect)

above is design of experiment. below is analysis:

estimate diff:  experimental prop - control prop
margin of error = z*se

conf int: estimated diff +/- margin of error

### Policy & Ethics

In the study, what risk is the participant undertaking? The main threshold is whether the risk exceeds that of “minimal risk”.

Next, what benefits might result from the study? Even if the risk is minimal, how might the results help? 

Third, what other choices do participants have? For example, if you are testing out changes to a search engine, participants always have the choice to use another search engine.

Finally, what data is being collected, and what is the expectation of privacy and confidentiality? This last question is quite nuanced, encompassing numerous questions: Do participants understand what data is being collected about them? What harm would befall them should that data be made public? Would they expect that data to be considered private and confidential?

Our recommendation is that there should be internal reviews of all proposed studies by experts regarding the questions:

Are participants facing more than minimal risk?
Do participants understand what data is being gathered?
Is that data identifiable?
How is the data handled?

Internal process recommendations
Finally, regarding internal process of data handling, we recommend that:

Every employee who might be involved in A/B test be educated about the ethics and the protection of the participants. Clearly there are other areas of ethics beyond what we’ve covered that discuss integrity, competence, and responsibility, but those generally are broader than protecting participants of A/B tests (cite ACM code of ethics).

All data, identified or not, be stored securely, with access limited to those who need it to complete their job. Access should be time limited. There should be clear policies of what data usages are acceptable and not acceptable. Moreover, all usage of the data should be logged and audited regularly for violations. You create a clear escalation path for how to handle cases where there is even possibly more than minimal risk or data sensitivity issues.

rate better for usability test than prob

### Choosing & Characterizing Metrics

**Define**

Make sure to granularize funnel

**Intuition**

NA

**Characterize**

Focus groups, UER (user experience study), survey, experiments, retrospective analysis, external data

temporal effects across hours, days, weeks are important

different browsers screw with the CTR depending on how they deal with JS

Can use cookies to track users

Metric definitions

Def #1 (Cookie probability): For each <time interval>, number of cookies that click divided by number of cookies

Def #2 (Pageview probability): Number of pageviews with a click within <time interval> divided by number of pageviews

Def #3 (Rate): Number of clicks divided by number of pageviews

CTR vs CTP: The first can be >1. 

Take into consideration mean and median. Median is robust but might not be useful so look at percentiles. 

Measuring sensitiviyt and robustness: 1) Running experiments to see if metrics move as predicted. A vs A experiments: Measure people who saw the same thing to see if there is a difference between them according to the metric or its too sensitive. Look at previous experiments. 2) Retrospective analysis of logs to see how the metrics responded in the past.

Metric For Loading Vid: Look at distribution of load times for vid. Want a robust metric that doesn't really change for comparable vids. But wnat a metric that is sensitive to a change that you care about like resolution.

How to compare experiment vs control: difference or relative change. Relative change allows you to keep same significance boundary. 

Check that praticial sig level is good for metric. Need to have handle on variability for confidence interval.

sign test: Good for looking to implement a change, but doesn't give effect size.

empirical vs analytic: underlying distribution could be weird so do empirical. Analyticcal for simple metrics. If empirical and anlytical don't agree then run AvA test.

A vs A test: Std proportional to square root of number of samples. Diminishing returns for many tests. Can use bootstrap if don't have enough traffic for a big A v A test.

### Designing An Experiment

**Choose Subject**

Unit of Diversion: How to indentify a person. Cookie (could clear so change), userid, deviceid, and address (changes all the time).

Intra-user vs inter-user experiment.

**Choose Population**

Size

Duration

Cohort: People who enter the experiment at the same time.

Cohort > Population: Learning effects, user retention

Can reduce sample if just targeting english speaking traffic

When going to run experiment: holiday or not? How many people to put through experiment and control?

**Analyzing Results**

Sanity Checks:

Pass all before move on

Good invariants: # of events, video load time (user has no control over it). Want these to  be about the same for experiment and control.

Single Metric:

sign test

Multiple Metrics:

More metrics increase false positive chance. Use Bonferroni assumption: makes no independence assumption and is conservative. 

margin of error smaller than observed diff so the confidence interval won't include 0. 

Different strategies: family-wise error rate, false discovery rate, 

Gotchas: 

Simpson's Paradox

Usually A/B testing works for testing changes in elements in the web page. A/B testing framework is following sequence:

Design a research question.
Choose test statistics method or metrics to evaluate experiment.
Designing the control group and experiment group.
Analyzing results, and draw valid conclusions.

## Networks

‘Classical SNA’ is mainly about descriptive network statistics: proximity, similarity, centrality, brokerage, positional measures, equivalence, etc. (Statistical Analysis of Complete Social Networks 

### Networkx

Import necessary modules and draw graph.

```{python, eval = FALSE}
import matplotlib.pyplot as plt
import networkx as nx
nx.draw(T_sub)
plt.show()
```

Get the nodes of interest.

```{python, eval = FALSE}
noi = [n for n, d in T.nodes(data=True) if d['occupation'] == 'scientist']

### Use a list comprehension to get the edges of interest: eoi
eoi = [(u, v) for u, v, d in T.edges(data=True) if d['date'] < date(2010, 1, 1)]

### Set the weight of the edge
weight = 2
for u, v, d in T.edges(data=True):
  # Check if node 293 is involved
  if 293 in [u,v]:
    
    # Set the weight to 1.1
    T[u][v]['weight'] = 1.1
```

## Stats

Sample size: A good formula is: if your sample is picked uniformly at random and is of size at least:

$\frac{-log(\frac{d}{2})}{2e^2}$

then with probability at least 1-d your sample measured proportion q is no further away than e from the unknown true population proportion p (what you are trying to estimate). Need to estimate with 99% certainty the unknown true population rate to +- 10% precision? That is d=0.01, e=0.1 and a sample of size 265 should be enough. A stricter precision of +-0.05 causes the estimated sample size to increase to 1060, but increasing the certainty requirement to 99.5% only increases the estimated sample size to 300.

* Type I Error: Saying there is an effect when there isn't one. Type II error: concluding there is no effect when, in fact, there is one.

* The MWW RankSum test is a useful test to determine if two distributions are significantly different or not. Unlike the t-test, the RankSum test does not assume that the data are normally distributed, potentially providing a more accurate assessment of the data sets.

* Probably the most useful types of statistics for skewed probability distributions are quantiles. 

* Hypothesis testing requires an inferential-statistical approach. Crucial are meaningful distributions of test statistics, on which p-values for hypothesis tests can be based. It is not trivial to construct such “meaningful distributions” for complete network data!

* Random sampling & random assignment: generalizable & causal

* Get 95% ci around p-value

* There is no one rule about when a number is not accurate enough to use, but as a rule of thumb, you should be cautious about using any number with a MOE (Margin-of-error) over 10%.

* Probability is calibrated if it is empirically correct, i.e. the empirical probability matches the theoretical one.

* [Effect size](https://artax.karlin.mff.cuni.cz/r-help/library/lsr/html/cohensD.html)

* jarque.bera.test: Tests the null of normaility for the series using the Jarque-Bera test statistics	

* box.test: Compute the Box-Pierce or Ljung-Box test statistics for examining the null of independence in a given series			

* shapiro.test: Test for normality		

* Sum of normally distributed random variables: sum of means & sum of variances

* Zero correlation doesn't mean independent variables. Covariance only determines linear relationships.

* test heteroscedaticity: breusch-pagan or ncv test

* Test Normality: Shapiro–Wilk test is a test of normality

* Bypass inferential stats if features at least 1/10 of data points. 

* The probability (p-value) of observing results at least as extreme as what is present in your data sample. P-value less than .05 retain h_0 else reject h_0.

* One Sample T-test: To examine the average difference between a sample and the known value 
of the population mean. Assumes the population from which the sample is drawn is normally distributed and the sample observations are randomly drawn and independent. 

* Two Sample T-test: To examine the average difference between two samples drawn from two 
different populations. Assumes the populations from which the samples are drawn are normally dist, the standard deviations of the two populations are equal, and sample observations are randomly drawn and independent.

* One-Way ANOVA: To assess the equality of means of two or more groups. Assumes the populations from which the samples are drawn are normally dist, the standard deviations of the populations are equal, and sample observations are randomly drawn and independent. 

* Chi-Squared Test of Independence: To test whether two categorical variables are independent.  Assumes the sample observations are randomly drawn and independent.

* F-Test: To assess whether the variances of two different populations are equal. Assumes the population from which the sample is drawn is normally distributed and the sample observations are randomly drawn and independent.

* Barlett Test: F-Test for more than two populations. 

* If we take a larger and larger sample from a population, its distribution will tend to become normal no matter what it is initially. It won't. The Central Limit Theorem, the misreading of which is the cause of this mistake, refers to the distribution of standardized sums of random variables as their number grow, not to the distribution of a collection of random variables.

* Frequentists: Probability is a measure of the the frequency of repeated events. The parameters are fixed (but unknown) and data are random.

* Bayesians: Probability is a measure of the degree of certainty about values, so the interpretation is that rameters are random and data are fixed.

* Independent rvs are uncorrelated. That is Cor (X,Y)=0

* Don’t sum sd’s, sum variances.

* One Sample T-Test? To examine the average difference between a sample and the known value of the population mean. Assumptions: The population from which the sample is drawn is normally distributed. Sample observations are randomly drawn and independent.

* When do we use the Two Sample T-Test? To examine the average difference between two samples drawn from two different populations. Assumptions: The populations from which the samples are drawn are normally dist. The standard deviations of the two populations are equal. Sample observations are randomly drawn and independent.

* When do we use the F-Test? To assess whether the variances of two different populations are equal. Assumptions: The populations from which the samples are drawn are normally dist. Sample observations are randomly drawn and independent.

* When do we use One-Way ANOVA?  To assess the equality of means of two or more groups. NB: When there are exactly two groups, this is equivalent to a Two Sample T-Test. Assumptions: The populations from which the samples are drawn are normally dist.  The standard deviations of the populations are equal. Sample observations are randomly drawn and independent.

* When do we use the chisq2 Test of Independence? To test whether two categorical variables are independent. Assumptions: Sample observations are randomly drawn and independent.

* To perform the one-way ANOVA, we can use the f_oneway() function of the SciPy package.

* The degrees of freedom is the number of data rows minus the number of coefficients fit.

* Hypothesis tests aim to describe the plausibility of a parameter taking on a specific value. Confidence intervals aim to describe a range of plausible values a parameter can take on. If the value of the parameter specified by H0 is contained within the 95% confidence interval, then H0 cannot be rejectedat the 0.05 p-value threshold. If the value of the parameter specified by H0 is not contained within the 95% confidence interval, then H0 can be rejectedat the 0.05 p-value threshold.

### [Inference Package](https://cran.r-project.org/web/packages/infer/infer.pdf)

Main verbs: specify() variables, hypothesize(), generate() data, calculate() metrics, visualize() results

```{r, eval = FALSE}
set.seed(4747)
perm_slope <- twins %>%
  specify(Foster ~ Biological) %>%
  hypothesize(null = "independence") %>%
  generate(reps = 10, type = "permute") %>%
  calculate(stat = "slope")
```

### Statistical Tests

The appropriate sample size depends on many things, chiefly the complexity of the analysis and the expected effect size. The "30" rule comes from one of the simplest cases: Whether to use a z-test or t-test for comparing two means. 

1) for large n (sample size), many distributions can be approximated to normal. [ courtesy : Central Limit Theorems ]

2) The approximation becomes better with larger n, given other things remaining the same.

However if you are wondering why 30 ? why not 25 or 40 or something else ? Note that actual answer will depend:

1) how much error are you going to tolerate

2) size of the effect

3) exact distribution family (like t vs chi-square vs gamma)

The power of any test of statistical significance is defined as the probability that it will reject a false null hypothesis. Statistical power is inversely related to beta or the probability of making a Type II error. In short, power = 1 – β. In plain English, statistical power is the likelihood that a study will detect an effect when there is an effect there to be detected. If statistical power is high, the probability of making a Type II error, or concluding there is no effect when, in fact, there is one, goes down. Statistical power is affected chiefly by the size of the effect and the size of the sample used to detect it. Bigger effects are easier to detect than smaller effects, while large samples offer greater test sensitivity than small samples.

The following four quantities have an intimate relationship:

* sample size

* effect size

* significance level = P(Type I error) = probability of finding an effect that is not there

* power = 1 - P(Type II error) = probability of finding an effect that is there

Given any three, we can determine the fourth.

**Other**

* The power of any test of statistical significance is defined as the probability that it will reject a false null hypothesis. Statistical power is inversely related to beta or the probability of making a Type II error. In short, power = 1 – beta.

* For quantitative explanatory variables, effect sizes always have the unit of the response variable divided by the unit of the explanatory variable.

* [Power Analysis Calculator](http://www.evanmiller.org/ab-testing/sample-size.html)

### Stats For Hackers
 
* Methods: Simluation, Boostrapping (with replacement, not good with ranks), Shuffling (Works when assumption is that the data are the same, representative samples, non-longitudinal data), & Cross Validation.

* Assumes iid.

### Think Stats

* Is it possible that the apparent effect is due to selection bias or some other error in the experimental setup? If so, then we might conclude that the effect is an artifact; that is, something we created (by accident) rather than found.

* The sampling distribution is the distribution of the samples mean.

* The CDF is the function that maps values to their percentile rank in a distribution.

* I’ll start with the exponential distribution because it is easy to work with. In the real world, exponential distributions come up when we look at a series of events and measure the times between events, which are called interarrival times. If the events are equally likely to occur at any time, the distribution of inter-arrival times tends to look like an exponential distribution.

* CLT: More specifically, if the distribution of the values has mean and standard deviation μ and σ, the distribution of the sum is approximately N (nμ, nσ^2).

* Another option is to report just the Bayes Factor or likelihood ratio, P(E | H A ) / P(E|H 0 ), rather than the posterior probability.

* Statistical power is the probability that the test will be positive if the null
hypothesis is false. In general, the power of a test depends on the sample
size, the magnitude of the effect, and the threshold α.

* If there are no outliers, the sample mean minimizes the mean squared error

* An estimator is unbiased if the expected total (or mean) error, after many iterations of the estimation game, is 0.

* A challenge in measuring correlation is that the variables we want to compare might not be expressed in the same units. For example, height might be in centimeters and weight in kilograms. And even if they are in the same
units, they come from different distributions. There are two common solutions to these problems:

1. Transform all values to standard scores. This leads to the Pearson coefficient of correlation.

2. Transform all values to their percentile ranks. This leads to the Spearman coefficient.
