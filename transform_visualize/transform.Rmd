---
title: "Untitled"
author: "Gordon"
date: "January 16, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Transform

* Too many levels for a category?

    * limit number of categories through feature selection
    * https://stats.stackexchange.com/questions/95212/improve-classification-with-many-categorical-variables
    * bucket groups by number of samples
 
* https://cran.r-project.org/web/packages/vtreat/index.html

* Long story short, if these outliers are really such (i.e. they appear with a very low frequency and very likely are bad/random/corrupted measurements) and they do not correspond to potential events/failures that your model should be aware of, you can safely remove them. In all other cases you should evaluate case by case what those outliers represent.

**Imputation**

Pros include helping to retain a larger data set, potentially avoiding bias, and the resulting standard errors tend to be too small.

Some types:

* Mean: Simple but can distort distribution, underestimate standard deviation, and distort variable relationships by dragging correlation to zero.

* Random: Can amplify outlier observation and induce bias.

* Regression: Use observed variables and relationships between these variables. Must make assumptions and can badly extrapolate.
