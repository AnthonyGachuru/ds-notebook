# Pre-Modeling

outliers: 1) dropping is an option but a bad one, 2) create a new columns to mark outliers, 3) rescale with the log to reduce the effect of outliers

tomek link: points of different classes which are neighbors of each other. Removing them often improves model performance

## Import

### General

* A schema is a blueprint for storing data. For a table every row has same number of columns, and each column is of the same type.
    
* Make codebooks

[R Spreadsheet](https://www.jamovi.org/)

https://www.webscraper.io/documentation#open-web-scraper

Documentation: [1](http://databasenotetaker.com/) | [2](https://dataedo.com/) | [3](https://www.apexsql.com/sql-tools-doc.aspx) | [4](https://techwriter.me/best-practices-guide/documenting-databases.aspx) | [5](http://help.osf.io/m/bestpractices/l/618767-how-to-make-a-data-dictionary) | [6](https://dataedo.com/blog/different-types-of-tools-you-can-use-to-document-your-database) | [7](https://drawsql.app/#features)
  
CLI: [1](http://bconnelly.net/working-with-csvs-on-the-command-line/#taking-a-peek-at-the-data-set) | [2](https://opensource.com/article/17/2/command-line-tools-data-analysis-linux?sc_cid=701600000011jJVAAY) | [3](http://alexisperrier.com/shell/2017/11/07/command-line-data-scientist.html)

[pdf table extraction](https://tabula.technology)

[DEaaS](https://www.astronomer.io/)
  
[data build tool](https://www.getdbt.com/)

https://www.stitchdata.com/

https://www.prefect.io/

### SQL

* Maintain a query library

* Common Table Expressions

* cross apply = inner join | outer apply = full join

* SQL UNION stacks one dataset on top of the other. It only appends distinct values. More specifically, when you use UNION, the dataset is appended, and any rows in the appended table that are exactly identical to rows in the first table are dropped. If you’d like to append all the values from the second table, use UNION ALL.

* COUNT(DISTINCT month): returns count of non-null values like table in R

* The CASE statement is followed by at least one pair of WHEN and THEN statements: CASE WHEN year = 'SR' THEN 'yes' ELSE NULL END. You can also define a number of outcomes in a CASE statement by including as many WHEN/THEN statements as you’d like

* If you include two (or more) columns in a SELECT DISTINCT clause, your results will contain all of the unique pairs of those two columns

## Tidy

* In tidy data: Each variable forms a column. Each observation forms a row. Each type of observational unit forms a table. There are three interrelated rules which make a dataset tidy.

  * Each variable must have its own column.
  * Each observation must have its own row.
  * Each value must have its own cell.
  
http://pyviz.org/

http://dataviz.tools/

https://www.datawrapper.de/

https://www.data-to-viz.com

https://dominikkoch.github.io/Bump-Chart/

https://www.r-graph-gallery.com/

http://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html#top

[Stats Viz](http://emilkirkegaard.dk/understanding_statistics/)

[Seeing Theory](http://students.brown.edu/seeing-theory/)

[Python Plotting Guide](http://pythonplot.com/)
  
[Py Graph Gallery](https://python-graph-gallery.com)

ggplot: [1](https://www.pitt.edu/~naraehan/presentation/Graphs_and_Plots_using_Plotly.html) | [2](https://plot.ly/python/table/) | [3](https://plot.ly/python/html-reports/) | [Extensions](http://www.ggplot2-exts.org/gallery/)

[Sankey Diagram Creator](http://sankeymatic.com)

[R Waffle Charts](https://nsaunders.wordpress.com/2017/09/08/infographic-style-charts-using-the-r-waffle-package/)

[High Dimensional](https://research.googleblog.com/2016/12/open-sourcing-embedding-projector-tool.html)

[Links](http://www.thehackerwithin.org/swinburne/links.html)

[Tufte In R](http://motioninsocial.com/tufte/)

[R Graph Compendium](http://shinyapps.org/apps/RGraphCompendium/index.php)

[Data Viz Catalogue](http://www.datavizcatalogue.com/index.html)

[Text Visualization](http://textvis.lnu.se/)
  
## Transform

### General

* There is no one rule about when a number is not accurate enough to use, but as a rule of thumb, you should be cautious about using any number with a MOE (Margin-of-error) over 10%.

A good formula is: if your sample is picked uniformly at random and is of size at least:

$\frac{-log(\frac{d}{2})}{2e^2}$

then with probability at least 1-d your sample measured proportion q is no further away than e from the unknown true population proportion p (what you are trying to estimate). Need to estimate with 99% certainty the unknown true population rate to +- 10% precision? That is d=0.01, e=0.1 and a sample of size 265 should be enough. A stricter precision of +-0.05 causes the estimated sample size to increase to 1060, but increasing the certainty requirement to 99.5% only increases the estimated sample size to 300.

* Use spatial sign to transform data with outliers.

* Too many levels for a category:
    * limit number of categories through feature selection
    * https://stats.stackexchange.com/questions/95212/improve-classification-with-many-categorical-variables   * bucket groups by number of samples

* Long story short, if these outliers are really such (i.e. they appear with a very low frequency and very likely are bad/random/corrupted measurements) and they do not correspond to potential events/failures that your model should be aware of, you can safely remove them. In all other cases you should evaluate case by case what those outliers represent.

* Otherwise, assuming levels of the categorical variable are ordered, the polyserial correlation (here it is in R), which is a variant of the better known polychoric correlation. Note the latter is defined based on the correlation between the numerical variable and a continuous latent trait underlying the categorical variable.

* You should use a logarithmic scale when percent change, or change in orders of magnitude, is more important than changes in absolute units. You should also use a log scale to better visualize data that is heavily skewed. Taking the logarithm only works if the data is non-negative. There are other transforms, such as arcsinh or signed log, that you can use to decrease data range if you have zero or negative values. 

* Normalizing by mean and standard deviation is most meaningful when the data distribution is roughly symmetric.

* Monetary amounts are often log- distributed—the log of the data is normally distributed. This leads us to the idea that taking the log of the data can restore symmetry to it. 

* Dixon and Chi-squared tests for outlier detection plus LOF Algorithm.

### Missingness & Imputation

* There are three types of missingness: 1) Completely at Random, 2) At Random, 3) Not at random

* Types of imputation

  * Mean: Simple but can distort distribution, underestimate standard deviation, and distort variable relationships by dragging correlation to zero.
  * Random: Can amplify outlier observation and induce bias.
  * Regression: Use observed variables and relationships between these variables. Must make assumptions and can badly extrapolate.
  & multiple stochastic regression imputation

* The pros of imputation: 1) Helps retain a larger sample size of your data, 2) Does not sacrifice all the available information in an observation because of sparse missingness, 3) Can potentially avoid unwanted bias.

* The cons of imputation: 1) The standard errors of any estimates made during analyses following imputation can tend to be too small, 2) The methods are under the assumption that all measurements are actually “known,” when in fact some were imputed, 3) Can potentially induce unwanted bias.

* logging converts multiplicative relationships to additive relationships, and by the same token it converts exponential (compound growth) trends to linear trends. By taking logarithms of variables which are multiplicatively related and/or growing exponentially over time, we can often explain their behavior with linear models.

## Visualize

* swarmplot | marimekko charts | radviz | Marey Chart

* https://mobile.twitter.com/andrewheiss/status/1095443941875343360

## General

* sklearn log_loss | brier score| cross entropy | youdens j

* hamming distance for categorical data. Available in sklearn DBSCAN.

* precision: how good were you at something being true when you said it was true. recall: how many of the true values did you catch. f1: harmonic mean of the two

* Brier score: Don't use for ordinal predictions. 0.25 if say .5 and .33 if randomly assign probs.

* R uses class encoded as 1 in classification to make predictions. Use levels() function on factor to determine what 1 is.

* 20 observations per feature is pretty good for making predictions.

* y = b_0 + b_1x_1 + b_2x_2 + b_3x_1x_2. ( b_1 + b_2x_2 ) is the increase in y with a unit increase in x_1.

* Algorithms for Rec Systems: Linear/Logistic/Elastic Net Regression, Restricted Boltzmann Machines, Singular Value Decomposition, Makov Chains, Latent Dirichlet Allocation, Association Rules, Gradient Boosted Decision Trees, Random Forests, Affinity Propagation, K-Means, Matrix Factorization, Alternating Least Squares.

* For example, multilevel models themselves may be referred to as hierarchical linear models, random effects models, multilevel models, random intercept models, random slope models, or pooling models.

* Multi-Modal Classification: In the machine learning community, the term Multi-Modal is used to refer to multiple kinds of data. For example, consider a YouTube video. It can be thought to contain 3 different modalities: 1) The video frames (visual modality), 2) The audio clip of what's being spoken (audio modality), 3) Some videos also come with the transcription of the words spoken in the form of subtitles (textual modality)

* Less samples and more features increase the chance of overfitting.

* You can choose either of the following inference strategies: offline inference, meaning that you make all possible predictions in a batch, using a MapReduce or something similar. You then write the predictions to an SSTable or Bigtable, and then feed these to a cache/lookup table. online inference, meaning that you predict on demand, using a server.

* A static model is trained offline. That is, we train the model exactly once and then use that trained model for a while. A dynamic model is trained online. That is, data is continually entering the system and we're incorporating that data into the model through continuous updates.

* How would multiplying all of the predictions from a given model by 2.0 (for example, if the model predicts 0.4, we multiply by 2.0 to get a prediction of 0.8) change the model's performance as measured by AUC? No change. AUC only cares about relative prediction scores. Yes, AUC is based on the relative predictions, so any transformation of the predictions that preserves the relative ranking has no effect on AUC. This is clearly not the case for other metrics such as squared error, log loss, or prediction bias (discussed later).

* AUC is desirable for the following two reasons: AUC is scale-invariant. It measures how well predictions are ranked, rather than their absolute values. UC is classification-threshold-invariant. It measures the quality of the model's predictions irrespective of what classification threshold is chosen. However, both these reasons come with caveats, which may limit the usefulness of AUC in certain use cases: Scale invariance is not always desirable. For example, sometimes we really do need well calibrated probability outputs, and AUC won’t tell us about that. Classification-threshold invariance is not always desirable. In cases where there are wide disparities in the cost of false negatives vs. false positives, it may be critical to minimize one type of classification error. For example, when doing email spam detection, you likely want to prioritize minimizing false positives (even if that results in a significant increase of false negatives). AUC isn't a useful metric for this type of optimization.

* Raising our classification threshold will cause the number of true positives to decrease or stay the same and will cause the number of false negatives to increase or stay the same. Thus, recall will either stay constant or decrease. In general, raising the classification threshold reduces false positives, thus raising precision.

* A feature cross is a synthetic feature formed by multiplying (crossing) two or more features. Crossing combinations of features can provide predictive abilities beyond what those features can provide individually.

* Bias: Error Introduced by approximating real-life problem by using a simple method

* Imagine a linear model with two strongly correlated features; that is, these two features are nearly identical copies of one another but one feature contains a small amount of random noise. If we train this model with L2 regularization, what will happen to the weights for these two features? L2 regularization will force the features towards roughly equivalent weights that are approximately half of what they would have been had only one of the two features been in the model.

* L2 regularization will encourage many of the non-informative weights to be nearly (but not exactly) 0.0.

* F1 Score = TP/(TP + FP + FN). Essentially how good you are at identifying the positive class.

* A variable that is completely useless by itself can provide a significant performance improvement when taken with others.

* Variance: The amount the model output will change if the model was trained using a different data

* Flexible Methods: higher variance, low bias, Example: KNN (k = 1). * Inflexible Methods: low variance, high bias, Example: Linear regression

* Set Random state by hand in sklearn.

* Generally need 3 rows of data per variable (to prevent model from seeing signal where there is only noise) [Nina Zumel]

* Domain knowledge for feature selection and random forest feature importance (5-10) best variables. [Nina Zumel]

* Because we have a couple thousand data points, even though salary can best be represented by a Poisson distribution, I'm going to use Gaussian distribution. (With bigger datasets, these two distribution start to become very similar).

* We can also tell that our input, smart_1_normalized, doesn't have a very strong relationship to our output because the standard error (0.009) is more than 1/10 of our estimate (0.02).

* The less complex an ML model, the more likely that a good empirical result is not just due to the peculiarities of the sample.

## Pre-processing

## Feature Engineering

* encode missingness a a feature

* Dealing with cyclical features: Hours of the day, days of the week, months in a year, and wind direction are all examples of features that are cyclical. Many new machine learning engineers don’t think to convert these features into a representation that can preserve information such as hour 23 and hour 0 being close to each other and not far. Keeping with the hour example, the best way to handle this is to calculate the sin and cos component so that you represent your cyclical feature as (x,y) coordinates of a circle. In this representation hour, 23 and hour 0 are right next to each other numerically, just as they should be.

```{python, eval = FALSE}
import numpy as np
df['hr_sin'] = np.sin(df.hr*(2.*np.pi/24))
df['hr_cos'] = np.cos(df.hr*(2.*np.pi/24))
df['mnth_sin'] = np.sin((df.mnth-1)*(2.*np.pi/12))
df['mnth_cos'] = np.cos((df.mnth-1)*(2.*np.pi/12))
```

* robust scaler python: median & quantiles

## Feature Selection

* Variance Threshold: Use this to exclude features that don't meet a variance threshold.
 
* Univariate Feature Selection: For classification can use chi-squared and F-Test while F-Test for regression. Functions: chi2, f_regression, f_classification from sklearn.feature_selection.

* SelectKBest/SelectPercentile: Keep the k highest scoring features and keep a user-specified highest scoring percentage of features. Can use the univariate feature selection in these functions. 

* sklearn.feature_selection.RFE(CV)

* To work around magic values, convert the feature into two features: One feature holds only quality ratings, never magic values. One feature holds a boolean value indicating whether or not a quality_rating was supplied. Give this boolean feature a name like is_quality_rating_defined.
    
* Handling extreme outliers: Given data with a very long tail, log scaling does a slightly better job, but there's still a significant tail of outlier values. Let's pick yet another approach. What if we simply "cap" or "clip" the maximum value at an arbitrary value, say 4.0? Clipping the feature value at 4.0 doesn't mean that we ignore all values greater than 4.0. Rather, it means that all values that were greater than 4.0 now become 4.0. This explains the funny hill at 4.0. Despite that hill, the scaled feature set is now more useful than the original data. Binning.

* Know your data. Follow these rules: 1) Keep in mind what you think your data should look like. 2) Verify that the data meets these expectations (or that you can explain why it doesn’t). 3) Double-check that the training data agrees with other sources (for example, dashboards).

## Other

### Unbalanced Classes

Research on imbalanced classes often considers imbalanced to mean a minority class of 10% to 20%. 

That said, here is a rough outline of useful approaches. These are listed approximately in order of effort:

* Do nothing. Sometimes you get lucky and nothing needs to be done. You can train on the so-called natural (or stratified) distribution and sometimes it works without need for modification.

* Balance the training set in some way: Oversample the minority class. Undersample the majority class.
Synthesize new minority classes.

*Throw away minority examples and switch to an anomaly detection framework.

* At the algorithm level, or after it: Adjust the class weight (misclassification costs). Adjust the decision threshold. Modify an existing algorithm to be more sensitive to rare classes.

* Construct an entirely new algorithm to perform well on imbalanced data.

* Use AUC, F1 Score, Cohen's Kappa, ROC curve, a precision-recall curve, a lift curve, or a profit (gain) curve for evaluation.

* Use probability estimates and not the default .5 threshold. 

* Undersampling, oversampling, increase weight of minority class

* Decision trees often perform well on imbalanced datasets because their hierarchical structure allows them to learn signals from both classes.

* Sklearn Eg: wclf = svm.SVC(kernel='linear', class_weight={1: 10}) & svm.SVC(kernel='linear', class_weight='balanced')
