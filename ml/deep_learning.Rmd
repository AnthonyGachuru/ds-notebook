---
title: "Market Basket"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

There are several activation functions you may encounter in practice:

* Sigmoid: Takes a real-valued input and squashes it to range between 0 and 1 ($σ(x) = 1 / (1 + exp(−x))$)

* Softmax: Same end result as sigmoid, but different function.

* tanh: Takes a real-valued input and squashes it to the range [-1, 1] ($tanh(x) = 2σ(2x) − 1$)

* ReLU: The rectified linear activation function (called ReLU) has been shown to lead to very high-performance networks. This function takes a single number as an input, returning 0 if the input is negative, and the input if the input is positive.

($f(x) = max(0, x)$)

* Advised to scale features

* Generally dealing with Images, Convolutional Neural Network is used mostly because of its better accuracy results.

* model capacity: Same as underfitting and overfitting in bias-variance. Less nodes and hidden layers corresponds to simpler model and vice versa.

* The perceptron is the simplest neural network. The perceptron is an iterative classification method.The perceptron starts with a random hyperplane then adjust its weights to separate the data.

**BackProp Algorithm**

Initially all the edge weights are randomly assigned. For every input in the training dataset, the ANN is activated and its output is observed. This output is compared with the desired output that we already know, and the error is “propagated” back to the previous layer. This error is noted and the weights are “adjusted” accordingly. This process is repeated until the output error is below a predetermined threshold.

* The last layer of a neural network captures the most complex interactions.

* When plotting the mean-squared error loss function against predictions, the slope is $2x(y-xb)$, or $2input_data error$.

* weights_updated = weights - slope*learning_rate

* This is exactly what's happening in the vanishing gradient problem -- the gradients of the network's output with respect to the parameters in the early layers become extremely small. That's a fancy way of saying that even a large change in the value of parameters for the early layers doesn't have a big effect on the output. 

* Batch: Subset of the data used to calculate slopes during back propagation. Different batches are used to calculate different updates.

* Epoch: One full pass through all the batches in the training data.

* Stochastic gradient descent calculates slopes one batch at a time.

* This network again uses the ReLU activation function, so the slope of the activation function is 1 for any node receiving a positive value as input.

* Few people use kfold cv in deep learning because of the large datasets in play. 

* Dying neuron + vanishing gradient: Change activation function

# Keras 

* declare: sequential, add: input-hidden-output, compile, fit

* Uses numpy arrays 

* keras.callbacks.EarlyStopping: Stop training when validation score stop improving after a certain number of epochs (baches?)

## Regression

* loss = mean_squared_error

* metric: rmse

* activation_function = relu

Pros: - Extremely powerful
- Can model even very complex relationships
- No need to understand the underlying data

Cons:  Prone to overfitting
- Long training time
- Requires significant computing power for large datasets
- Model is essentially unreadable

```{python}
# Import necessary modules

import keras
from keras.layers import Dense
from keras.models import Sequential

# Specify the model: Two hidden layers
n_cols = predictors.shape[1]
model = Sequential()

## Input
model.add(Dense(50, activation='relu', input_shape = (n_cols,)))

# Hidden
model.add(Dense(32, activation='relu'))

# Output
model.add(Dense(1))

# Compile the model
model.compile(optimizer = 'adam', loss = 'mean_squared_error') 

# Fit the model
model.fit(predictors, target)

#Look at summary
model.summary()

# Calculate predictions: predictions
predictions = model.predict(pred_data)
```

## Classification

* loss = categorical_crossentropy

* metric = accuracy

* activation_function = softmax

* output layer with stuff equal to number of categorical groups

```{python}
# Import necessary modules

import keras
from keras.layers import Dense
from keras.models import Sequential

# Specify the model: Two hidden layers

n_cols = predictors.shape[1]
model = Sequential()

## Input
model.add(Dense(50, activation='relu', input_shape = (n_cols,)))

# Hidden

model.add(Dense(32, activation='relu'))

# Output
model.add(Dense(2, activation = 'softmax'))

# Compile the model
model.compile(optimizer = 'sgd', loss = 'categorical_crossentropy', metrics = ['accuracy'])

# Fit the model
model.fit(predictors, target)

#Look at summary
model.summary()

#Calculate predictions: predictions
predictions = model.predict(pred_data)

#Calculate predicted probability of survival
predicted_prob_true = predictions[:, 1]
```

## Another Example

```{python}
# Import the SGD optimizer
from keras.optimizers import SGD

# Create list of learning rates: lr_to_test
lr_to_test = [.000001, 0.01, 1]

# Loop over learning rates
for lr in lr_to_test:
    print('\n\nTesting model with learning rate: %f\n'%lr )
    
    # Build new model to test, unaffected by previous models
    model = get_new_model()
    
    # Create SGD optimizer with specified learning rate: my_optimizer
    my_optimizer = SGD(lr = lr)
    
    # Compile the model
    model.compile(optimizer = my_optimizer, loss = 'categorical_crossentropy')
    
    # Fit the model
    model.fit(predictors, 
              target, 
              validation_split = 0.3, 
              epochs = 20, 
              callbacks = [early_stopping_monitor*], 
              verbose = False)
```


### Week 1

* [Rule of 30](https://www.youtube.com/watch?v=nqEYVzJLR_c&feature=youtu.be&t=31): A change that affects at least 30 data points in your validation set is usually significant and not just noise.

* Gradient descent takes about 3 times longer than the loss function. Stochastic gradient descent works off an estimate of the loss function from a sample of the training set. Scales better than normal gradient descent.

* Momentum and learning rate decay are good for knowing which way to go in gradient descent.

* Always try to lower your learning rate for improvement. 

* ADAGRAD is another optimization option that takes care of some of the hyperparameters for you.

### Week 2

* n inputs and k outputs gives you $(n+1)*k$ parameters. 

* Back propogation takes about twice the memory as forward propogation. 

* Stop model from overoptimizing on training set: early termination (stop as soon as performance on validation set drops) and regularization which applies artifical constraints to reduce the number of free parameters while making it difficult to optimize. Uses l2 regularization or dropout. Dropout works by randomly setting activations from one layer to the next to 0 repeatly. Forces the network to learn redundant representations, and takes average consensus for final prediction. Makes things more robust and prevents overfitting. Scale the non-zero activates by 2 to get the right average. If it doesn't work, go deeper.

### Week 3

* Two ways are to average the yellow, blue, and green channels or to use YUV representation. If position on the screen doesn't matter then use translation invariance. Use weight sharing if this occurs in text. 

* Things that don't chnage across time, space, etc are called statistical invariants.

### Week 4

* RNN's use shared parameters over time to extract patterns. Uses a recurrent connection to provide a summary of the past and pass this info to the classifier. 

* Backpropagation occurs throw time to the beginning. All derivative applied to same parameters, so very correlated. Bad for stochastic gradient descent and makes math very unstable. Gradients are either zero (doesn't remember the past well) or infinity. The latter is fixed by gradient clipping and the former by LSTM (long-short
term memory). LSTM replaced the rectified linear units by continuous functions. You can use dropout or L2 regularization with an LSTM.
