---
title: "Bayesian"
author: "Gordon"
date: "April 29, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## PCA

Center and scale data. The eigenvectors yield orthogonal directions of greatest variability(principal components). The eigenvalues correspond to the magnitude of variancealong the principal components. Interpretability is a negative. PCA is completely nonparametric: any data set can be plugged in and an answer comes out, requiring no parameters to tweak and no regard for how the data was recorded. From one perspective, the fact that PCA is non-parametric (or plug-and-play) can be considered a positive feature because the answer is unique and independent of the user. From another perspective the fact that PCA is agnostic to the source of the data is also a weakness. When using dimensional reduction we restrict ourselves to simpler models. Thus, we expect bias to increase and variance to decrease. Getting latent components. Visualize high dimensional data. Reduce noise. Preprocessing. scale and center data before doing. Needs regularization. Too many principal components F1 score starts to drop after increase. Best F1 is 1. set number of components. Use fit_transforms, use explained_variance_ratio_ to see how much of the variance is explained by each component. Choose features that explain most of the variances, then use a inverse_transform to reconstruct your x. Minimum number of principal components is min(n,p).

pca: It accounts for as much of the variability in the data as possible by considering highly correlated features. Each succeeding component in turn 
has the highest variance using the features that are less correlated with the first principal component and that are orthogonal to the preceding component.

1) look at variance: apply(USArrests , 2, var)
2) pca & scale: pca = prcomp(USArrests , scale = TRUE)
3) plot: biplot(pca, scale = 0)

NB: pca$rotation contains the principal component loadings matrix which explains proportion of each variable along each principal component.


Kaiser-Harris criterion suggests retaining PCs with eigenvalues > 1; PCs with eigenvalues < 1 explain less variance than contained in a single variable. Cattell Scree test visually inspects the elbow graph for diminishing return; retain PCs before a drastic drop-off.

PC columns contain loadings; correlations of the observed variables with the Pcs. h2 column displays the component comunalities; amount of variance explained by the components.

Note: Look at factorplot. Shows pot of relationship between variables and principal components.

It is also possible to decompress the reduced dataset back to 784 dimensions by applying the inverse transformation of the PCA projection. Of course this won’t give you back the original data, since the projection lost a bit of information (within the 5% variance that was dropped), but it will likely be quite close to the original data. The mean squared distance between the original data and the reconstructed data (compressed and then decompressed) is called the reconstruction error.

It turns out that many things behave very differently in high-dimensional space. For example, if you pick a random point in a unit square (a 1 × 1 square), it will have only about a 0.4% chance of being located less than 0.001 from a border (in other words, it is very unlikely that a random point will be “extreme” along any dimension). But in a 10,000-dimensional unit hypercube (a 1 × 1 × ⋯ × 1 cube, with ten thousand 1s), this probability is greater than 99.999999%. Most points in a high-dimensional hypercube are very close to the border.

Before looking at the PCA algorithm for dimensionality reduction in more detail, let's summarize the approach in a few simple steps:
Standardize the -dimensional dataset.

Construct the covariance matrix.Decompose the covariance matrix into its eigenvectors and eigenvalues.
Select  eigenvectors that correspond to the  largest eigenvalues, where  is the dimensionality of the new feature subspace ().

Construct a projection matrix  from the ""top""  eigenvectors.

Transform the -dimensional input dataset  using the projection matrix  to obtain the new -dimensional feature subspace.
