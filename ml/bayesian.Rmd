---
title: "Bayesian"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Intro

Bayes Rule:

old_prob [P(h)/p(~h)] * strength_of_evidence [p(e|h)/p(e|~h)] = new_prob [p(h|e)/p(~h|e)]

so I'll give you three reasons why you space you methods the first reason is messy data Big Data messy data non-random samples nonrandomized
experiments wanting to learn from available data environmental measurements getting data on real people real humans simple formulas simple
averages and standard deviations aren't enough we need to adjust our data we need to adjust our data to match the treatment group to the control group to
match the sample to the population of interest to do these adjustments we fit models we fit somewhat complicated miles adjusting I'm many different variables 
models have a lot of parameters classical methods that use simple point estimates of the parameters don't adequately capture uncertainty we turn to Bayesian 
methods because Bayesian inference is one way of finding a large model with metadata the second reason we use Bayesian inference is for combining information 
you might have experimental data on the drug under one condition and aggregate data under another condition we can combine polls and election forecast of 
public opinion data from multiple poles and environmental statistics we have measurements of different quality Bayesian methods are particularly adapted to 
combining information the third appeal of Bayesian methods and the sort of applications that I work on is that the inferences can map directly to decisions
based on inferences express not its estimates and standard errors but rather as probability distributions we can take these probability distributions and pipe
them directly into decision analysis for these reasons.

An advantage of recognizing that the prior distribution is a testable part of a Bayesian model is that it clarifies the role of the prior inference, and where it comes from.

##Naïve Bayes

**Intro**

**Assumptions**

**Characteristics**

**Evaluation**

**Pros/Cons**

Document/spam classification is one use. Assumptions are that all the features are equally likely and are all important. Would be computational expensive without these assumptions with having to track all the joint probabilities. Gaussian for continuous variables, Bernoulli for binary input, and Multinomial for binary and more input. Use MLE to estimate parameters for Gaussian. Bernoulli/Multinomial used for spam classification.The Laplace Estimator (usually chosen to be 1) is a corrective measure that adds a small amount of error to each of the counts in the frequency table of words. The addition of error ensures that each resulting probability of each event will necessarily be nonzero, even if the event did not appear in the training data.

Note: In addition, we can use function "partial_fit" to fit on a batch of samples incrementally while we are using. MultinomialNB and Bernoulli NB also support sample weighting. 

Pros: It is relatively simple to understand. Training the classifier does not require many observations, and the method also works well with large amounts of data. It is easy to obtain the estimated probabilityfor a classification prediction.

Cons: The method relies upon the faulty assumptions that the features in the dataset are independentand equally important. While easily attainable, the estimated probabilities are often less reliable than the predicted class labels themselves.
Formula: posterior = likelihood*prior/(marginal likelihood) or p(spam|word) = p(word|spam)*p(spam)/p(word)

Pros

* Computationally fast

* Simple to implement

* Works well with high dimensions

Cons

* Relies on independence assumption and will perform badly if this assumption is not met

##LDA

Before we take a look into the inner workings of LDA in the following subsections, let's summarize the key steps of the LDA approach:
Standardize the -dimensional dataset ( is the number of features).

For each class, compute the -dimensional mean vector.

Construct the between-class scatter matrix  and the within-class scatter matrix .

Compute the eigenvectors and corresponding eigenvalues of the matrix .

Choose the  eigenvectors that correspond to the  largest eigenvalues to construct a -dimensional transformation matrix ; the eigenvectors are the columns of this matrix.

Project the samples onto the new feature subspace using the transformation matrix.

LDA similar to Naive Bayes but doesn't assume variables are independent, assumes that p(x|y) is a multivariate normal distribution, and assumes gaussian distributions for each class share the same covariance matrix. If they don't share the same covariance matrix, then we use Quadratic Discriminant Analysis which assumes each class has its own. 

##QDA


##Bayes CTR

```{python}
from numpy.random import beta as beta_dist
import numpy as np
N_samp = 10000 #number of samples to draw
clicks_A = 450
views_A = 56000
clicks_B = 345
views_B = 49000
alpha = 1.1
beta = 14.2
A_samples = beta_dist(clicks_A + alpha, views_A - clicks_A + beta, N_samp)
B_samples = beta_dist(clicks_B + alpha, views_B - clicks_B + beta, N_samp)
```

Posterior prob that CTR_A > CTR_B given data = np.mean(A_samples > B_samples)
Prob that lift of A relative to B is >=3%: np.mean(100*(A_samples-B_samples)/B_samples > 3)

## Heavy

Thus, the “simulate from the prior, pull out the ones who matched our evidence” approach was able to combine the prior and the evidence.

Approximate Bayesian Computation

Laplace approximation: approximate the posterior of a non-conjugate model

A second-order Markov chain uses the previous 2 observations when making a prediction of the next observation.

The stationary distribution of a Markov chain is related to the first eigenvector.

An iterative algorithm is not required to find the maximum likelihood estimate of the transition matrix associated with an observed Markov chain.

In a hidden Markov model, the "hidden" portion corresponds to the state transition sequence.

A continuous hidden Markov model can be thought of as a Gaussian mixture model with a Markovian transition property between clusters.

When one uses likelihood to get point estimates of model parameters, it’s called maximum-likelihood estimation, or MLE. If one also takes the prior into account, then it’s maximum a posteriori estimation (MAP). MLE and MAP are the same if the prior is uniform.

