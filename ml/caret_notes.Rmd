---
title: "Caret Notes"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## LM

#### Vanilla

```{r cars}
# Set seed
set.seed(42)

# Fit lm model: model
model <- lm(price ~ ., data = diamonds)

# Predict on full data: p
p <- predict(model, diamonds)

# Compute errors: error
error <- p - diamonds$price

# Calculate RMSE
sqrt(mean(error^2))
```

## LM with CV

```{r}
model <- train(
  medv ~ ., Boston,
  method = "lm",
  trControl = trainControl(
    method = "cv", number = 5,
    verboseIter = TRUE
  )
)

# Fit lm model using 5 x 5-fold CV: model
model <- train(
  medv ~ ., Boston,
  method = "lm",
  trControl = trainControl(
    method = "cv", number = 5,
    repeats = 5, verboseIter = TRUE
  )
)
```

## LogReg

#### Vanilla

```{r}
# Fit glm model
model = glm(Class ~ ., family = "binomial", train)

# Predict on test: p
p = predict(model, test, type = "response")

# Calculate class probabilities: p_class
p_class <- ifelse(p > 0.50, "M", "R")
# Create confusion matrix
confusionMatrix(p_class, test$Class)
```

## Train-Test-Split & CV

```{r}

# Shuffle row indices: rows
rows = sample(nrow(Sonar))

# Randomly order data: Sonar
Sonar = Sonar[rows,]

# Identify row to split on: split
split <- round(nrow(Sonar) * .6)

# Create train: 60%
train = Sonar[1:split,]

# Create test: 40%
test = Sonar[(split+1):nrow(Sonar),]

# Create custom indices: myFolds
myFolds <- createFolds(churn_y, k = 5)

# Create reusable trainControl object: myControl
myControl <- trainControl(
  summaryFunction = twoClassSummary,
  classProbs = TRUE, # IMPORTANT!
  verboseIter = TRUE,
  savePredictions = TRUE,
  index = myFolds
)
```


## AUC/ROC

```{r}
# Predict on test: p
p = predict(model, test, type = 'response')
# Make ROC curve
colAUC(p, test$Class, plotROC = TRUE)

# Create trainControl object: myControl
myControl <- trainControl(
  method = "cv",
  number = 10,
  summaryFunction = twoClassSummary, #default option is default summary
  classProbs = TRUE, # IMPORTANT!
  verboseIter = TRUE
)
```

## Random Forest

```{r}
# Fit random forest: model
model <- train(
  quality~.,
  tuneLength = 3,
  data = wine, method = 'ranger',
  trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE)
)

model <- train(
  quality~.,
  tuneGrid = data.frame(mtry = c(2, 3, 7)),
  data = wine, method = 'ranger',
  trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE)
)

# Print model to console
model

# Plot model
plot(model)
```

## Lasso/Ridge

Alpha controls balance between lasso and ridge. Lambda controls penalty.

```{r}
# Fit glmnet model: model
model <- train(
  y~., data = overfit,
  method = "glmnet",
  trControl = myControl
)

# Train glmnet with custom trainControl and tuning: model
model <- train(
  y~., overfit,
  tuneGrid = expand.grid(alpha = 0:1, lambda = seq(0.0001, 1, length = 20)),
  method = 'glmnet',
  trControl = myControl
)

# Print model to console
model

# Print maximum ROC statistic
max(model[["results"]][["ROC"]])
```

## Preprocessing

```{r}
# Apply median imputation: model
model <- train(
  x = breast_cancer_x, 
  y = breast_cancer_y,
  method = 'glm',
  trControl = myControl,
  preProcess = c('medianImpute', "knnImpute", 'center', 'scale', 'pca')
)

# Print model to console
model

dotplot(resamples, metric = "ROC")
min(model$results$RSME)
```

```{r}
# Identify near zero variance predictors: remove_cols
remove_cols <- nearZeroVar(bloodbrain_x, names = TRUE, 
                           freqCut = 2, uniqueCut = 20)

# Get all column names from bloodbrain_x: all_cols
all_cols = colnames(bloodbrain_x)

# Remove from data: bloodbrain_x_small
bloodbrain_x_small <- bloodbrain_x[ , setdiff(all_cols, remove_cols)]
```

## Model Selection

Now that you have fit two models to the churn dataset, it's time to compare their out-of-sample predictions and choose which one is the best model for your dataset.

You can compare models in caret using the resamples() function, provided they have the same training data and use the same trainControl object with preset cross-validation folds. resamples() takes as input a list of models and can be used to compare dozens of models at once (though in this case you are only comparing two models).

```{r}
# Create model_list
model_list <- list(item1 = model_glmnet, item2 = model_rf)

# Pass model_list to resamples(): resamples
resamples = resamples(model_list)

# Summarize the results
summary(resamples)

# Create bwplot
bwplot(resamples, metric = 'ROC')

# Create xyplot
xyplot(resamples)
```

## Stacking

caretEnsemble provides the caretList() function for creating multiple caret models at once on the same dataset, using the same resampling folds. You can also create your own lists of caret models. Use the caretStack() function to make a stack of caret models, with the two sub-models (glmnet and ranger) feeding into another (hopefully more accurate!) caret model.

```{r}
# Create ensemble model: stack
stack <- caretStack(model_list, method = 'glm')

# Look at summary
summary(stack)
```

https://cran.r-project.org/web/packages/caretEnsemble/vignettes/caretEnsemble-intro.html

caret: finalModel of model object

























=======
>>>>>>> 00525289f4e436a27c3ff4dfb970ac96774c225f:ml/_caret_notes_.Rmd
