# Causality

## General

[Causality & DL](https://twitter.com/tdietterich/status/1034631407904018437)

Instrumental variables

propensity score matching

http://causalinference.gitlab.io/icwsm-tutorial/

http://causalinference.gitlab.io/dowhy/do_why_simple_example.html

## Causality edX

Draw a complete DAG if your expert knowledge is insufficient to exclude any possible effect.
Knowledge is expressed in the form of missing arrows.

Causal Markov Condition: If two vars share a cause this cause is also on the graph.

An association exists b/w A & Y if having info on A on average better allows us to predict Y.

An association for a -> y exists if the proportion of individuals with y is different given having or not having a.

A -> B -> Y: B is a mediator which blocks the association b/w A & Y when conditioning on it 
even though A is causal for Y.

Systematic bias: Any assoication between A & Y not causes by an effect of A on Y.

Bias due to a common cause is called confounding. We expect an association b/w two items with a commong cause.

a <- t -> y: t is a commong cause which blocks the association from a to y when conditioned upon.

Common effects ( a -> y <- b) don't cause an association but common causes ( a <- y -> b) do.

For a -> y <- b, y is called a collider. It blocks association between a and b.

Selection bias: Conditioning on a collider introduces an association between its causes.

Bias stems from The existence of a shared cause of treatment and outcome and conditioning on a common effect of treatment and outcome. (Also conditioning on a mediator.)

Associations: 1) Cause & effect, 2) Common causes, 3) Conditioning on a common effect

Bias introduced by association by chance can be removed by increasing the sample size.

A (parent) -> B (descendant)

D(irectionally)-separation rules: Determine whether paths (which don't need to follow arrows) are blocked or open. 

D-separation rules:

1. If there are no variables being conditioned on, a path is blocked iff two arrows on the path collide at some
variable on the path.

2. Any path with a non-collider that has been conditioned on is blocked. 

3. A collider that has been conditioned on doesn't block the path.

4. A collider with a descendant that has been conditioned on doesn't block the path.

The summary of these D-separation rules is that a path is blocked if, and only if, it
contains a non-collider that has been conditioned on, or a collider that has not been conditioned on
and has no descendant that had been conditioned on. 

Two variables are d-separated if all paths between then are blocked

And two variables are marginally or unconditionally independent if they are D-separated without conditioning
on all the variables. On the other hand, we say that two variables are conditionally independent, even a set variables L, if they are D-separated after conditioning on the variables L. 

Faithfulness: Causal effects in opposite directions that cancel out, i.e a certain medication causes diseases in some
people and prevents it in others. Rare.

All paths are blocked variables are not associated.

back door path: a path connecting A and Y without using arrows that leave from A. eg. Given L → A → Y ← L, the path A - L - Y is a back door path.

back door path criterion:  We can identify the causal effect of A on Y if we have enough data to block all back door paths between them

confounder: A variable that, possibly with other variables, can block back door paths between treatment and outcome.

Change in estimate definition of confounder: A var is a confounder of the effect of a on y if adjusting for it alters the association between a and y

Conventional definition of confounder: L is a confounder of the effect of a on y if L meets 3 conditions: L is associated with a, L is associated with y conditional on a, L is not on a causal pathway from a to y.

surrogate confounder: a descendant of a cofounder.

turn on recruiter linkedin

cofounding if absolute but cofounder is relative to other variables.

If you can't randomize then you have to rely on observational data.  Methods: 1) Measure enough variables to block all backdoor paths between A & Y, 2) Alternatives to blocking back door paths (instrumental variable estimation) Here's are a few options for 1):

Stratification (adjust for confounding): A way to adjust for a variable by only looking at a subset of it, for example only looking at people who smoke instead of all people. An alternative is to use regression to look at the effects in each subset and then pool the estimates.

Matching (adjust for confounding): Randomly match someone with a & b to ~a & b and a & ~b to ~a & ~b. Then see if there is an associating in the match subset.

G-methods: The following three methods should be used when time-varying confounders are affected by prior treatment.

Inverse Probability Weighting (adjust for confounding): Compute 1/p(a = 1 | b) and 1/p(a = 0 | b), and use these as weights in the association computation. Eliminates backdoor. 

Standardization (G-formula)

G-estimation

All these confounding adjustment methods require knowledge of the true causal DAG since you need to know which paths to block.

Randomization, when possible, is the preferred approach to eliminating confounding.

Selection bias under the null is called collider stratification bias. One way it occurs is by conditioning on a collider or the child of  a collider.

Case-control study: Outcome based selection design

Selection bias can occur due to eligibility criteria and loss to follow-up. The latter can occur from causes that have significance in the causal graph.

Internal validity: We say that a study has internal validity when the estimated association has a causal interpretation in the study population. That is, there is no selection bias, there is no confounding, et cetera.

External validity:  We say that a study has external validity when the estimated association has a causal interpretation in another population. That is, we can transport or generalize the estimated effect to other populations.

In randomized trials, self-selection bias does not affect internal validity, but may affect external validity.

Bias under the null due to self-selection at baseline can happen in a follow up study.

Smart selection, adjustment for selection bias

Use inverse probability weighting to control for the four selection bias dags discussed: {insert dags here}

Competing events

Measurement error: True value isn't equal to measured value. 

Measurement bias: When the measure of association between a and y (measured treatment & outcome values) differs from that of a and y.

Types of measurement error:

Type 1, independent non-differential.
Type 2, dependent non-differential.
Type 3, independent differential.
Type 4, dependent differential.

Independent non-differential error is the only type of measurement error that does not create bias under the null.

information bias

Statistical models and validation samples can be used to correct measurement error.

We can't correctly choose methods of measurement error correction without considering the structure of the measurement error.

A causal DAG can't eliminate bias due to confounding. A causal DAG can improve precision. A causal DAG can't improve external validity. A causal DAG can represent sources of bias.

Time-varying treatments

treatment-confounder feedback: In its presence most methods are biased. Use G Methods.

The difference method: Run regression with E[M|A,C] which has the term beta_1 times a and E[Y|A,M,C] with the terms theta_1 times a & theta_2 times m. The direct effect is theta_1 and the indirect effect is beta_1 times theta_1.

### Causality Coursera

The fundamental problem of causal inference: We can only observe one potential outcome for each person.

Causal effect: E(y^1) - E(y^0)
