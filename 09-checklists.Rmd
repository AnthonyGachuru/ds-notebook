# Checklists

An attempt to apply the principles in ["The Checklist Manifesto"](https://www.amazon.com/dp/B0030V0PEW/ref=dp-kindle-redirect?_encoding=UTF8&btkr=1) to Data Science.

## General

* Understand the business question, goals, and measures of success

* See if the data to answer the question exists, and if it is useful.

* Structure project as user stories 

* Initialize report to track and summarise work and to do list.

* Remember to use atomic commit messages

* checklists | code books | appendices | post mortems | tests | gacp

* Character Encoding Precendence: utf-8, iso-8859-1, utf-16

* Sampling from data if its too large

* [Follow tidy data principles](http://vita.had.co.nz/papers/tidy-data.pdf). 

* Figure out what you're trying to do with the data.

* See what the data looks like: types of variables, the first and last few observations, missing values or outliers.

* Outlier Detection: interquartile range, kernel density estimation, bonferroni test

* Look at missingness.

* Imputation. Options include the Mean, Mode, KNN, Random, and Regression.

* Use the Gower distance for categorical dissimilarity.

* Check for blank spaces and replace with NA. In R: `df[df==""] = NA`.

* [Binned Stats](https://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.stats.binned_statistic.html)

* Quick Exploration Guide: S.O.C.S (Shape, Outlier, Center, Spread)

* Trendlines & Histograms

* Confidence intervals

* Compare the distributions of variables with overlayed density plots 

* Scatterplots: Pairwise and color/size the dots by other variables to try to identify any confounding relationships.

* Dimensionality reduction (PCA, Kernel PCA, TSNE, Isomap) or hierarchical clustering for multivariate data to get a feel for high dimensional structure.

* Stick to regression models for prescriptive analysis. Validate assumptions and tune appropriately. If using Python leverage statsmodels.

* Pre-processing: Use binning + chisquare / mutual information to see correlation between feature and target.

* Pipelines & Feature Unions

* Feature Selection: Variance Threshold, Univariate Feature Selection, Multivariate Feature Selection.
    
* Feature Engineering: Standardization, dimensionality reduction, dummying. sklearn.preprocessing.binarize, pandas.factorize, and featuretools for automation in Python. 

* Model Explanability: SHAP

* Tuning: Do bayesian tuning instead of grid or random search. Also use hyperopt-sklearn, sklearn-deap, and scikit-optimize. 

* Deal with potential class imbalance: Gradient boosting is one option.

* Create ensembles
    
* Ensembling: mlxtend in Python, and caretEnsemble in R.

* Look at learning curves: [1](https://www.dataquest.io/blog/learning-curves-machine-learning/) | [2](http://www.scikit-yb.org/en/latest/api/model_selection/learning_curve.html)

## Linear Regression

Confidence Intervals for Model Parameters: confint in R
