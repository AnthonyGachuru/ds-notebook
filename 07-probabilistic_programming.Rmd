# Probabilistic Programming

## Bayes Rule 

old_prob [P(h)/p(~h)] * strength_of_evidence [p(e|h)/p(e|~h)] = new_prob [p(h|e)/p(~h|e)]

Posterior = Likelihood * Prior / Marginal Likelihood

Likelihood P(Data|theta): Probability of the date could be generated by a model with the given parameter(s)

Prior P(theta): Probability of parameter value

Posterior P(theta|D):  Credibility of the parameter value with the data taken into account.

Marginal Likelihood (Evidence): Probability of evidence

## Other

http://brunaw.com/slides/SER2019/talk/pres.html#1 https://causal.app/ [PyMc3 Quick Intro](https://docs.google.com/presentation/d/1buknIrG5b8u0twrwvlxcTudIOdx68AlqDiST_A_jJ9g/edit#slide=id.g4254d546f6_0_0) [Prior Guide](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations)

[PGM Viz](http://daft-pgm.org)

Another option is to report just the Bayes Factor or likelihood ratio, rather than the posterior probability.

Models have a lot of parameters classical methods that use simple point estimates of the parameters don't adequately capture uncertainty so we turn to Bayesian methods because Bayesian inference is one way of finding a large model with metadata. 

The second reason we use Bayesian inference is for combining information you might have experimental data on the drug under one condition and aggregate data under another condition we can combine polls and election forecast of public opinion data from multiple poles and environmental statistics we have measurements of different quality Bayesian methods are particularly adapted tocombining information. 

The third appeal of Bayesian methods and the sort of applications that I work on is that the inferences can map directly to decisions based on inferences express not its estimates and standard errors but rather as probability distributions we can take these probability distributions and pipe them directly into decision analysis for these reasons.

An advantage of recognizing that the prior distribution is a testable part of a Bayesian model is that it clarifies the role of the prior inference, and where it comes from.

Approximate Bayesian Computation

Laplace approximation: Approximate the posterior of a non-conjugate model

When one uses likelihood to get point estimates of model parameters, it’s called maximum-likelihood estimation, or MLE. If one also takes the prior into account, then it’s maximum a posteriori estimation (MAP). MLE and MAP are the same if the prior is uniform.

Beta conjugacy: Beta for prior & binomial for likelihoood implies beta for posterior. Update: beta(alpha + successes, beta + failures). Mean: alpha/(alpha + beta)

Small alpha and beta means the prior is less informative 

Empirical Bayes is an approximation to more exact Bayesian methods. With a lot of data it is a very good approximation.

## Doing Bayesian DA

In general, Bayesian analysis of data follows these steps:

1. Identify the data relevant to the research questions. What are the measurement scales of the data? Which data variables are to be predicted, and which data variables are supposed to act as predictors?

2. Define a descriptive model for the relevant data. The mathematical form and its parameters should be meaningful and appropriate to the theoretical purposes of the analysis.

3. Specify a prior distribution on the parameters. The prior must pass muster with the audience of the analysis, such as skeptical scientists.

4. Use Bayesian inference to re-allocate credibility across parameter values. Interpret the posterior distribution with respect to theoretically meaningful issues (assuming that the model is a reasonable description of the data; see next step).

5. Check that the posterior predictions mimic the data with reasonable accuracy (i.e., conduct a “posterior predictive check”). If not, then consider a different descriptive model.

## Statistical Rethinking

bayesian: 1) likelihood: binomial 2) parameters of the likelihood 3) prior for each param

Data are measured and known; parameters are unknown and must be estimated from data.

Compared to MCMC, variational inference tends to be faster and easier to scale to large data

To combat under/overfitting use a regularizing prior or a scoring device like information criteria.

 As you’ll see once you start using AIC and related measures, it is true that predictor variables that do improve prediction are not always statistically significant. It is also possible for variables that are statistically significant to do nothing useful for prediction. 

But the prior on β is narrower and is meant to regularize. The prior β ~ Normal(0,1) says that, before seeing the data, the machine should be very skeptical of values above 2 and below −2, as a Gaussian prior with a standard deviation of 1 assigns only 5% plausibility to values above and below 2 standard deviations. Because the predictor variable x is standardized, you can interpret this as meaning that a change of 1 standard deviation in x is very unlikely to produce 2 units of change in the outcome.

metrics: dic, waic

Sampling from the posterior now and plotting the model’s predictions would help immensely with interpretation. And that’s the course I want to encourage and provide code for. In general, it’s not safe to interpret interactions without plotting them.

bayesian model: generative model (binomial number of success) + prior (eg. sample of probabilities of success from uniform)

bayesian inference: bayesian model + data

bayesian inference is conditioning on the data to learn the parameter values.

The reason that we call it posterior is because it represents the uncertainty after (that is, posterior to) having included the information in the data. 

Bayesian Inference Methods: Sampling, Grid Approximation

## Causality

### General

[Causality & DL](https://twitter.com/tdietterich/status/1034631407904018437) Instrumental variables propensity score matching http://causalinference.gitlab.io/icwsm-tutorial/ http://causalinference.gitlab.io/dowhy/do_why_simple_example.html

### Causality edX

Draw a complete DAG if your expert knowledge is insufficient to exclude any possible effect.
Knowledge is expressed in the form of missing arrows.

Causal Markov Condition: If two vars share a cause this cause is also on the graph.

An association exists b/w A & Y if having info on A on average better allows us to predict Y.

An association for a -> y exists if the proportion of individuals with y is different given having or not having a.

A -> B -> Y: B is a mediator which blocks the association b/w A & Y when conditioning on it 
even though A is causal for Y.

Systematic bias: Any assoication between A & Y not causes by an effect of A on Y.

Bias due to a common cause is called confounding. We expect an association b/w two items with a commong cause.

a <- t -> y: t is a commong cause which blocks the association from a to y when conditioned upon.

Common effects ( a -> y <- b) don't cause an association but common causes ( a <- y -> b) do.

For a -> y <- b, y is called a collider. It blocks association between a and b.

Selection bias: Conditioning on a collider introduces an association between its causes.

Bias stems from The existence of a shared cause of treatment and outcome and conditioning on a common effect of treatment and outcome. (Also conditioning on a mediator.)

Associations: 1) Cause & effect, 2) Common causes, 3) Conditioning on a common effect

Bias introduced by association by chance can be removed by increasing the sample size.

A (parent) -> B (descendant)

D(irectionally)-separation rules: Determine whether paths (which don't need to follow arrows) are blocked or open. 
D-separation rules:

1. If there are no variables being conditioned on, a path is blocked iff two arrows on the path collide at some
variable on the path.

2. Any path with a non-collider that has been conditioned on is blocked. 

3. A collider that has been conditioned on doesn't block the path.

4. A collider with a descendant that has been conditioned on doesn't block the path.

The summary of these D-separation rules is that a path is blocked if, and only if, it
contains a non-collider that has been conditioned on, or a collider that has not been conditioned on
and has no descendant that had been conditioned on. 

Two variables are d-separated if all paths between then are blocked

And two variables are marginally or unconditionally independent if they are D-separated without conditioning
on all the variables. On the other hand, we say that two variables are conditionally independent, even a set variables L, if they are D-separated after conditioning on the variables L. 

Faithfulness: Causal effects in opposite directions that cancel out, i.e a certain medication causes diseases in some
people and prevents it in others. Rare.

All paths are blocked variables are not associated.

back door path: a path connecting A and Y without using arrows that leave from A. eg. Given L → A → Y ← L, the path A - L - Y is a back door path.

back door path criterion:  We can identify the causal effect of A on Y if we have enough data to block all back door paths between them

confounder: A variable that, possibly with other variables, can block back door paths between treatment and outcome.

Change in estimate definition of confounder: A var is a confounder of the effect of a on y if adjusting for it alters the association between a and y

Conventional definition of confounder: L is a confounder of the effect of a on y if L meets 3 conditions: L is associated with a, L is associated with y conditional on a, L is not on a causal pathway from a to y.

surrogate confounder: a descendant of a cofounder.

turn on recruiter linkedin

cofounding if absolute but cofounder is relative to other variables.

If you can't randomize then you have to rely on observational data.  Methods: 1) Measure enough variables to block all backdoor paths between A & Y, 2) Alternatives to blocking back door paths (instrumental variable estimation) Here's are a few options for 1):

Stratification (adjust for confounding): A way to adjust for a variable by only looking at a subset of it, for example only looking at people who smoke instead of all people. An alternative is to use regression to look at the effects in each subset and then pool the estimates.

Matching (adjust for confounding): Randomly match someone with a & b to ~a & b and a & ~b to ~a & ~b. Then see if there is an associating in the match subset.

G-methods: The following three methods should be used when time-varying confounders are affected by prior treatment.

Inverse Probability Weighting (adjust for confounding): Compute 1/p(a = 1 | b) and 1/p(a = 0 | b), and use these as weights in the association computation. Eliminates backdoor. 

Standardization (G-formula)

G-estimation

All these confounding adjustment methods require knowledge of the true causal DAG since you need to know which paths to block.

Randomization, when possible, is the preferred approach to eliminating confounding.

Selection bias under the null is called collider stratification bias. One way it occurs is by conditioning on a collider or the child of  a collider.

Case-control study: Outcome based selection design

Selection bias can occur due to eligibility criteria and loss to follow-up. The latter can occur from causes that have significance in the causal graph.

Internal validity: We say that a study has internal validity when the estimated association has a causal interpretation in the study population. That is, there is no selection bias, there is no confounding, et cetera.

External validity:  We say that a study has external validity when the estimated association has a causal interpretation in another population. That is, we can transport or generalize the estimated effect to other populations.

In randomized trials, self-selection bias does not affect internal validity, but may affect external validity.

Bias under the null due to self-selection at baseline can happen in a follow up study.

Smart selection, adjustment for selection bias

Use inverse probability weighting to control for the four selection bias dags discussed: {insert dags here}

Competing events

Measurement error: True value isn't equal to measured value. 

Measurement bias: When the measure of association between a and y (measured treatment & outcome values) differs from that of a and y.

Types of measurement error:

Type 1, independent non-differential.
Type 2, dependent non-differential.
Type 3, independent differential.
Type 4, dependent differential.

Independent non-differential error is the only type of measurement error that does not create bias under the null.

information bias

Statistical models and validation samples can be used to correct measurement error.

We can't correctly choose methods of measurement error correction without considering the structure of the measurement error.

A causal DAG can't eliminate bias due to confounding. A causal DAG can improve precision. A causal DAG can't improve external validity. A causal DAG can represent sources of bias.

Time-varying treatments

treatment-confounder feedback: In its presence most methods are biased. Use G Methods.

The difference method: Run regression with E[M|A,C] which has the term beta_1 times a and E[Y|A,M,C] with the terms theta_1 times a & theta_2 times m. The direct effect is theta_1 and the indirect effect is beta_1 times theta_1.

### Causality Coursera

The fundamental problem of causal inference: We can only observe one potential outcome for each person.

Causal effect: E(y^1) - E(y^0)

