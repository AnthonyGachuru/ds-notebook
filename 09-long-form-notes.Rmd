# Notes

measurement theory / decision science, semi-supervised, unsupervised, online, & weak learning

set the seed always

Shapiro-Wilk Test for Normality: H_0 says the data is normally distributed while H_a says it is not. To run this test in R, the syntax is quite simple:

import statsmodels.api as sm
X = sm.add_constant(X)
rgr_lr = sm.OLS(Y, X).fit(disp=False)
print(results.summary())

## Other

### Vanderplas Jupyter

* create helper functions and units tests as R/py for Rmd/ipynb

* pytest/hypothesis for testing

* use makefile to run cmd commands

* Write file with date: df.to_csv('{}_model.csv'.format(str(datetime.datetime.now()).split(' ')[0]))

* Make package for workflow with data (__init__.py) and use this formation for function docs:

* `def fun(a): return(a) #Role Parameters Returns`

### Sample R Pipeline

[Source](https://mobile.twitter.com/TheStephLocke/status/990251709531344896):

IO: odbc readxl httr 
EDA: DataExplorer
Prep: tidyverse
Sampling: rsample modelr
Feature Engineering: recipes
Modeling: glmnet h2o FFTrees
Evaluation: broom yardstick
Deployment: sqlrutils AzureML opencpu
Monitoring: flexdashboard
Docs: rmarkdown


natural log goes from unit change to percentage change

generalized low rank models

conditional random field

linear probability model

[http://r-pkgs.had.co.nz/tests.html](http://r-pkgs.had.co.nz/tests.html)

[https://cartesianfaith.com/2013/03/10/better-logging-in-r-aka-futile-logger-1-3-0-released/](https://cartesianfaith.com/2013/03/10/better-logging-in-r-aka-futile-logger-1-3-0-released/)

[http://www.markvanderloo.eu/yaRb/2016/03/25/easy-data-validation-with-the-validate-package/](http://www.markvanderloo.eu/yaRb/2016/03/25/easy-data-validation-with-the-validate-package/)

[https://cran.r-project.org/web/packages/assertr/vignettes/assertr.html](https://cran.r-project.org/web/packages/assertr/vignettes/assertr.html)

Power-Of-Test: https://rpsychologist.com/d3/NHST/ https://juliasilge.shinyapps.io/power-app/ https://www.statmethods.net/stats/power.html https://www.graphpad.com/quickcalcs/binomial1.cfm https://stat.ethz.ch/R-manual/R-devel/library/stats/html/power.prop.test.html https://stat.ethz.ch/R-manual/R-devel/library/stats/html/prop.test.html

confint conjoint analysis Incremental PCA fe: Permutation Measure meta learning | Rasch Model | Learning_to_rank

ff | bigmemory | biganalytics | bigtabulate | purrrlyr: dplyr purrr | furrr: parallelized purrr

foreach + doMC | parallel | doParallel

ts: Vector autoregression, quantile regression forests

Out-of-Core: pandas chunksize Semantic Style Transfer symbolic regression

Quantile Regression models the median while Linear Regression models the mean

Linear Regression - BLUE: Best Linear Unbiased (average amount wrong by is error) Estimator

* With large data we can to the train x dev x test split of 98 x 1 x 1.

Bayes optimal error: Aim for this. The difference b/w training error and BOE is the avoidable bias.

end to end deep learning: an end-to-end approach as it maps directly the input (x) to the output (y). end-to-end learning works better in practice than multi-task learning, but requires a large amount of data. 

new data degrades your model. what should you do? Use the data you have to define a new evaluation metric (using a new dev/test set) taking into account the new species, and use that to drive further progress for your team.

because even though you have higher overall accuracy, you have more false negatives (failing to raise an alarm when a bird is in the air). What should you do? Rethink the appropriate metric for this task, and ask your team to tune to the new metric.

If you overfit to the dev set, get a bigger one. 

Spend a few days training a basic model and see what mistakes it makes.

softmax not used for multi-classification


* But map allows us to bypass the function function. Using a tilda (~) in place of function and a dot (.) in place of x, we can do this: map(dat, ~mean(.$Open))

* Cohen's H | Cohen's D | Multiple comparisons Problem | Importance sampling | Bayes Error Rate

* **The Coefficient of Unalikeability**: Unalikeability is defined as how often observations differ from one another. It varies from 0 to 1. The higher the value, the more unalike the data are.

Consequently, practical decision-tree learning algorithms are based on heuristics such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. To reduce the greedy effect of local optimality, some methods such as the dual information distance (DID) tree were proposed

Laplace smoothing/estimator in Naive Bayes: To see why consider the worst case where none of the words in the training sample appear in the test sentence. In this case, under your model we would conclude that the sentence is impossible but it clearly exists creating a contradiction.

Zero correlation doesn't imply independence

* Active Learning: Finding the optimal data to manually label

* Simulated Annealing: Gradient descent extension

* "No Free Lunch" theorem: no single classifier works best across all possible scenarios

* So, if your problem involves kind of searching a needle in the haystack when for ex: the positive class samples are very rare compared to the negative classes, use a precision recall curve. Otherwise use a ROC curve because a ROC curve remains the same regardless of the baseline prior probability of your positive class (the important rare class).

* Sequence mining Algorithms: spade, gsp, freespan; hmmlearn 

* Network models: graphical lasso, ising model, granger causality test, network hypothesis testing, bayesian networks

* Parametric: Model has a function shape and form, Model is trained/fit using training data to determine parameter values, reduces the problem of training a model down to training a set of parameter values, Examples: linear regression. Non-Parametric: No assumption about model form is made, Example: KNN

* The ML Fine Print: Three basic assumptions: 1) We draw examples independently and identically (i.i.d.) at random from the distribution, 2) The distribution is stationary: It doesn't change over time, 3) We always pull from the same distribution: Including training, validation, and test sets. In practice, we sometimes violate these assumptions. For example: 1) Consider a model that chooses ads to display. The i.i.d. assumption would be violated if the model bases its choice of ads, in part, on what ads the user has previously seen. 2) Consider a data set that contains retail sales information for a year. User's purchases change seasonally, which would violate stationarity. When we know that any of the preceding three basic assumptions are violated, we must pay careful attention to metrics.

* empirical risk minimization: In supervised learning, a machine learning algorithm builds a model by examining many examples and attempting to find a model that minimizes loss.

* Loss is the penalty for a bad prediction. That is, loss is a number indicating how bad the model's prediction was on a single example. If the model's prediction is perfect, the loss is zero; otherwise, the loss is greater. 

* If a model suffers from overfitting, we also say that the model has a high variance, which can be caused by having too many parameters that lead to a model that is too complex given the underlying data. Similarly, our model can also suffer from underfitting (high bias), which means that our model is not complex enough to capture the pattern in the training data well and therefore also suffers from low performance on unseen data.

* "Product classification is an example of multicategory or multinomial classification. Most classification problems and most classification algorithms are specialized for two-category, or binomial, classification. There are tricks to using binary classifiers to solve multicategory problems (for example, building one classifier for each category, called a “one versus rest” classifier). But in most cases it’s worth the effort to find a suitable multiple-category implementation, as they tend to work better than multiple binary classifiers (for example, using the package mlogit instead of the base method glm() for logistic regression)."
