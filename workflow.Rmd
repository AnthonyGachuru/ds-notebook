---
title: "Workflow"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Preamble

* [Conda Setup](http://tdhopper.com/blog/2015/Nov/24/my-python-environment-workflow-with-conda/)

**YAML**

A really useful feature is sharing environments so others can install all the packages used in your code, with the correct versions. You can save the packages to a YAML file with:

`conda env export > environment.yaml`

The first part conda env export writes out all the packages in the environment, including the Python version. Above you can see the name of the environment and all the dependencies (along with versions) are listed. The second part of the export command, > environment.yaml writes the exported text to a YAML file environment.yaml. This file can now be shared and others will be able to create the same environment you used for the project. To create an environment from an environment file use:

`conda env create -f environment.yaml`

This will create a new environment with the same name listed in environment.yaml.

## Data Ingestion/Wrangling

* [Sqlite Converter](https://github.com/thombashi/sqlitebiter)

* Easy Database Management: [dataset](https://dataset.readthedocs.io/en/latest/)

* [Data Validation](https://github.com/data-cleaning/validate)

* Encoding Precendence: utf-8, iso-8859-1, utf-16

* na.strings: Replace empty strings by na: `x <- read.csv("filepath/R_NA.csv",header=TRUE,na.strings=c(""))`

* Missingness: Missing completely at random/missing at random/missing not at random

* Dealing with missing data: df.fillna(value), df.isnull().sum(), df = df[np.isfinite(df['EPS'])]

* scipy.stats.describe

* group_by aggregate functions: size, count, sum, mean, median, sd, var, min, max, prod, first, last

* Replacing blank cells by nas: titanic3[titanic3==""] = NA

**Explore Data**

* Histograms

* [EdaR](https://github.com/ujjwalkarn/xda)

* [Binned Stats](https://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.stats.binned_statistic.html)

* Quick Exploration Guide: S.O.C.S (Shape, Outlier, Center, Spread). 

* Outlier Detection - Tukey IQR, Kernel density estimation, Bonferroni test

* PCA & non-linear dimensionality reduction, Isomap - manifold or cluster

**Imputation**

* Pros include helping to retain a larger data set, potentially avoiding bias, and the resulting standard errors tend to be too small.

* Mean: Simple but can distort distribution, underestimate standard deviation, and distort variable relationships by dragging correlation to zero.

* Random: Can amplify outlier observation and induce bias.

* Regression: Use observed variables and relationships between these variables. Must make assumptions and can badly extrapolate.
 
## Stats

* [Effect size](https://artax.karlin.mff.cuni.cz/r-help/library/lsr/html/cohensD.html)

* Zero correlation doesn't mean independent variables. Covariance only determines linear relationships.

* test heteroscedaticity: breusch-pagan or ncv test

* Test Normality: Shapiroâ€“Wilk test is a test of normality

* Bypass inferential stats if features at least 1/10 of data points. 

* The probability (p-value) of observing results at least as extreme as what is present in your data sample. P-value less than .05 retain h_0 else reject h_0.

* One Sample T-test: To examine the average difference between a sample and the known value 
of the population mean. Assumes the population from which the sample is drawn is normally distributed and the sample observations are randomly drawn and independent. 

* Two Sample T-test: To examine the average difference between two samples drawn from two 
different populations. Assumes the populations from which the samples are drawn are normally dist, the standard deviations of the two populations are equal, and sample observations are randomly drawn and independent.

* F-Test: To assess whether the variances of two different populations are equal. Assumes the population from which the sample is drawn is normally distributed and the sample observations are randomly drawn and independent.

* Barlett Test: F-Test for more than two populations. 

* One-Way ANOVA: To assess the equality of means of two or more groups. Assumes the populations from which the samples are drawn are normally dist, the standard deviations of the populations are equal, and sample observations are randomly drawn and independent. 

* Chi-Squared Test of Independence: To test whether two categorical variables are independent.  Assumes the sample observations are randomly drawn and independent.

## Visualization

* Make trendlines, scatterplots, and add confidence intervals

## Modeling

### Preprocessing

* sklearn: preprocessing.binarize

* pandas.factorize: Encode input values as an enumerated type

### Feature Selection

* Variance Threshold: Use this to exclude features that don't meet a variance threshold.
 
* Univariate Feature Selection: For classification can use chi-squared and F-Test while F-Test for regression. Functions: chi2, f_regression, f_classification from sklearn.feature_selection.

* SelectKBest/SelectPercentile: Keep the k highest scoring features and keep a user-specified highest scoring percentage of features. Can use the univariate feature selection in these functions. 

* sklearn: feature_selection.RFE/CV

### Model Selection

* Gradient boosting for class imbalance

* cross_val_score & cross_val_predict

* Building custom ensembles: mlxtend

* [Sklearn Evaluation](https://edublancas.github.io/sklearn-evaluation/)

* Skplot::[1](https://github.com/DistrictDataLabs/yellowbrick):[2](https://github.com/reiinakano/scikit-plot)

## Test Stuff

https://drsimonj.svbtle.com/easy-machine-learning-pipelines-with-pipelearner-intro-and-call-for-contributors

https://topepo.github.io/recipes/

https://github.com/tidyverse/modelr

[Tuning & Ensembling](https://github.com/reiinakano/xcessiv)
