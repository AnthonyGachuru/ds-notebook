---
title: "Placeholder"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## PH

[Optimal Bins For Histogram](https://stats.stackexchange.com/questions/798/calculating-optimal-number-of-bins-in-a-histogram/862#862)

**The Coefficient of Unalikeability**: (Unalikeability is defined as how often observations differ from one another). It varies from 0 to 1. The higher the value, the more unalike the data are.

tidyr: complete function == pandas ravel; df.index.name = None; r functions: identify function, lev: compare leverage to average leverage
join (py) = paste(sep, collapse), strsplit = split
stack(): long form + unstack(): wide form 


##SQL

text functions [concatenate and mid] to manipulate text characters to clean text data
Remove duplicate rows and perform text manipulations
Transform and rearrange columns and rows
Reconcile table data by joining and matching
Explain differences between relational databases (tabular data storage) and document-based databases (key-value pairs)
Collect data using standard sql commands [Select, From, Insert, Update, Delete, Create, Drop, Truncate]
Create relationships between tables and data including has_many and many_to_many with join tables using Joins [‘left’, ‘right’, ‘inner’, ‘full’, and ‘union’]
Use union and union_all to collect data
Explain the differences between null vs. 0
Use sql conditional operators [=,!=,>,<,IN and BETWEEN] and Null functions[‘is Null’, ‘ is not Null’ and ‘NVL’ ] to create boolean statements
Use sql mathematical functions [ABS, SIGN, MOD, FLOOR, CEILING, ROUND, SQRT] to clean data
Use CASE statements to clean and analyze data
handle null values in excel/sql
stat methods in excel and sql
agg n math funcs in excel/sql, window function & lag
SQL: select, update, delete, insert, create + set, case, decode + how to deal with nulls sql
A schema is a blueprint for storing data. For a table every row has same number of columns, and each column is of the same type.


Missingness: 1) Completely at Random, At Random, Not at random

The pros of imputation:
➢ Helps retain a larger sample size of your data.
➢ Does not sacrifice all the available information in an observation because of
sparse missingness.
➢ Can potentially avoid unwanted bias.
❖ The cons of imputation:
➢ The standard errors of any estimates made during analyses following
imputation can tend to be too small.
■ The methods are under the assumption that all measurements are
actually “known,” when in fact some were imputed.
➢ Can potentially induce unwanted bias.