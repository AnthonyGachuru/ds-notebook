# Communicate, Deploy, Maintain

## Communicate

https://jupyter.org/jupyter-book/intro.html#installation

https://metabase.com/
  
http://koaning.io/make-docs.html

[Mode Studio](https://about.modeanalytics.com/)

Summarize the motivation and goals of the project. It should either be an executive summary or a technical abstract.

State results with details as needed. For an end user presentation show how the model fits into and improves the user's workflow, and how to use it.

Discuss recommendations, outstanding issues, and possible future work.

Structure: Introduction | Summary | Results | Conclusion

Interactive Reports Without Shiny: plotly, dt, crosstalk

Reference this [blog](https://www.dataquest.io/blog/data-science-project-style-guide/).

appendices | post mortems

## Deploy/Maintain

https://www.meltano.com/

http://pachyderm.io

https://datacoral.com/

Algorithmia

Apache Airflow

https://cloud.google.com/ai-platform/

https://www.hyperscience.com/

https://ropensci.org/technotes/2019/03/18/drake-700/

https://www.mlflow.org

[Weights & Biases](https://www.wandb.com/blog/towards-reproducibility)

Databricks

Ansible

https://www.prefect.io/

https://aws.amazon.com/sagemaker/

[Pipelines](http://stat545.com/automation01_slides/#/automating-data-analysis-pipelines)

[Workflow System](https://snakemake.readthedocs.io/en/stable/)

[Pipeline Automation](http://stat545.com/automation01_slides/#/automating-data-analysis-pipelines)

[Streaming Data](https://www.wallaroolabs.com)

[Spark](https://docs.azuredatabricks.net/spark/latest/mllib/mllib-pipelines-and-stuctured-streaming.html)

[batch job execution framework](https://stitchfix.github.io/flotilla-os/)

[Sklearn Serialization](https://cmry.github.io/notes/serialize)

[Auto Deploy](http://content.nexosis.com/twimlai)

TDD: [1](http://engineering.pivotal.io/post/test-driven-development-for-data-science/) | [2](http://www.tdda.info/) | [3](http://stochasticsolutions.com/)

[Model Deployment aaS](https://orchestrahq.com)

https://dvc.org/

docker

design by contract: contracts

property based testing: hypothesis

unit tests: pytest, static typing: mypy

packrat & conda

https://matrixds.com/ | https://www.crestle.com | CoCalc

[Git Workflow](https://blog.osteele.com/2008/05/my-git-workflow/)

Tests: [1](http://engineering.pivotal.io/post/test-driven-development-for-data-science/) | [2](http://www.tdda.info/) | [3](http://stochasticsolutions.com/) | [4](https://github.com/ericmjl/data-testing-tutorial) | [R](http://r-pkgs.had.co.nz/tests.html) | pytest/hypothesis
 
Validation: [1](https://github.com/data-cleaning/validate) | [2](https://rdrr.io/cran/checkmate/) | [3](https://github.com/shawnbrown/datatest)

A really useful feature is sharing environments so others can install all the packages used in your code, with the correct versions. You can save the packages to a YAML file with: `conda env export > environment.yaml`. 
The first part conda env export writes out all the packages in the environment, including the Python version. Above you can see the name of the environment and all the dependencies (along with versions) are listed. The second part of the export command, > environment.yaml writes the exported text to a YAML file environment.yaml. This file can now be shared and others will be able to create the same environment you used for the project. To create an environment from an environment file use: `conda env create -f environment.yaml`. This will create a new environment with the same name listed in environment.yaml.

package for helper functions and units tests

use makefile to run cmd commands
