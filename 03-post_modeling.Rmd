# Communicate, Deploy, Maintain

## Communicate

### Outline

Reference this [blog](https://www.dataquest.io/blog/data-science-project-style-guide/).

**Summary**

Summarize the motivation and goals of the project. It should either be an executive summary or a technical abstract.

**Results**

State results with details as needed. For an end user presentation show how the model fits into and improves the user's workflow, and how to use it.

**Conclusion**

Discuss recommendations, outstanding issues, and possible future work.

### Other

Write appendices and port mortems

Interactive Rmarkdown reports without shiny: plotly, dt, crosstalk

https://metabase.com/ http://koaning.io/make-docs.html [Mode Studio](https://about.modeanalytics.com/)

## Maintain

TDD: [1](http://engineering.pivotal.io/post/test-driven-development-for-data-science/) | [2](http://www.tdda.info/) | [3](http://stochasticsolutions.com/)

Tests: [1](http://engineering.pivotal.io/post/test-driven-development-for-data-science/) | [2](http://www.tdda.info/) | [3](http://stochasticsolutions.com/) | [4](https://github.com/ericmjl/data-testing-tutorial) | [R](http://r-pkgs.had.co.nz/tests.html) | pytest/hypothesis
 
Validation: [1](https://github.com/data-cleaning/validate) | [2](https://rdrr.io/cran/checkmate/) | [3](https://github.com/shawnbrown/datatest)

dvc https://matrixds.com/ | https://www.crestle.com | CoCalc

design by contract: contracts, unit tests: pytest Hypothesis (property based testing), static typing: mypy

packrat & conda

A really useful feature is sharing environments so others can install all the packages used in your code, with the correct versions. You can save the packages to a YAML file with: `conda env export > environment.yaml`. 
The first part conda env export writes out all the packages in the environment, including the Python version. Above you can see the name of the environment and all the dependencies (along with versions) are listed. The second part of the export command, > environment.yaml writes the exported text to a YAML file environment.yaml. This file can now be shared and others will be able to create the same environment you used for the project. To create an environment from an environment file use: `conda env create -f environment.yaml`. This will create a new environment with the same name listed in environment.yaml.

## Deploy

DE: [aaS](https://www.astronomer.io/)  [data build tool](https://www.getdbt.com/) https://www.stitchdata.com/ https://www.prefect.io/ https://mlflow.org/ Apache Arrow https://fivetran.com/ + snowflake

https://www.meltano.com/ http://pachyderm.io https://datacoral.com/ Algorithmia https://cloud.google.com/ai-platform/ https://ropensci.org/technotes/2019/03/18/drake-700/ [Weights & Biases](https://www.wandb.com/blog/towards-reproducibility) Databricks Ansible https://aws.amazon.com/sagemaker/ https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/

[Pipelines](http://stat545.com/automation01_slides/#/automating-data-analysis-pipelines) [Workflow System](https://snakemake.readthedocs.io/en/stable/) [Pipeline Automation](http://stat545.com/automation01_slides/#/automating-data-analysis-pipelines) [Streaming Data](https://www.wallaroolabs.com) [Spark](https://docs.azuredatabricks.net/spark/latest/mllib/mllib-pipelines-and-stuctured-streaming.html)

[batch job execution framework](https://stitchfix.github.io/flotilla-os/) [Sklearn Serialization](https://cmry.github.io/notes/serialize) [Auto Deploy](http://content.nexosis.com/twimlai) [Model Deployment](https://orchestrahq.com)

use makefile to run cmd commands pipelines and feature unions Sig Opt fabric-python & http://www.pyinvoke.org/
